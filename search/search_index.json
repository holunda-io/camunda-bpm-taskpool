{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Why should I use this? # Are you building a process application or process platform containing multiple process applications using Camunda BPM Engine? Are you building a custom task list for your existing application using Camunda BPM Engine? Are you trying to solve performance issues with your task list? Are you building an archive view for the business objects processed by your processes? Are you interested in an audit log for any changes performed during the execution of the business process? If you can answer one of the previous questions with yes, the taskpool / datapool library might help you. How to start? # We provide documentation for different people and different tasks. A good starting point is the Introduction . You might want to look at Reference Guide containing a Working Example and details about Usage Scenarios . Get in touch # If you are missing a feature, have a question regarding usage or deployment, you should definitely get in touch with us. There are various ways to do so: Slack Github Issues","title":"Home"},{"location":"index.html#why-should-i-use-this","text":"Are you building a process application or process platform containing multiple process applications using Camunda BPM Engine? Are you building a custom task list for your existing application using Camunda BPM Engine? Are you trying to solve performance issues with your task list? Are you building an archive view for the business objects processed by your processes? Are you interested in an audit log for any changes performed during the execution of the business process? If you can answer one of the previous questions with yes, the taskpool / datapool library might help you.","title":"Why should I use this?"},{"location":"index.html#how-to-start","text":"We provide documentation for different people and different tasks. A good starting point is the Introduction . You might want to look at Reference Guide containing a Working Example and details about Usage Scenarios .","title":"How to start?"},{"location":"index.html#get-in-touch","text":"If you are missing a feature, have a question regarding usage or deployment, you should definitely get in touch with us. There are various ways to do so: Slack Github Issues","title":"Get in touch"},{"location":"developer-guide/contribution.html","text":"There are several ways in which you may contribute to this project. File issues Submit a pull requests Found a bug or missing feature? # Please file an issue in our issue tracking system. Submit a Pull Request # If you found a solution to an open issue and implemented it, we would be happy to add your contribution in the code base. For doing so, please create a pull request. Prior to that, please make sure you: rebase against the develop branch stick to project coding conventions added test cases for the problem you are solving added docs, describing the change generally comply with codacy report","title":"Contribution"},{"location":"developer-guide/contribution.html#found-a-bug-or-missing-feature","text":"Please file an issue in our issue tracking system.","title":"Found a bug or missing feature?"},{"location":"developer-guide/contribution.html#submit-a-pull-request","text":"If you found a solution to an open issue and implemented it, we would be happy to add your contribution in the code base. For doing so, please create a pull request. Prior to that, please make sure you: rebase against the develop branch stick to project coding conventions added test cases for the problem you are solving added docs, describing the change generally comply with codacy report","title":"Submit a Pull Request"},{"location":"developer-guide/project-setup.html","text":"If you are interested in developing and building the project please follow the following instruction. Version control # To get sources of the project, please execute: git clone https://github.com/holunda-io/camunda-bpm-taskpool.git cd camunda-bpm-taskpool We are using gitflow in our git SCM. That means that you should start from develop branch, create a feature/<name> out of it and once it is completed create a pull request containing it. Please squash your commits before submitting and use semantic commit messages, if possible. Project Build # Perform the following steps to get a development setup up and running. ./mvnw clean install Integration Tests # By default, the build command will ignore the run of failsafe Maven plugin executing the integration tests (usual JUnit tests with class names ending with ITest). In order to run integration tests, please call from your command line: ./mvnw integration-test failsafe:verify -Pitest -DskipFrontend Project build modes and profiles # Camunda Version # You can choose the used Camunda version by specifying the profile camunda-ee or camunda-ce . The default version is a Community Edition. Specify -Pcamunda-ee to switch to Camunda Enterprise edition. This will require a valid Camunda license. You can put it into a file ~/.camunda/license.txt and it will be detected automatically. Skip Frontend # TIP: Components for production use of camunda-bpm-taskpool are backend components only. Frontend components are only created for examples and demonstration purpose. If you are interested in backend only, specify the -DskipFrontend switch. This will accelerate the build significantly. Generate SQL scripts # The project uses Flyway for versioning of database changes. In doing so we provide the required SQL scripts for initialization of required database objects (including Camunda BPM schema, Axon schema and some example schema). If you change any of those you will need to create SQL scripts describing your change. For doing so, you can re-generate the scripts running: ./mvnw -Pgenerate-sql NOTE: The existing scripts must not be replaced or changed, but new additional scripts needs to added. Build Documentation # We are using MkDocs for generation of a static site documentation and rely on Markdown as much as possible. MkDocs is a written in Python 3 and needs to be installed on your machine. For the installation please run the following command from your command line: python3 -m pip install --upgrade pip python3 -m pip install -r ./docs/requirements.txt For creation of documentation, please run: mkdocs build The docs are generated into site directory. !!!note If you want to develop your docs in 'live' mode, run mkdocs serve and access the http://localhost:8000/ from your browser. Examples # Polyflow provides a series of examples demonstrating different features of the library. By default, the examples are built during the project build. If you want to skip the examples, please add the following parameter to your command line or disable the examples module in your IDE. ./mvnw clean package -DskipExamples Local Start # !!!important If you want to run examples locally, you will need docker and docker-compose . Pre-requirements # Before starting the example applications, make sure the required infrastructure is set up and running. Please run the following from your command line: ./.docker/setup.sh This will create required docker volumes and network. Start containers # In order to operate, the distributed example applications will require several containers. These are: Axon Server PostgreSQL Database Mongo Database (if used in projection) Please start the required containers executing the corresponding command from examples/scenarios/distributed-axon-server : cd ./examples/scenarios/distributed-axon-server docker-compose up Starting application (distributed scenario) # For the distributed scenario, the containers from the previous section needs to be started. To start applications, either use your IDE and create two run configurations for the classes (in this order): io.holunda.camunda.taskpool.example.process.ExampleTaskpoolApplicationDistributedWithAxonServer io.holunda.camunda.taskpool.example.process.ExampleProcessApplicationDistributedWithAxonServer Alternatively, you can run them from the command line: ./mvnw spring-boot:run -f examples/scenarios/distributed-axon-server/taskpool-application ./mvnw spring-boot:run -f examples/scenarios/distributed-axon-server/process-application Continuous Integration # Travis CI is building all branches on commit hook. In addition, a private-hosted Jenkins CI is used to build the releases. Release Management # Release management has been set-up for use of Sonatype Nexus (= Maven Central) What modules get deployed to repository # Every module is enabled by default. If you want to change this, please provide the property <maven.deploy.skip> true </maven.deploy.skip> inside the corresponding pom.xml . Currently, all examples are EXCLUDED from publication into Maven Central. Trigger new release # WARNING: This operation requires special permissions. We use gitflow for development (see A successful git branching model for more details). You could use gitflow with native git commands, but then you would have to change the versions in the poms manually. Therefore, we use the mvn gitflow plugin , which handles this and other things nicely. You can build a release with: ./mvnw gitflow:release-start ./mvnw gitflow:release-finish This will update the versions in the pom.xml s accordingly and push the release tag to the master branch and update the develop branch for the new development version. Trigger a deploy # !!! warning This operation requires special permissions. Currently, CI allows for deployment of artifacts to Maven Central and is executed using github actions. This means, that a push to master branch will start the corresponding build job, and if successful the artifacts will get into Staging Repositories of OSS Sonatype without manual intervention. Run deploy from local machine # WARNING: This operation requires special permissions. If you still want to execute the deployment from your local machine, you need to have GPG keys at place and to execute the following command on the master branch: export GPG_KEYNAME = \"<keyname>\" export GPG_PASSPHRASE = \"<secret>\" ./mvnw clean deploy -B -DskipTests -DskipExamples -Prelease -Dgpg.keyname = $GPG_KEYNAME -Dgpg.passphrase = $GPG_PASSPHRASE Release to public repositories # WARNING: This operation requires special permissions. The deploy job will publish the artifacts to Nexus OSS staging repositories. Don't forget to close and release the repository to enable it's sync with Maven Central.","title":"Project Setup"},{"location":"developer-guide/project-setup.html#version-control","text":"To get sources of the project, please execute: git clone https://github.com/holunda-io/camunda-bpm-taskpool.git cd camunda-bpm-taskpool We are using gitflow in our git SCM. That means that you should start from develop branch, create a feature/<name> out of it and once it is completed create a pull request containing it. Please squash your commits before submitting and use semantic commit messages, if possible.","title":"Version control"},{"location":"developer-guide/project-setup.html#project-build","text":"Perform the following steps to get a development setup up and running. ./mvnw clean install","title":"Project Build"},{"location":"developer-guide/project-setup.html#integration-tests","text":"By default, the build command will ignore the run of failsafe Maven plugin executing the integration tests (usual JUnit tests with class names ending with ITest). In order to run integration tests, please call from your command line: ./mvnw integration-test failsafe:verify -Pitest -DskipFrontend","title":"Integration Tests"},{"location":"developer-guide/project-setup.html#project-build-modes-and-profiles","text":"","title":"Project build modes and profiles"},{"location":"developer-guide/project-setup.html#camunda-version","text":"You can choose the used Camunda version by specifying the profile camunda-ee or camunda-ce . The default version is a Community Edition. Specify -Pcamunda-ee to switch to Camunda Enterprise edition. This will require a valid Camunda license. You can put it into a file ~/.camunda/license.txt and it will be detected automatically.","title":"Camunda Version"},{"location":"developer-guide/project-setup.html#skip-frontend","text":"TIP: Components for production use of camunda-bpm-taskpool are backend components only. Frontend components are only created for examples and demonstration purpose. If you are interested in backend only, specify the -DskipFrontend switch. This will accelerate the build significantly.","title":"Skip Frontend"},{"location":"developer-guide/project-setup.html#generate-sql-scripts","text":"The project uses Flyway for versioning of database changes. In doing so we provide the required SQL scripts for initialization of required database objects (including Camunda BPM schema, Axon schema and some example schema). If you change any of those you will need to create SQL scripts describing your change. For doing so, you can re-generate the scripts running: ./mvnw -Pgenerate-sql NOTE: The existing scripts must not be replaced or changed, but new additional scripts needs to added.","title":"Generate SQL scripts"},{"location":"developer-guide/project-setup.html#build-documentation","text":"We are using MkDocs for generation of a static site documentation and rely on Markdown as much as possible. MkDocs is a written in Python 3 and needs to be installed on your machine. For the installation please run the following command from your command line: python3 -m pip install --upgrade pip python3 -m pip install -r ./docs/requirements.txt For creation of documentation, please run: mkdocs build The docs are generated into site directory. !!!note If you want to develop your docs in 'live' mode, run mkdocs serve and access the http://localhost:8000/ from your browser.","title":"Build Documentation"},{"location":"developer-guide/project-setup.html#examples","text":"Polyflow provides a series of examples demonstrating different features of the library. By default, the examples are built during the project build. If you want to skip the examples, please add the following parameter to your command line or disable the examples module in your IDE. ./mvnw clean package -DskipExamples","title":"Examples"},{"location":"developer-guide/project-setup.html#local-start","text":"!!!important If you want to run examples locally, you will need docker and docker-compose .","title":"Local Start"},{"location":"developer-guide/project-setup.html#pre-requirements","text":"Before starting the example applications, make sure the required infrastructure is set up and running. Please run the following from your command line: ./.docker/setup.sh This will create required docker volumes and network.","title":"Pre-requirements"},{"location":"developer-guide/project-setup.html#start-containers","text":"In order to operate, the distributed example applications will require several containers. These are: Axon Server PostgreSQL Database Mongo Database (if used in projection) Please start the required containers executing the corresponding command from examples/scenarios/distributed-axon-server : cd ./examples/scenarios/distributed-axon-server docker-compose up","title":"Start containers"},{"location":"developer-guide/project-setup.html#starting-application-distributed-scenario","text":"For the distributed scenario, the containers from the previous section needs to be started. To start applications, either use your IDE and create two run configurations for the classes (in this order): io.holunda.camunda.taskpool.example.process.ExampleTaskpoolApplicationDistributedWithAxonServer io.holunda.camunda.taskpool.example.process.ExampleProcessApplicationDistributedWithAxonServer Alternatively, you can run them from the command line: ./mvnw spring-boot:run -f examples/scenarios/distributed-axon-server/taskpool-application ./mvnw spring-boot:run -f examples/scenarios/distributed-axon-server/process-application","title":"Starting application (distributed scenario)"},{"location":"developer-guide/project-setup.html#continuous-integration","text":"Travis CI is building all branches on commit hook. In addition, a private-hosted Jenkins CI is used to build the releases.","title":"Continuous Integration"},{"location":"developer-guide/project-setup.html#release-management","text":"Release management has been set-up for use of Sonatype Nexus (= Maven Central)","title":"Release Management"},{"location":"developer-guide/project-setup.html#what-modules-get-deployed-to-repository","text":"Every module is enabled by default. If you want to change this, please provide the property <maven.deploy.skip> true </maven.deploy.skip> inside the corresponding pom.xml . Currently, all examples are EXCLUDED from publication into Maven Central.","title":"What modules get deployed to repository"},{"location":"developer-guide/project-setup.html#trigger-new-release","text":"WARNING: This operation requires special permissions. We use gitflow for development (see A successful git branching model for more details). You could use gitflow with native git commands, but then you would have to change the versions in the poms manually. Therefore, we use the mvn gitflow plugin , which handles this and other things nicely. You can build a release with: ./mvnw gitflow:release-start ./mvnw gitflow:release-finish This will update the versions in the pom.xml s accordingly and push the release tag to the master branch and update the develop branch for the new development version.","title":"Trigger new release"},{"location":"developer-guide/project-setup.html#trigger-a-deploy","text":"!!! warning This operation requires special permissions. Currently, CI allows for deployment of artifacts to Maven Central and is executed using github actions. This means, that a push to master branch will start the corresponding build job, and if successful the artifacts will get into Staging Repositories of OSS Sonatype without manual intervention.","title":"Trigger a deploy"},{"location":"developer-guide/project-setup.html#run-deploy-from-local-machine","text":"WARNING: This operation requires special permissions. If you still want to execute the deployment from your local machine, you need to have GPG keys at place and to execute the following command on the master branch: export GPG_KEYNAME = \"<keyname>\" export GPG_PASSPHRASE = \"<secret>\" ./mvnw clean deploy -B -DskipTests -DskipExamples -Prelease -Dgpg.keyname = $GPG_KEYNAME -Dgpg.passphrase = $GPG_PASSPHRASE","title":"Run deploy from local machine"},{"location":"developer-guide/project-setup.html#release-to-public-repositories","text":"WARNING: This operation requires special permissions. The deploy job will publish the artifacts to Nexus OSS staging repositories. Don't forget to close and release the repository to enable it's sync with Maven Central.","title":"Release to public repositories"},{"location":"examples/example-approval.html","text":"Along with library modules several example modules and applications are provided, demonstrating the main features of the solution. This includes a series of example applications for usage in different Usage Scenarios . They all share the same business process described in the next section. Business context: Approval # Take a look on the process model above. Imagine you are building a system that responsible for management of all approval requests in the company. Using this system, you can submit requests which then get eventually approved or rejected. Sometimes, the approver doesn't approve or reject, but returns the request back to the originator for correction (that is the person, who submitted the request). Then, the originator can amend the request and resubmit it or cancel the request. An approval request is modelled in the following way. The subject describes what the request is about, the applicant is the person whom it is about (can be different from originator ). Finally, the amount and currency denote the costs of the request. All requests must be stored for compliance purposes. The request is initially created in DRAFT mode. It gets to state IN PROGRESS as soon as the process is started and will eventually get to ACCEPTED or REJECTED as a final state. For sample purposes two groups of users are created: The Muppet Show ( Kermit , Piggy , Gonzo and Fozzy ) and The Avengers ( Ironman , Hulk ). Gonzo and Fozzy are responsible for approvals. Process Run # Let's play the following run through this process model. Ironman submits an Advanced Training request on behalf of Hulk The request costs are provided in wrong currency and Gonzo returns the request to Ironman for correction (EUR instead of USD) Ironman changes the currency to USD and re-submits the request Gonzo is out of office, so Fozzy takes over and approves the request Running Examples # To run the example please consult the link:scenarios[Usage Scenarios] section. TIP: Since the process application includes Camunda BPM engine, you can use the standard Camunda webapps by navigating to http://localhost:8080/camunda/app/ . The default user and password are admin / admin . Story board # The following storyboard can be used to understand the mechanics behind the provided implementation: TIP: In this storyboard, we assume you started the single node scenario and the application runs locally on http://localhost:8080. Please adjust the URLs accordingly, if you started differently. To start the approval process for a given request open your browser and navigate to the Example Tasklist : http://localhost:8080/taskpool/ . Please note that the selected user is Ironman . Open the menu ( Start new... ) in and select 'Request Approval'. You should see the start form for the example approval process. Select Advanced Training from one of predefined templates and click Start . The start form will disappear and redirect back to the empty Tasklist . Since you are still acting as Ironman there are nothing you can do here. Please switch the user to Gonzo in the top right corner and you should see the user task Approve Request from process Request Approval . Examine the task details by clicking Data tab in Details column. You can see the data of the request correlated to the current process instance. Click on the task name and you will see the user task form of the Approve Request task. Select the option Return request to originator and click complete. Switch to Workpieces and you should see the request business object. Examine the approval request by clicking Data , Audit and Description tabs in Details column. Change user back to Ironman and switch back to the Tasklist and open the Amend request task. Change the currency to USD and re-submit the request. Change user back to Fozzy , open the Approve Request task and approve the request by selecting the appropriate option. Switch to Workpieces and you should still see the request business object, even after the process is finished. Examine the approval request by clicking Data , Audit and Description tabs in Details column.","title":"Example Approval"},{"location":"examples/example-approval.html#business-context-approval","text":"Take a look on the process model above. Imagine you are building a system that responsible for management of all approval requests in the company. Using this system, you can submit requests which then get eventually approved or rejected. Sometimes, the approver doesn't approve or reject, but returns the request back to the originator for correction (that is the person, who submitted the request). Then, the originator can amend the request and resubmit it or cancel the request. An approval request is modelled in the following way. The subject describes what the request is about, the applicant is the person whom it is about (can be different from originator ). Finally, the amount and currency denote the costs of the request. All requests must be stored for compliance purposes. The request is initially created in DRAFT mode. It gets to state IN PROGRESS as soon as the process is started and will eventually get to ACCEPTED or REJECTED as a final state. For sample purposes two groups of users are created: The Muppet Show ( Kermit , Piggy , Gonzo and Fozzy ) and The Avengers ( Ironman , Hulk ). Gonzo and Fozzy are responsible for approvals.","title":"Business context: Approval"},{"location":"examples/example-approval.html#process-run","text":"Let's play the following run through this process model. Ironman submits an Advanced Training request on behalf of Hulk The request costs are provided in wrong currency and Gonzo returns the request to Ironman for correction (EUR instead of USD) Ironman changes the currency to USD and re-submits the request Gonzo is out of office, so Fozzy takes over and approves the request","title":"Process Run"},{"location":"examples/example-approval.html#running-examples","text":"To run the example please consult the link:scenarios[Usage Scenarios] section. TIP: Since the process application includes Camunda BPM engine, you can use the standard Camunda webapps by navigating to http://localhost:8080/camunda/app/ . The default user and password are admin / admin .","title":"Running Examples"},{"location":"examples/example-approval.html#story-board","text":"The following storyboard can be used to understand the mechanics behind the provided implementation: TIP: In this storyboard, we assume you started the single node scenario and the application runs locally on http://localhost:8080. Please adjust the URLs accordingly, if you started differently. To start the approval process for a given request open your browser and navigate to the Example Tasklist : http://localhost:8080/taskpool/ . Please note that the selected user is Ironman . Open the menu ( Start new... ) in and select 'Request Approval'. You should see the start form for the example approval process. Select Advanced Training from one of predefined templates and click Start . The start form will disappear and redirect back to the empty Tasklist . Since you are still acting as Ironman there are nothing you can do here. Please switch the user to Gonzo in the top right corner and you should see the user task Approve Request from process Request Approval . Examine the task details by clicking Data tab in Details column. You can see the data of the request correlated to the current process instance. Click on the task name and you will see the user task form of the Approve Request task. Select the option Return request to originator and click complete. Switch to Workpieces and you should see the request business object. Examine the approval request by clicking Data , Audit and Description tabs in Details column. Change user back to Ironman and switch back to the Tasklist and open the Amend request task. Change the currency to USD and re-submit the request. Change user back to Fozzy , open the Approve Request task and approve the request by selecting the appropriate option. Switch to Workpieces and you should still see the request business object, even after the process is finished. Examine the approval request by clicking Data , Audit and Description tabs in Details column.","title":"Story board"},{"location":"examples/example-components/index.html","text":"For demonstration purposes we built several example components and reuse them to demonstrate the Example application in different Usage Scenarios . These components are not part of a polyflow distribution and serve demonstration purposes only. They still show implementation of components which needs to be implemented by you and are not part of the provided library. Process Application Example Components # To show the integration of Polyflow components into a Process Application, the process discussed in Example Approval application has been implemented. Process Application Frontend Process Application Backend Process Platform Example Components # To show the integration of Polyflow components into a Process Platform, a simple task list and a workpieces view (archive view for business objects) has been implemented. Process Platform Frontend Process Platform Backend Shared Example Components # Components used by other example components. User Management","title":"Example Components Overview"},{"location":"examples/example-components/index.html#process-application-example-components","text":"To show the integration of Polyflow components into a Process Application, the process discussed in Example Approval application has been implemented. Process Application Frontend Process Application Backend","title":"Process Application Example Components"},{"location":"examples/example-components/index.html#process-platform-example-components","text":"To show the integration of Polyflow components into a Process Platform, a simple task list and a workpieces view (archive view for business objects) has been implemented. Process Platform Frontend Process Platform Backend","title":"Process Platform Example Components"},{"location":"examples/example-components/index.html#shared-example-components","text":"Components used by other example components. User Management","title":"Shared Example Components"},{"location":"examples/example-components/pa-backend.html","text":"The process application backend is implementing the process described in the Example application . It demonstrates a typical three-tier application, following the Boundary-Control-Entity pattern. Boundary Tier # The REST API is defined using OpenAPI specification and is implemented by Spring MVC controllers. It defines of four logical parts: Environment Controller Request Controller Approve Task Controller Amend Task Controller Control Tier # The control tier is implemented using stateless Spring Beans and orchestrated by the Camunda BPM Process. Typical JavaDelegate and ExecutionListener are used as a glue layer. Business services of this layer are responsible for the integration with Datapool Components to reflect the status of the Request business entity. The Camunda BPM Engine is configured to use the TaskCollector to integrate with remaining components of the camunda-bpm-taskpool . Entity Tier # The entity tier is implemented using Spring Data JPA, using Hibernate entities. Application data and process engine data is stored using a RDBMS.","title":"Process Application Backend"},{"location":"examples/example-components/pa-backend.html#boundary-tier","text":"The REST API is defined using OpenAPI specification and is implemented by Spring MVC controllers. It defines of four logical parts: Environment Controller Request Controller Approve Task Controller Amend Task Controller","title":"Boundary Tier"},{"location":"examples/example-components/pa-backend.html#control-tier","text":"The control tier is implemented using stateless Spring Beans and orchestrated by the Camunda BPM Process. Typical JavaDelegate and ExecutionListener are used as a glue layer. Business services of this layer are responsible for the integration with Datapool Components to reflect the status of the Request business entity. The Camunda BPM Engine is configured to use the TaskCollector to integrate with remaining components of the camunda-bpm-taskpool .","title":"Control Tier"},{"location":"examples/example-components/pa-backend.html#entity-tier","text":"The entity tier is implemented using Spring Data JPA, using Hibernate entities. Application data and process engine data is stored using a RDBMS.","title":"Entity Tier"},{"location":"examples/example-components/pa-frontend.html","text":"The process application backend is implementing the user task forms and business object views for the example application. It is built as a typical Angular Single Page Application (SPA) and provides views for both user tasks and the business object. It communicates with process application backend via REST API, defined in the latter. The user primarily interacts with the process platform which seamless integrate with the process applications. Usually, it provides integration points for user-task embedding / UI-composition. Unfortunately, this topic strongly depends on the frontend technology and is not a subject we want to demonstrate in this example. For simplicity, we built a very simple example, skipping the UI composition / integration entirely. The navigation between the process platform and process application is just as simple as a full page navigation of a hyperlink.","title":"Process Application Frontend"},{"location":"examples/example-components/pp-backend.html","text":"The purpose of the example process platform backend is to demonstrate how process agnostic parts of the process solution can be implemented.","title":"Process Platform Backend"},{"location":"examples/example-components/pp-frontend.html","text":"The example process platform frontend provides example implementation of two views: Example Tasklist Example Workpieces List Example Tasklist # Example Tasklist is a simple implementation of task inbox for a single user. It provides the following features: Lists tasks in the system for selected user Allows for switching users (faking different user login for demonstration purposes) Tasks include information about the process, name, description, create time, due date, priority and assignment. Tasks include process data (from process instance) Tasks include correlated business data (correlated via variable from process instance) The list of tasks is sortable The list of tasks is pageable (7 items per page) Allows claiming / unclaiming Provides a deeplink to the user task form provided by the process application Allows starting new process instances Here is, how it looks like showing task descriptions: you can optionally show the business data correlated with user task: Example Workpieces List # The example workpieces list is provides a list of business objects / workpieces that are currently processed by the processes even after the process has already been finished. It provides the following features: Lists business objects in the system for selected user Allows for switching users (faking different user login for demonstration purposes) Business objects include information about the type, status (with sub status), name, details Business objects include details about contained data Business objects include audit log with all state changes The list is pageable (7 items per page) Business object view Provides a deeplink to the business object view provided by the process application Here is, how it looks like showing the audit log:","title":"Process Platform Frontend"},{"location":"examples/example-components/pp-frontend.html#example-tasklist","text":"Example Tasklist is a simple implementation of task inbox for a single user. It provides the following features: Lists tasks in the system for selected user Allows for switching users (faking different user login for demonstration purposes) Tasks include information about the process, name, description, create time, due date, priority and assignment. Tasks include process data (from process instance) Tasks include correlated business data (correlated via variable from process instance) The list of tasks is sortable The list of tasks is pageable (7 items per page) Allows claiming / unclaiming Provides a deeplink to the user task form provided by the process application Allows starting new process instances Here is, how it looks like showing task descriptions: you can optionally show the business data correlated with user task:","title":"Example Tasklist"},{"location":"examples/example-components/pp-frontend.html#example-workpieces-list","text":"The example workpieces list is provides a list of business objects / workpieces that are currently processed by the processes even after the process has already been finished. It provides the following features: Lists business objects in the system for selected user Allows for switching users (faking different user login for demonstration purposes) Business objects include information about the type, status (with sub status), name, details Business objects include details about contained data Business objects include audit log with all state changes The list is pageable (7 items per page) Business object view Provides a deeplink to the business object view provided by the process application Here is, how it looks like showing the audit log:","title":"Example Workpieces List"},{"location":"examples/example-components/user-management.html","text":"Usually, a central user management like a Single-Sign-On (SSO) is a part deployed into the process application landscape. This is responsible for authentication and authorization of the user and is required to control the role-based access to user tasks. In our example application, we disable any security checks to avoid the unneeded complexity. In doing so, we implemented a trivial user store holding some pre-defined users used in example components and allow to simply switch users from the frontend by choosing a different user from the drop-down list. It is integrated with the example frontends by passing around the user id along with the requests and provide a way to resolve the user by that id. Technically speaking, the user id can be used to retrieve the permissions of the user and check them in the backend.","title":"User Management"},{"location":"examples/scenarios/index.html","text":"Depending on your requirements and infrastructure available several deployment scenarios of the components is possible. The simplest setup is to run all components on a single node. A more advanced scenario is to distribute components and connect them. In doing so, one of the challenging issues for distribution and connecting microservices is a setup of messaging technology supporting required message exchange patterns (MEPs) for a CQRS system. Because of different semantics of commands, events and queries and additional requirements of event-sourced persistence a special implementation of command bus, event bus and event store is required. In particular, two scenarios can be distinguished: using Axon Server or using a different distribution technology. The provided Example application is implemented several times demonstrating the following usage scenarios: Single Node Scenario Distributed Scenario using Axon Server Distributed Scenario without Axon Server It is a good idea to understand the single node scenario first and then move on to more elaborated scenarios.","title":"Scenarios Overview"},{"location":"examples/scenarios/distributed-axon-server.html","text":"Scenario description # A distributed scenario is helpful if you intend to build a central process platform and multiple process applications using it. In general, this is the main use case for taskpool itself, but the distribution aspects adds technical complexity to the resulting architecture. Especially, following the architecture blueprint of Axon Framework, the three buses (command bus, event bus and query bus) needs to be distributed and act as connecting infrastructure between components. Axon Server provides an implementation for this requirement leading to a distributed buses and a central event store. It is easy to use, easy to configure and easy to run. If you need a HA setup, you will need the enterprise license of Axon Server. Essentially, if don't have another HA ready-to use messaging, this scenario might be your way to go. This scenario supports: central process platform components (including task pool and data pool) free choice for projection persistence (can be replayed) no direct communication between process platform and process application is required (e.g. via REST, since it is routed via command bus) The following configuration is used in the distributes scenario with Axon Server: Bus distribution is provided by Axon Server Connector (command bus, event bus, query bus) Axon Server is used as Event Store Postgresql is used as a database for: Camunda BPM Engine Process Application Datasource Mongo is used as persistence for projection view ( mongo-view ) The following diagram depicts the distribution of the components and the messaging: Running Example # This example is demonstrating the usage of the Camunda BPM Taskpool with components distributed with help of Axon Server. It provides two applications for demonstration purposes: the process application and the process platform. Both applications are built as SpringBoot applications. System Requirements # JDK 11 Docker Docker Compose Preparations # Before you begin, please build the entire project with mvn clean install from the command line in the project root directory. You will need some backing services (Axon Server, PostgreSQL, MongoDB) and you can easily start them locally by using the provided docker-compose.yml file. Before you start change the directory to examples/scenarios/distributed-axon-server and run a preparation script .docker/setup.sh . You can do it with the following code from your command line (you need to do it once): cd examples/scenarios/distributed-axon-server .docker/setup.sh Now, start required containers. The easiest way to do so is to run: docker-compose up -d To verify it is running, open your browser http://localhost:8024/ . You should see the Axon Server administration console. Start # The demo application consists of several Maven modules. In order to start the example, you will need to start only two of them in the following order: taskpool-application (process platform) process-application (example process application) The modules can be started by running from command line in the examples/scenarios/distributed-axon-server directory using Maven or start the packaged application using: java -jar taskpool-application/target/*.jar java -jar process-application/target/*.jar Useful URLs # Process Platform # http://localhost:8081/taskpool/ http://localhost:8081/swagger-ui/ Process Application # http://localhost:8080/camunda/app/ http://localhost:8080/swagger-ui/","title":"Distributed using Axon Server"},{"location":"examples/scenarios/distributed-axon-server.html#scenario-description","text":"A distributed scenario is helpful if you intend to build a central process platform and multiple process applications using it. In general, this is the main use case for taskpool itself, but the distribution aspects adds technical complexity to the resulting architecture. Especially, following the architecture blueprint of Axon Framework, the three buses (command bus, event bus and query bus) needs to be distributed and act as connecting infrastructure between components. Axon Server provides an implementation for this requirement leading to a distributed buses and a central event store. It is easy to use, easy to configure and easy to run. If you need a HA setup, you will need the enterprise license of Axon Server. Essentially, if don't have another HA ready-to use messaging, this scenario might be your way to go. This scenario supports: central process platform components (including task pool and data pool) free choice for projection persistence (can be replayed) no direct communication between process platform and process application is required (e.g. via REST, since it is routed via command bus) The following configuration is used in the distributes scenario with Axon Server: Bus distribution is provided by Axon Server Connector (command bus, event bus, query bus) Axon Server is used as Event Store Postgresql is used as a database for: Camunda BPM Engine Process Application Datasource Mongo is used as persistence for projection view ( mongo-view ) The following diagram depicts the distribution of the components and the messaging:","title":"Scenario description"},{"location":"examples/scenarios/distributed-axon-server.html#running-example","text":"This example is demonstrating the usage of the Camunda BPM Taskpool with components distributed with help of Axon Server. It provides two applications for demonstration purposes: the process application and the process platform. Both applications are built as SpringBoot applications.","title":"Running Example"},{"location":"examples/scenarios/distributed-axon-server.html#system-requirements","text":"JDK 11 Docker Docker Compose","title":"System Requirements"},{"location":"examples/scenarios/distributed-axon-server.html#preparations","text":"Before you begin, please build the entire project with mvn clean install from the command line in the project root directory. You will need some backing services (Axon Server, PostgreSQL, MongoDB) and you can easily start them locally by using the provided docker-compose.yml file. Before you start change the directory to examples/scenarios/distributed-axon-server and run a preparation script .docker/setup.sh . You can do it with the following code from your command line (you need to do it once): cd examples/scenarios/distributed-axon-server .docker/setup.sh Now, start required containers. The easiest way to do so is to run: docker-compose up -d To verify it is running, open your browser http://localhost:8024/ . You should see the Axon Server administration console.","title":"Preparations"},{"location":"examples/scenarios/distributed-axon-server.html#start","text":"The demo application consists of several Maven modules. In order to start the example, you will need to start only two of them in the following order: taskpool-application (process platform) process-application (example process application) The modules can be started by running from command line in the examples/scenarios/distributed-axon-server directory using Maven or start the packaged application using: java -jar taskpool-application/target/*.jar java -jar process-application/target/*.jar","title":"Start"},{"location":"examples/scenarios/distributed-axon-server.html#useful-urls","text":"","title":"Useful URLs"},{"location":"examples/scenarios/distributed-axon-server.html#process-platform","text":"http://localhost:8081/taskpool/ http://localhost:8081/swagger-ui/","title":"Process Platform"},{"location":"examples/scenarios/distributed-axon-server.html#process-application","text":"http://localhost:8080/camunda/app/ http://localhost:8080/swagger-ui/","title":"Process Application"},{"location":"examples/scenarios/distributed-no-axon-server.html","text":"Scenario description # If you already have another messaging at place, like Kafka or RabbitMQ, you might skip the usage of Axon Server. In doing so, you will be responsible for distribution of events and will need to surrender some features. This scenario supports: distributed task pool / data pool view must be persistent direct communication between task list / engines required (addressing, routing) concurrent access to engines might become a problem (no unit of work guarantees) The following diagram depicts the distribution of the components and the messaging. The following diagram depicts the task run from Process Application to the end user, consuming it via Tasklist API. The CamundaEventingEnginePlugin provided with the Taskpool tracks events in the Camunda engine (e.g. the creation, deletion or modification of a User Task) and makes them available as Spring events. The Taskpool Collector component listens to those events. It collects all relevant events that happen in a single transaction and registers a transaction synchronization to process them beforeCommit. Just before the transaction is committed, the collected events are accumulated and sent as Axon Commands through the CommandGateway . The Taskpool Core processes those commands and issues Axon Events through the EventGateway which are stored in Axon's database tables within the same transaction. The transaction commit finishes. If anything goes wrong before this point, the transaction rolls back and it is as though nothing ever happened. In the Axon Kafka Extension , a TrackingEventProcessor polls for events and sees them as soon as the transaction that created them is committed. It sends each event to Kafka and waits for an acknowledgment from Kafka. If sending fails or times out, the event processor goes into error mode and retries until it succeeds. This can lead to events being published to Kafka more than once but guarantees at-least-once delivery. Within the Tasklist API, the Axon Kafka Extension polls the events from Kafka and another TrackingEventProcessor forwards them to the TaskPoolMongoService where they are processed to update the Mongo DB accordingly. When a user queries the Tasklist API for tasks, two things happen: Firstly, the Mongo DB is queried for the current state of tasks for this user and these tasks are returned. Secondly, the Tasklist API subscribes to any changes to the Mongo DB. These changes are filtered for relevance to the user and relevant changes are returned after the current state as an infinite stream until the request is cancelled or interrupted for some reason. From Process Application to Kafka # From Kafka to Tasklist API #","title":"Distributed without Axon Server"},{"location":"examples/scenarios/distributed-no-axon-server.html#scenario-description","text":"If you already have another messaging at place, like Kafka or RabbitMQ, you might skip the usage of Axon Server. In doing so, you will be responsible for distribution of events and will need to surrender some features. This scenario supports: distributed task pool / data pool view must be persistent direct communication between task list / engines required (addressing, routing) concurrent access to engines might become a problem (no unit of work guarantees) The following diagram depicts the distribution of the components and the messaging. The following diagram depicts the task run from Process Application to the end user, consuming it via Tasklist API. The CamundaEventingEnginePlugin provided with the Taskpool tracks events in the Camunda engine (e.g. the creation, deletion or modification of a User Task) and makes them available as Spring events. The Taskpool Collector component listens to those events. It collects all relevant events that happen in a single transaction and registers a transaction synchronization to process them beforeCommit. Just before the transaction is committed, the collected events are accumulated and sent as Axon Commands through the CommandGateway . The Taskpool Core processes those commands and issues Axon Events through the EventGateway which are stored in Axon's database tables within the same transaction. The transaction commit finishes. If anything goes wrong before this point, the transaction rolls back and it is as though nothing ever happened. In the Axon Kafka Extension , a TrackingEventProcessor polls for events and sees them as soon as the transaction that created them is committed. It sends each event to Kafka and waits for an acknowledgment from Kafka. If sending fails or times out, the event processor goes into error mode and retries until it succeeds. This can lead to events being published to Kafka more than once but guarantees at-least-once delivery. Within the Tasklist API, the Axon Kafka Extension polls the events from Kafka and another TrackingEventProcessor forwards them to the TaskPoolMongoService where they are processed to update the Mongo DB accordingly. When a user queries the Tasklist API for tasks, two things happen: Firstly, the Mongo DB is queried for the current state of tasks for this user and these tasks are returned. Secondly, the Tasklist API subscribes to any changes to the Mongo DB. These changes are filtered for relevance to the user and relevant changes are returned after the current state as an infinite stream until the request is cancelled or interrupted for some reason.","title":"Scenario description"},{"location":"examples/scenarios/distributed-no-axon-server.html#from-process-application-to-kafka","text":"","title":"From Process Application to Kafka"},{"location":"examples/scenarios/distributed-no-axon-server.html#from-kafka-to-tasklist-api","text":"","title":"From Kafka to Tasklist API"},{"location":"examples/scenarios/single-node.html","text":"Scenario description # In a single node scenario, the process application and the process platform components are deployed in a single node. In most production environments this scenario doesn't make sense because of poor reliability. Still, it is valid for demonstration purpose and is ideal to play around with components and understand their purpose and interaction between them. In a single node scenario the following configuration is used: All buses are local (command bus, event bus, query bus) In-memory H2 is used as a database for: Camunda BPM Engine Axon Event Store (JPA-based) Process Application Datasource In-memory transient projection view is used ( simple-view ) Check the following diagram for more details: Running Example # This example demonstrates the usage of the Camunda BPM Taskpool deployed in one single node and is built as a SpringBoot application. System Requirements # JDK 11 Preparations # Before you begin, please build the entire project with ./mvnw clean install from the command line in the project root directory. Start # The demo application consists of one Maven module which can be started by running from command line in the examples/scenarios/single-node directory using Maven. Alternatively you can start the packaged application using: [source,bash] java -jar target/*.jar Useful URLs # http://localhost:8080/taskpool/ http://localhost:8080/swagger-ui/ http://localhost:8080/camunda/app/tasklist/default/","title":"Single Node"},{"location":"examples/scenarios/single-node.html#scenario-description","text":"In a single node scenario, the process application and the process platform components are deployed in a single node. In most production environments this scenario doesn't make sense because of poor reliability. Still, it is valid for demonstration purpose and is ideal to play around with components and understand their purpose and interaction between them. In a single node scenario the following configuration is used: All buses are local (command bus, event bus, query bus) In-memory H2 is used as a database for: Camunda BPM Engine Axon Event Store (JPA-based) Process Application Datasource In-memory transient projection view is used ( simple-view ) Check the following diagram for more details:","title":"Scenario description"},{"location":"examples/scenarios/single-node.html#running-example","text":"This example demonstrates the usage of the Camunda BPM Taskpool deployed in one single node and is built as a SpringBoot application.","title":"Running Example"},{"location":"examples/scenarios/single-node.html#system-requirements","text":"JDK 11","title":"System Requirements"},{"location":"examples/scenarios/single-node.html#preparations","text":"Before you begin, please build the entire project with ./mvnw clean install from the command line in the project root directory.","title":"Preparations"},{"location":"examples/scenarios/single-node.html#start","text":"The demo application consists of one Maven module which can be started by running from command line in the examples/scenarios/single-node directory using Maven. Alternatively you can start the packaged application using: [source,bash] java -jar target/*.jar","title":"Start"},{"location":"examples/scenarios/single-node.html#useful-urls","text":"http://localhost:8080/taskpool/ http://localhost:8080/swagger-ui/ http://localhost:8080/camunda/app/tasklist/default/","title":"Useful URLs"},{"location":"integration/index.html","text":"This guide is describing steps required to configure an existing Camunda BPM Spring Boot Process Application and connect to existing Process Platform. Add dependency to Polyflow integration starter # Apart from the example application, you might be interested in integrating Polyflow Taskpool and Datapool into your existing application. To do so, you need to enable your Camunda BPM process engine to use the library. For doing so, add the polyflow-integration-camunda-bpm-engine-parent library. In Maven, add the following dependency to your pom.xml : <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-integration-camunda-bpm-engine-parent </artifactId> <version> ${polyflow.version} </version> </dependency> Activate Polyflow Support # Now, find your SpringBoot application class and add an additional annotation to it: @SpringBootApplication @EnableTaskpoolEngineSupport public class MyApplication { public static void main ( String ... args ) { SpringApplication . run ( MyApplication . class , args ); } } Configure your Polyflow provisioning # Finally, add the following block to your application.yml : camunda : bpm : default-serialization-format : application/json history-level : full polyflow : integration : client : camunda : application-name : ${spring.application.name} # default collector : camunda : application-name : ${spring.application.name} # default process-instance : enabled : true process-definition : enabled : true process-variable : enabled : true task : enabled : true enricher : type : processVariables sender : enabled : true data-entry : enabled : true type : simple application-name : ${spring.application.name} # default process-definition : enabled : true process-instance : enabled : true process-variable : enabled : true task : enabled : true type : tx send-within-transaction : true # Must be set to true in single node scenario. form-url-resolver : defaultTaskTemplate : \"/tasks/${formKey}/${id}?userId=%userId%\" defaultApplicationTemplate : \"http://localhost:${server.port}/${applicationName}\" defaultProcessTemplate : \"/${formKey}?userId=%userId%\" Now, start your process engine. If you run into a user task, you should see on the console how this is passed to task pool. For more details on the configuration of different options, please consult the Polyflow Components sections.","title":"Integration"},{"location":"integration/index.html#add-dependency-to-polyflow-integration-starter","text":"Apart from the example application, you might be interested in integrating Polyflow Taskpool and Datapool into your existing application. To do so, you need to enable your Camunda BPM process engine to use the library. For doing so, add the polyflow-integration-camunda-bpm-engine-parent library. In Maven, add the following dependency to your pom.xml : <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-integration-camunda-bpm-engine-parent </artifactId> <version> ${polyflow.version} </version> </dependency>","title":"Add dependency to Polyflow integration starter"},{"location":"integration/index.html#activate-polyflow-support","text":"Now, find your SpringBoot application class and add an additional annotation to it: @SpringBootApplication @EnableTaskpoolEngineSupport public class MyApplication { public static void main ( String ... args ) { SpringApplication . run ( MyApplication . class , args ); } }","title":"Activate Polyflow Support"},{"location":"integration/index.html#configure-your-polyflow-provisioning","text":"Finally, add the following block to your application.yml : camunda : bpm : default-serialization-format : application/json history-level : full polyflow : integration : client : camunda : application-name : ${spring.application.name} # default collector : camunda : application-name : ${spring.application.name} # default process-instance : enabled : true process-definition : enabled : true process-variable : enabled : true task : enabled : true enricher : type : processVariables sender : enabled : true data-entry : enabled : true type : simple application-name : ${spring.application.name} # default process-definition : enabled : true process-instance : enabled : true process-variable : enabled : true task : enabled : true type : tx send-within-transaction : true # Must be set to true in single node scenario. form-url-resolver : defaultTaskTemplate : \"/tasks/${formKey}/${id}?userId=%userId%\" defaultApplicationTemplate : \"http://localhost:${server.port}/${applicationName}\" defaultProcessTemplate : \"/${formKey}?userId=%userId%\" Now, start your process engine. If you run into a user task, you should see on the console how this is passed to task pool. For more details on the configuration of different options, please consult the Polyflow Components sections.","title":"Configure your Polyflow provisioning"},{"location":"introduction/features.html","text":"Supported process engines # Camunda BPM Task List # A task list is an application allowing to represent a list of user tasks. This list is created based on user's profile (including authorizations based on roles) for every user. The library provides the following features: user task API providing attributes important for processing mirroring tasks: provides a list of tasks in the system including all task attributes provided by Camunda BPM Engine reacts on all task life cycle events fired by the process engine high performance queries: creates read-optimized projections including task-, process- and business data centralized task list: running several Camunda BPM Engines in several applications is a common use case for larger companies. From the user's perspective, it is not feasible to login to several task lists and check for relevant user tasks. The demand for the centralized task list arises and can be addressed by the use of the taskpool library if the tasks from several process engines are collected and transmitted over the network. data enrichment: all scenarios, in which the data is not stored in the process payload, result in a cascade of queries executed after the task fetch. In contrast to that, the usage of the taskpool library with a data enrichment plugin mechanism (allowing to plug-in some data enricher on task creation) allows for caching the additional business data along with the task information, instead of querying it during task fetch. Archive List # An archive list provides a list of business objects processed during the execution of business process. Such a business object lifecycle spans over a longer period of time than the process instance. A common requirement is to get a list of such objects with different statuses like preliminary, in process or completed. The datapool library provides the following features: business object API providing attributes important for processing business object modification API for creating an audit log authorization API for business objects Process List # A process list provides a list of running instances and a list of process definitions deployed in the process engines connected to the library. It provides the following features: list of startable process definitions (including URLs to start forms) list of running process instances reacts on life cycle events of process instance","title":"Features"},{"location":"introduction/features.html#supported-process-engines","text":"Camunda BPM","title":"Supported process engines"},{"location":"introduction/features.html#task-list","text":"A task list is an application allowing to represent a list of user tasks. This list is created based on user's profile (including authorizations based on roles) for every user. The library provides the following features: user task API providing attributes important for processing mirroring tasks: provides a list of tasks in the system including all task attributes provided by Camunda BPM Engine reacts on all task life cycle events fired by the process engine high performance queries: creates read-optimized projections including task-, process- and business data centralized task list: running several Camunda BPM Engines in several applications is a common use case for larger companies. From the user's perspective, it is not feasible to login to several task lists and check for relevant user tasks. The demand for the centralized task list arises and can be addressed by the use of the taskpool library if the tasks from several process engines are collected and transmitted over the network. data enrichment: all scenarios, in which the data is not stored in the process payload, result in a cascade of queries executed after the task fetch. In contrast to that, the usage of the taskpool library with a data enrichment plugin mechanism (allowing to plug-in some data enricher on task creation) allows for caching the additional business data along with the task information, instead of querying it during task fetch.","title":"Task List"},{"location":"introduction/features.html#archive-list","text":"An archive list provides a list of business objects processed during the execution of business process. Such a business object lifecycle spans over a longer period of time than the process instance. A common requirement is to get a list of such objects with different statuses like preliminary, in process or completed. The datapool library provides the following features: business object API providing attributes important for processing business object modification API for creating an audit log authorization API for business objects","title":"Archive List"},{"location":"introduction/features.html#process-list","text":"A process list provides a list of running instances and a list of process definitions deployed in the process engines connected to the library. It provides the following features: list of startable process definitions (including URLs to start forms) list of running process instances reacts on life cycle events of process instance","title":"Process List"},{"location":"introduction/further-outlook.html","text":"This library serves as a foundation of several follow-up projects and tools: Skill-based-routing: based on information stored in the taskpool, a skill-based routing for task assignment can be implemented. Workload management: apart from the operative task management, the workload management is addressing issues like dynamic task assignment, optimal task distribution, assignment based on presence etc.","title":"Further Outlook"},{"location":"introduction/motivation.html","text":"Motivation and Goal # In the last years, we built several process applications and process platforms on behalf of the customers using Camunda BPM Engine. In doing so, we were able to extract common requirements, especially if the task-oriented application has been implemented. The requirements were basic and independent of the frontend implementation technology. It turned out that some issues occurred every time during the implementation. These were: coping with performance issues of Camunda BPM Engine by the big amount of tasks available (total tasks, tasks per user) creating high-performance custom queries for pre-loading process variables for tasks creating high-performance custom queries to pre-load business data associated with the running process instance high-performance re-ordering (sorting) of user tasks high-performance retrieving of tasks from several process engines repetitive queries with same result creating an archive view for business data items handled during the process execution creating an audit log of changes performed on business data items Since we were developing customer software, we found solutions to those requirements and gathered experience in applying different approaches for that. Some issues listed above result from fact that data on a single user task is being read much more often than written, depending on the user count. For systems with a big amount of users this becomes a serious performance issue and needs to be addressed. A possible solution to most of those issues listed above is to create a special component which has a read-optimized representation of tasks. Such component acts as a cache for tasks and allows for serving a high amount of queries without any performance impact to the process engine itself at the costs of loosing strong consistency (and working with eventual-consistent task list). We successfully applied this approach by multiple customers and identified the high initial invest as a main drawback of the solution. The goal of this project is to provide a component as a free and open source library, to be used as a foundation for creation of process platforms for Camunda BPM engine. It can also be used as an integration layer for custom process applications, custom user task lists and other components of process automation solutions.","title":"Motivation"},{"location":"introduction/motivation.html#motivation-and-goal","text":"In the last years, we built several process applications and process platforms on behalf of the customers using Camunda BPM Engine. In doing so, we were able to extract common requirements, especially if the task-oriented application has been implemented. The requirements were basic and independent of the frontend implementation technology. It turned out that some issues occurred every time during the implementation. These were: coping with performance issues of Camunda BPM Engine by the big amount of tasks available (total tasks, tasks per user) creating high-performance custom queries for pre-loading process variables for tasks creating high-performance custom queries to pre-load business data associated with the running process instance high-performance re-ordering (sorting) of user tasks high-performance retrieving of tasks from several process engines repetitive queries with same result creating an archive view for business data items handled during the process execution creating an audit log of changes performed on business data items Since we were developing customer software, we found solutions to those requirements and gathered experience in applying different approaches for that. Some issues listed above result from fact that data on a single user task is being read much more often than written, depending on the user count. For systems with a big amount of users this becomes a serious performance issue and needs to be addressed. A possible solution to most of those issues listed above is to create a special component which has a read-optimized representation of tasks. Such component acts as a cache for tasks and allows for serving a high amount of queries without any performance impact to the process engine itself at the costs of loosing strong consistency (and working with eventual-consistent task list). We successfully applied this approach by multiple customers and identified the high initial invest as a main drawback of the solution. The goal of this project is to provide a component as a free and open source library, to be used as a foundation for creation of process platforms for Camunda BPM engine. It can also be used as an integration layer for custom process applications, custom user task lists and other components of process automation solutions.","title":"Motivation and Goal"},{"location":"introduction/solution-architecture.html","text":"General Idea # The implementation of a single (small) process application can be performed mostly using the Camunda BPM library itself. If the solution becomes larger, for example by setting up multiple engines for different processes or the load on a single process engine becomes unmanageable for a process engine cluster, it is worth to separate the solution into process specific and process agnostic parts. We call the process specific part of the solution Process Application and the process agnostic part - Process Platform. Based on the assumption of the asymmetric read/write characteristics of task-oriented process applications, we decided to apply Command Query Responsibility Segregation (CQRS) pattern during the architectural design. As a result, we supply components to collect the user tasks from the process engines and create a read-optimized projections with user tasks and correlated business data. The components can be easily integrated into process applications and be used as foundation to build parts of the process platform. Design Decisions # We decided to build the library as a collection of loose-coupled components which can be used during the construction of the process automation solution in different ways, depending on your Usage Scenario . The process platform is a central application consisting of business process independent components like a central user management, task inbox (aka task list), archive view for business data processed, audit logs and others. One or many process applications integrate with the process platform by implementing individual business processes and provide user tasks and business data changes to it. They may also ship application frontends, which are integrated into/with the frontends of the process platform, including business object views, user task forms and other required pieces. The following diagram depicts the overall logical architecture: Implementation Decisions # The components are implemented using Kotlin programming language and rely on SpringBoot as execution environment. They make a massive use of Axon Framework as a basis of the CQRS implementation. // FIXME: replace this image with architecture of the camunda integration","title":"Solution Architecture"},{"location":"introduction/solution-architecture.html#general-idea","text":"The implementation of a single (small) process application can be performed mostly using the Camunda BPM library itself. If the solution becomes larger, for example by setting up multiple engines for different processes or the load on a single process engine becomes unmanageable for a process engine cluster, it is worth to separate the solution into process specific and process agnostic parts. We call the process specific part of the solution Process Application and the process agnostic part - Process Platform. Based on the assumption of the asymmetric read/write characteristics of task-oriented process applications, we decided to apply Command Query Responsibility Segregation (CQRS) pattern during the architectural design. As a result, we supply components to collect the user tasks from the process engines and create a read-optimized projections with user tasks and correlated business data. The components can be easily integrated into process applications and be used as foundation to build parts of the process platform.","title":"General Idea"},{"location":"introduction/solution-architecture.html#design-decisions","text":"We decided to build the library as a collection of loose-coupled components which can be used during the construction of the process automation solution in different ways, depending on your Usage Scenario . The process platform is a central application consisting of business process independent components like a central user management, task inbox (aka task list), archive view for business data processed, audit logs and others. One or many process applications integrate with the process platform by implementing individual business processes and provide user tasks and business data changes to it. They may also ship application frontends, which are integrated into/with the frontends of the process platform, including business object views, user task forms and other required pieces. The following diagram depicts the overall logical architecture:","title":"Design Decisions"},{"location":"introduction/solution-architecture.html#implementation-decisions","text":"The components are implemented using Kotlin programming language and rely on SpringBoot as execution environment. They make a massive use of Axon Framework as a basis of the CQRS implementation. // FIXME: replace this image with architecture of the camunda integration","title":"Implementation Decisions"},{"location":"reference-guide/index.html","text":"This reference guide is a primary source of information in order to understand how Polyflow components are used and how to configure them. It is divided into tow major sections: Components Configuration","title":"Reference Overview"},{"location":"reference-guide/components/index.html","text":"We decided to build a library as a collection of loose-coupled components which can be used during the construction of the process automation solution. In doing so, we provide Process Engine Integration Components which are intended to be deployed as a part of the process application. In addition, we provide Process Platform Components which serve as a building blocks for the process platform itself. Process Engine Integration Components # The Process Engine Integration Components are designed to be a part of process application deployment and react on engine changes / interact with the engine. These are split into common components which are independent of the used product and framework-dependent adapters: Common Integration Components # Datapool Sender Taskpool Sender Camunda BPM Integration Components # Camunda BPM Engine Interaction Client Camunda BPM Engine Taskpool Collector Camunda BPM Engine Taskpool Spring Boot Starter Process Platform Components # Process Platform Components are designed to build the process platform. The platform provides common functionality used by all process applications like common user management, unified task list and so on. Core Components # Core Components are responsible for the processing of commands about user tasks, process instances, process variables, business data items and form an event stream consumed by the view components. Depending on the scenario, they can be deployed either within the process application, process platform or even completely separately. Taskpool Core Datapool Core View Components # View Components are responsible for creation of a unified read-only projection of process definitions, process instances, process variables, user tasks and business data items. They are typically deployed as a part of the process platform. In-Memory View Mongo DB View Property Form URL Resolver Other Components # Variable Serializer Tasklist URL Resolver","title":"Component Overview"},{"location":"reference-guide/components/index.html#process-engine-integration-components","text":"The Process Engine Integration Components are designed to be a part of process application deployment and react on engine changes / interact with the engine. These are split into common components which are independent of the used product and framework-dependent adapters:","title":"Process Engine Integration Components"},{"location":"reference-guide/components/index.html#common-integration-components","text":"Datapool Sender Taskpool Sender","title":"Common Integration Components"},{"location":"reference-guide/components/index.html#camunda-bpm-integration-components","text":"Camunda BPM Engine Interaction Client Camunda BPM Engine Taskpool Collector Camunda BPM Engine Taskpool Spring Boot Starter","title":"Camunda BPM Integration Components"},{"location":"reference-guide/components/index.html#process-platform-components","text":"Process Platform Components are designed to build the process platform. The platform provides common functionality used by all process applications like common user management, unified task list and so on.","title":"Process Platform Components"},{"location":"reference-guide/components/index.html#core-components","text":"Core Components are responsible for the processing of commands about user tasks, process instances, process variables, business data items and form an event stream consumed by the view components. Depending on the scenario, they can be deployed either within the process application, process platform or even completely separately. Taskpool Core Datapool Core","title":"Core Components"},{"location":"reference-guide/components/index.html#view-components","text":"View Components are responsible for creation of a unified read-only projection of process definitions, process instances, process variables, user tasks and business data items. They are typically deployed as a part of the process platform. In-Memory View Mongo DB View Property Form URL Resolver","title":"View Components"},{"location":"reference-guide/components/index.html#other-components","text":"Variable Serializer Tasklist URL Resolver","title":"Other Components"},{"location":"reference-guide/components/camunda-interaction-client.html","text":"Camunda Engine Interaction Client # Purpose # This component performs changes delivered by Camunda Interaction Events on Camunda BPM engine. The following Camunda Interaction Events are supported: Claim User Task Unclaim User Task Defer User Task Undefer User Task Complete User Task","title":"Camunda BPM Engine Interaction Client"},{"location":"reference-guide/components/camunda-interaction-client.html#camunda-engine-interaction-client","text":"","title":"Camunda Engine Interaction Client"},{"location":"reference-guide/components/camunda-interaction-client.html#purpose","text":"This component performs changes delivered by Camunda Interaction Events on Camunda BPM engine. The following Camunda Interaction Events are supported: Claim User Task Unclaim User Task Defer User Task Undefer User Task Complete User Task","title":"Purpose"},{"location":"reference-guide/components/camunda-starter.html","text":"Purpose # The Camunda Engine Taskpool Support SpringBoot Starter is a convenience module providing a single module dependency to be included in the process application. It includes all process application modules and provides meaningful defaults for their options. Configuration # In order to enable the starter, please put the following annotation on any @Configuration annotated class of your SpringBoot application. @SpringBootApplication @EnableProcessApplication @EnableTaskpoolEngineSupport public class MyApplication { public static void main ( String ... args ) { SpringApplication . run ( MyApplication . class , args ); } } <1> Annotation to enable the engine support. The @EnableTaskpoolEngineSupport annotation has the same effect as the following block of annotations: @EnableCamundaSpringEventing @EnableCamundaEngineClient @EnableTaskCollector @EnableDataEntryCollector public class MyApplication { //... }","title":"Camunda BPM Engine Taskpool SpringBoot Starter"},{"location":"reference-guide/components/camunda-starter.html#purpose","text":"The Camunda Engine Taskpool Support SpringBoot Starter is a convenience module providing a single module dependency to be included in the process application. It includes all process application modules and provides meaningful defaults for their options.","title":"Purpose"},{"location":"reference-guide/components/camunda-starter.html#configuration","text":"In order to enable the starter, please put the following annotation on any @Configuration annotated class of your SpringBoot application. @SpringBootApplication @EnableProcessApplication @EnableTaskpoolEngineSupport public class MyApplication { public static void main ( String ... args ) { SpringApplication . run ( MyApplication . class , args ); } } <1> Annotation to enable the engine support. The @EnableTaskpoolEngineSupport annotation has the same effect as the following block of annotations: @EnableCamundaSpringEventing @EnableCamundaEngineClient @EnableTaskCollector @EnableDataEntryCollector public class MyApplication { //... }","title":"Configuration"},{"location":"reference-guide/components/camunda-taskpool-collector.html","text":"Taskpool Collector # Purpose # Taskpool Collector is a component deployed as a part of the process application (aside with Camunda BPM Engine) that is responsible for collecting information from the Camunda BPM Engine. It detects the intent of the operations executed inside the engine and creates the corresponding commands for the taskpool. The commands are enriched with data and transmitted to other taskpool components (via Axon Command Bus). In the following description, we use the terms event and command . Event denotes an entity received from Camunda BPM Engine (from delegate event listener or from history event listener) which is passed over to the Taskpool Collector using internal Spring eventing mechanism. The Taskpool Collector converts the series of such events into a Taskpool Command - an entity carrying an intent of change inside of the taskpool core. Please note that event has another meaning in CQRS/ES systems and other components of the taskpool, but in the context of Taskpool collector an event alway originates from Spring eventing. Features # Collection of process definitions Collection of process instance events Collection of process variable change events Collection of task events and history events Creation of task engine commands Enrichment of task engine commands with process variables Attachment of correlation information to task engine commands Transmission of commands to Axon command bus Provision of properties for process application Architecture # The Taskpool Collector consists of several components which can be devided into the following groups: Event collectors receive are responsible for gathering information and form commands Processors performs the command enrichment with payload and data correlation Command senders are responsible for accumulating commands and sending them to Command Gateway Usage and configuration # In order to enable collector component, include the Maven dependency to your process application: <dependency> <groupId> io.holunda.taskpool <groupId> <artifactId> camunda-bpm-taskpool-collector </artifactId> <version> ${camunda-taskpool.version} </version> <dependency> Then activate the taskpool collector by providing the annotation on any Spring Configuration: @Configuration @EnableTaskpoolCollector class MyProcessApplicationConfiguration { } Event collection # Taskpool Collector registers Spring Event Listener to the following events, fired by Camunda Eventing Engine Plugin: DelegateTask events: create assign delete complete HistoryEvent events: HistoricTaskInstanceEvent HistoricIdentityLinkLogEvent HistoricProcessInstanceEventEntity HistoricVariableUpdateEventEntity ** HistoricDetailVariableInstanceUpdateEntity The events are transformed into corresponding commands and passed over to the processor layer. Task commands enrichment # Alongside with attributes received from the Camunda BPM engine, the engine task commands can be enriched with additional attributes. There are three enrichment modes available controlled by the camunda.taskpool.task.collector.enricher.type property: no : No enrichment takes place process-variables : Enrichment of engine task commands with process variables custom : User provides own implementation Process variable enrichment # In particular cases, the data enclosed into task attibutes is not sufficient for the task list or other user-related components. The information may be available as process variables and need to be attached to the task in the taskpool. This is where Process Variable Task Enricher can be used. For this purpose, active it setting the property camunda.taskpool.collector.task.enricher.type to process-variables and the enricher will put process variables into the task payload. You can control what variables will be put into task command payload by providing the Process Variables Filter. The ProcessVariablesFilter is a Spring bean holding a list of individual VariableFilter - at most one per process definition key and optionally one without process definition key (a global filter). If the filter is not provded, a default filter is used which is an empty EXCLUDE filter, resulting in all process variables being attached to the user task. A VariableFilter can be of the following type: TaskVariableFilter : INCLUDE : task-level include filter, denoting a list of variables to be added for the task defined in the filter. EXCLUDE : task-level exclude filter, denoting a list of variables to be ignored for the task defined in the filter. All other variables are included. ProcessVariableFilter with process definition key: INCLUDE : process-level include filter, denoting a list of variables to be added for all tasks of the process. EXCLUDE : process-level exclude filter, denoting a list of variables to be ignored for all tasks of the process. ProcessVariableFilter without process definition key: INCLUDE : global include filter, denoting a list of variables to be added for all tasks of all processes for which no dedicated ProcessVariableFilter is defined. EXCLUDE : global exclude filter, denoting a list of variables to be ignored for all tasks of all processes for which no dedicated ProcessVariableFilter is defined. Here is an example, how the process variable filter can configure the enrichment: @Configuration public class MyTaskCollectorConfiguration { @Bean public ProcessVariablesFilter myProcessVariablesFilter () { return new ProcessVariablesFilter ( // define a variable filter for every process new VariableFilter [] { // define for every process definition // either a TaskVariableFilter or ProcessVariableFilter new TaskVariableFilter ( ProcessApproveRequest . KEY , // filter type FilterType . INCLUDE , ImmutableMap . < String , List < String >> builder () // define a variable filter for every task of the process . put ( ProcessApproveRequest . Elements . APPROVE_REQUEST , Lists . newArrayList ( ProcessApproveRequest . Variables . REQUEST_ID , ProcessApproveRequest . Variables . ORIGINATOR ) ) // and again . put ( ProcessApproveRequest . Elements . AMEND_REQUEST , Lists . newArrayList ( ProcessApproveRequest . Variables . REQUEST_ID , ProcessApproveRequest . Variables . COMMENT , ProcessApproveRequest . Variables . APPLICANT ) ). build () ), // optionally add a global filter for all processes // for that no individual filter was created new ProcessVariableFilter ( FilterType . INCLUDE , Lists . newArrayList ( CommonProcessVariables . CUSTOMER_ID )) } ); } } TIP: If you want to implement a custom enrichment, please provide your own implementation of the interface VariablesEnricher (register a Spring Component of the type) and set the property camunda.taskpool.collector.task.enricher.type to custom . Data Correlation # Apart from task payload attached by the enricher, the so-called Correlation with data entries can be configured. The data correlation allows to attach one or several references (that is a pair of values entryType and entryId ) of business data entry(ies) to a task. In the projection (which is used for querying of tasks) this correlations is be resolved and the information from business data events can be shown together with task information. The correlation to data events can be configured by providing a ProcessVariablesCorrelator bean. Here is an example how this can be done: @Bean fun processVariablesCorrelator () = ProcessVariablesCorrelator ( ProcessVariableCorrelation ( ProcessApproveRequest . KEY , < 1 > mapOf ( ProcessApproveRequest . Elements . APPROVE_REQUEST to mapOf ( < 2 > ProcessApproveRequest . Variables . REQUEST_ID to BusinessDataEntry . REQUEST ) ), mapOf ( ProcessApproveRequest . Variables . REQUEST_ID to BusinessDataEntry . REQUEST ) < 3 > ) ) <1> define correlation for every process <2> define a correlation for every task needed <3> define a correlation globally (for the whole process) The process variable correlator holds a list of process variable correlations - one for every process definition key. Every ProcessVariableCorrelation configures for all tasks or for an individual taskby providing a so-called correlation map. A correlation map is keyed by the name of a process variable inside Camunda Process Engine and holds the type of business data entry as value. Here is an example. Imagine the process instance is storing the id of an approval request in a process variable called varRequestId . The system responsible for storing approval requests fires data entry events supplying the data and using the entry type io.my.approvalRequest and the id of the request as entryId . In order to create a correlation in task task_approve_request of the process_approval_process we would provide the following configuration of the correlator: @Bean fun processVariablesCorrelator () = ProcessVariablesCorrelator ( ProcessVariableCorrelation ( \"process_approval_process\" , mapOf ( \"task_approve_request\" to mapOf ( \"varRequestId\" to \"io.my.approvalRequest\" // process variable 'varRequestId' holds the id of a data entry of type 'io.my.approvalRequest' ) ) ) ) If the process instance now contains the approval request id \"4711\" in the process variable varRequestId and the process reaches the task task_approve_request , the task will get the following correlation created (here written in JSON): \"correlations\" : [ { \"entryType\" : \"approvalRequest\" , \"entryId\" : \"4711\" } ] Command aggregation # In order to control sending of commands to command sender, the command sender activation property camunda.taskpool.collector.task.enabled is available. If disabled, the command sender will log any command instead of aggregating sending it to the command gateway. In addition you can control by the property camunda.taskpool.collector.task.sender.type if you want to use the default command sender or provide your own implementation. The default provided command sender (type: tx ) is collects all task commands during one transaction, group them by task id and accumulates by creating one command reflecting the intent of the task operation. It uses Axon Command Bus (encapsulated by the AxonCommandListGateway for sending the result over to the Axon command gateway. TIP: If you want to implement a custom command sending, please provide your own implementation of the interface EngineTaskCommandSender (register a Spring Component of the type) and set the property camunda.taskpool.collector.task.sender.type to custom . The Spring event listeners receiving events from the Camunda Engine plugin are called before the engine commits the transaction. Since all processing inside collector component and enricher is performed synchronously, the sender must waits until transaction to be successfully committed before sending any commands to the Command Gateway. Otherwise, on any error the transaction would be rolled-back and the command would create an inconsistency between the taskpool and the engine. Depending on your deployment scenario, you may want to control the exact point in time when the commands are sent to command gateway. The property camunda.taskpool.collector.task.sender.send-within-transaction is designed to influence this. If set to true , the commands are sent before the process engine transaction is committed, otherwise commands are sent after the process engine transaction is committed. WARNING: Never send commands over remote messaging before the transaction is committed, since you may produce unexpected results if Camunda fails to commit the transaction. Handling command transmission # The commands sent via gateway (e.g. AxonCommandListGateway ) are received by Command Handlers. The latter may accept or reject commands, depending on the state of the aggregate and other components. The AxonCommandListGateway is informed about the command outcome. By default, it will log the outcome to console (success is logged in DEBUG log level, errors are using ERROR log level). In some situations it is required to take care of command outcome. A prominent example is to include a metric for command dispatching errors into monitoring. For doing so, it is possible to provide own handlers for success and error command outcome. For this purpose, please provide a Spring Bean implementing the CommandSuccessHandler and CommandErrorHandler accordingly. Here is an example, how such a handler may look like: @Bean @Primary fun taskCommandErrorHandler (): TaskCommandErrorHandler = object : LoggingTaskCommandErrorHandler ( logger ) { override fun apply ( commandMessage : Any , commandResultMessage : CommandResultMessage < out Any? > ) { logger . info { \"<--------- CUSTOM ERROR HANDLER REPORT --------->\" } super . apply ( commandMessage , commandResultMessage ) logger . info { \"<------------------- END ----------------------->\" } } } Message codes # Please note that the logger root hierarchy is io.holunda.camunda.taskpool.collector Message Code Severity Logger* Description Meaning COLLECTOR-001 INFO Task commands will be collected. COLLECTOR-002 INFO Task commands not be collected. COLLECTOR-005 DEBUG .process.definition Process definition collecting has been disabled by property, skipping ${command.processDefinitionId}. COLLECTOR-006 DEBUG .process.instance Process instance collecting has been disabled by property, skipping ${command.processInstanceId}. COLLECTOR-007 DEBUG .process.variable Process variable collecting has been disabled by property, skipping ${command.processInstanceId}. COLLECTOR-008 DEBUG .task Task command collecting is disabled by property, would have enriched and sent command $command. ENRICHER-001 INFO Task commands will be enriched with process variables. ENRICHER-002 INFO Task commands will not be enriched. ENRICHER-003 INFO Task commands will be enriched by a custom enricher. ENRICHER-004 DEBUG .task.enricher Could not enrich variables from running execution ${command.sourceReference.executionId}, since it doesn't exist (anymore).","title":"Camunda BPM Engine Taskpool Collector"},{"location":"reference-guide/components/camunda-taskpool-collector.html#taskpool-collector","text":"","title":"Taskpool Collector"},{"location":"reference-guide/components/camunda-taskpool-collector.html#purpose","text":"Taskpool Collector is a component deployed as a part of the process application (aside with Camunda BPM Engine) that is responsible for collecting information from the Camunda BPM Engine. It detects the intent of the operations executed inside the engine and creates the corresponding commands for the taskpool. The commands are enriched with data and transmitted to other taskpool components (via Axon Command Bus). In the following description, we use the terms event and command . Event denotes an entity received from Camunda BPM Engine (from delegate event listener or from history event listener) which is passed over to the Taskpool Collector using internal Spring eventing mechanism. The Taskpool Collector converts the series of such events into a Taskpool Command - an entity carrying an intent of change inside of the taskpool core. Please note that event has another meaning in CQRS/ES systems and other components of the taskpool, but in the context of Taskpool collector an event alway originates from Spring eventing.","title":"Purpose"},{"location":"reference-guide/components/camunda-taskpool-collector.html#features","text":"Collection of process definitions Collection of process instance events Collection of process variable change events Collection of task events and history events Creation of task engine commands Enrichment of task engine commands with process variables Attachment of correlation information to task engine commands Transmission of commands to Axon command bus Provision of properties for process application","title":"Features"},{"location":"reference-guide/components/camunda-taskpool-collector.html#architecture","text":"The Taskpool Collector consists of several components which can be devided into the following groups: Event collectors receive are responsible for gathering information and form commands Processors performs the command enrichment with payload and data correlation Command senders are responsible for accumulating commands and sending them to Command Gateway","title":"Architecture"},{"location":"reference-guide/components/camunda-taskpool-collector.html#usage-and-configuration","text":"In order to enable collector component, include the Maven dependency to your process application: <dependency> <groupId> io.holunda.taskpool <groupId> <artifactId> camunda-bpm-taskpool-collector </artifactId> <version> ${camunda-taskpool.version} </version> <dependency> Then activate the taskpool collector by providing the annotation on any Spring Configuration: @Configuration @EnableTaskpoolCollector class MyProcessApplicationConfiguration { }","title":"Usage and configuration"},{"location":"reference-guide/components/camunda-taskpool-collector.html#event-collection","text":"Taskpool Collector registers Spring Event Listener to the following events, fired by Camunda Eventing Engine Plugin: DelegateTask events: create assign delete complete HistoryEvent events: HistoricTaskInstanceEvent HistoricIdentityLinkLogEvent HistoricProcessInstanceEventEntity HistoricVariableUpdateEventEntity ** HistoricDetailVariableInstanceUpdateEntity The events are transformed into corresponding commands and passed over to the processor layer.","title":"Event collection"},{"location":"reference-guide/components/camunda-taskpool-collector.html#task-commands-enrichment","text":"Alongside with attributes received from the Camunda BPM engine, the engine task commands can be enriched with additional attributes. There are three enrichment modes available controlled by the camunda.taskpool.task.collector.enricher.type property: no : No enrichment takes place process-variables : Enrichment of engine task commands with process variables custom : User provides own implementation","title":"Task commands enrichment"},{"location":"reference-guide/components/camunda-taskpool-collector.html#process-variable-enrichment","text":"In particular cases, the data enclosed into task attibutes is not sufficient for the task list or other user-related components. The information may be available as process variables and need to be attached to the task in the taskpool. This is where Process Variable Task Enricher can be used. For this purpose, active it setting the property camunda.taskpool.collector.task.enricher.type to process-variables and the enricher will put process variables into the task payload. You can control what variables will be put into task command payload by providing the Process Variables Filter. The ProcessVariablesFilter is a Spring bean holding a list of individual VariableFilter - at most one per process definition key and optionally one without process definition key (a global filter). If the filter is not provded, a default filter is used which is an empty EXCLUDE filter, resulting in all process variables being attached to the user task. A VariableFilter can be of the following type: TaskVariableFilter : INCLUDE : task-level include filter, denoting a list of variables to be added for the task defined in the filter. EXCLUDE : task-level exclude filter, denoting a list of variables to be ignored for the task defined in the filter. All other variables are included. ProcessVariableFilter with process definition key: INCLUDE : process-level include filter, denoting a list of variables to be added for all tasks of the process. EXCLUDE : process-level exclude filter, denoting a list of variables to be ignored for all tasks of the process. ProcessVariableFilter without process definition key: INCLUDE : global include filter, denoting a list of variables to be added for all tasks of all processes for which no dedicated ProcessVariableFilter is defined. EXCLUDE : global exclude filter, denoting a list of variables to be ignored for all tasks of all processes for which no dedicated ProcessVariableFilter is defined. Here is an example, how the process variable filter can configure the enrichment: @Configuration public class MyTaskCollectorConfiguration { @Bean public ProcessVariablesFilter myProcessVariablesFilter () { return new ProcessVariablesFilter ( // define a variable filter for every process new VariableFilter [] { // define for every process definition // either a TaskVariableFilter or ProcessVariableFilter new TaskVariableFilter ( ProcessApproveRequest . KEY , // filter type FilterType . INCLUDE , ImmutableMap . < String , List < String >> builder () // define a variable filter for every task of the process . put ( ProcessApproveRequest . Elements . APPROVE_REQUEST , Lists . newArrayList ( ProcessApproveRequest . Variables . REQUEST_ID , ProcessApproveRequest . Variables . ORIGINATOR ) ) // and again . put ( ProcessApproveRequest . Elements . AMEND_REQUEST , Lists . newArrayList ( ProcessApproveRequest . Variables . REQUEST_ID , ProcessApproveRequest . Variables . COMMENT , ProcessApproveRequest . Variables . APPLICANT ) ). build () ), // optionally add a global filter for all processes // for that no individual filter was created new ProcessVariableFilter ( FilterType . INCLUDE , Lists . newArrayList ( CommonProcessVariables . CUSTOMER_ID )) } ); } } TIP: If you want to implement a custom enrichment, please provide your own implementation of the interface VariablesEnricher (register a Spring Component of the type) and set the property camunda.taskpool.collector.task.enricher.type to custom .","title":"Process variable enrichment"},{"location":"reference-guide/components/camunda-taskpool-collector.html#data-correlation","text":"Apart from task payload attached by the enricher, the so-called Correlation with data entries can be configured. The data correlation allows to attach one or several references (that is a pair of values entryType and entryId ) of business data entry(ies) to a task. In the projection (which is used for querying of tasks) this correlations is be resolved and the information from business data events can be shown together with task information. The correlation to data events can be configured by providing a ProcessVariablesCorrelator bean. Here is an example how this can be done: @Bean fun processVariablesCorrelator () = ProcessVariablesCorrelator ( ProcessVariableCorrelation ( ProcessApproveRequest . KEY , < 1 > mapOf ( ProcessApproveRequest . Elements . APPROVE_REQUEST to mapOf ( < 2 > ProcessApproveRequest . Variables . REQUEST_ID to BusinessDataEntry . REQUEST ) ), mapOf ( ProcessApproveRequest . Variables . REQUEST_ID to BusinessDataEntry . REQUEST ) < 3 > ) ) <1> define correlation for every process <2> define a correlation for every task needed <3> define a correlation globally (for the whole process) The process variable correlator holds a list of process variable correlations - one for every process definition key. Every ProcessVariableCorrelation configures for all tasks or for an individual taskby providing a so-called correlation map. A correlation map is keyed by the name of a process variable inside Camunda Process Engine and holds the type of business data entry as value. Here is an example. Imagine the process instance is storing the id of an approval request in a process variable called varRequestId . The system responsible for storing approval requests fires data entry events supplying the data and using the entry type io.my.approvalRequest and the id of the request as entryId . In order to create a correlation in task task_approve_request of the process_approval_process we would provide the following configuration of the correlator: @Bean fun processVariablesCorrelator () = ProcessVariablesCorrelator ( ProcessVariableCorrelation ( \"process_approval_process\" , mapOf ( \"task_approve_request\" to mapOf ( \"varRequestId\" to \"io.my.approvalRequest\" // process variable 'varRequestId' holds the id of a data entry of type 'io.my.approvalRequest' ) ) ) ) If the process instance now contains the approval request id \"4711\" in the process variable varRequestId and the process reaches the task task_approve_request , the task will get the following correlation created (here written in JSON): \"correlations\" : [ { \"entryType\" : \"approvalRequest\" , \"entryId\" : \"4711\" } ]","title":"Data Correlation"},{"location":"reference-guide/components/camunda-taskpool-collector.html#command-aggregation","text":"In order to control sending of commands to command sender, the command sender activation property camunda.taskpool.collector.task.enabled is available. If disabled, the command sender will log any command instead of aggregating sending it to the command gateway. In addition you can control by the property camunda.taskpool.collector.task.sender.type if you want to use the default command sender or provide your own implementation. The default provided command sender (type: tx ) is collects all task commands during one transaction, group them by task id and accumulates by creating one command reflecting the intent of the task operation. It uses Axon Command Bus (encapsulated by the AxonCommandListGateway for sending the result over to the Axon command gateway. TIP: If you want to implement a custom command sending, please provide your own implementation of the interface EngineTaskCommandSender (register a Spring Component of the type) and set the property camunda.taskpool.collector.task.sender.type to custom . The Spring event listeners receiving events from the Camunda Engine plugin are called before the engine commits the transaction. Since all processing inside collector component and enricher is performed synchronously, the sender must waits until transaction to be successfully committed before sending any commands to the Command Gateway. Otherwise, on any error the transaction would be rolled-back and the command would create an inconsistency between the taskpool and the engine. Depending on your deployment scenario, you may want to control the exact point in time when the commands are sent to command gateway. The property camunda.taskpool.collector.task.sender.send-within-transaction is designed to influence this. If set to true , the commands are sent before the process engine transaction is committed, otherwise commands are sent after the process engine transaction is committed. WARNING: Never send commands over remote messaging before the transaction is committed, since you may produce unexpected results if Camunda fails to commit the transaction.","title":"Command aggregation"},{"location":"reference-guide/components/camunda-taskpool-collector.html#handling-command-transmission","text":"The commands sent via gateway (e.g. AxonCommandListGateway ) are received by Command Handlers. The latter may accept or reject commands, depending on the state of the aggregate and other components. The AxonCommandListGateway is informed about the command outcome. By default, it will log the outcome to console (success is logged in DEBUG log level, errors are using ERROR log level). In some situations it is required to take care of command outcome. A prominent example is to include a metric for command dispatching errors into monitoring. For doing so, it is possible to provide own handlers for success and error command outcome. For this purpose, please provide a Spring Bean implementing the CommandSuccessHandler and CommandErrorHandler accordingly. Here is an example, how such a handler may look like: @Bean @Primary fun taskCommandErrorHandler (): TaskCommandErrorHandler = object : LoggingTaskCommandErrorHandler ( logger ) { override fun apply ( commandMessage : Any , commandResultMessage : CommandResultMessage < out Any? > ) { logger . info { \"<--------- CUSTOM ERROR HANDLER REPORT --------->\" } super . apply ( commandMessage , commandResultMessage ) logger . info { \"<------------------- END ----------------------->\" } } }","title":"Handling command transmission"},{"location":"reference-guide/components/camunda-taskpool-collector.html#message-codes","text":"Please note that the logger root hierarchy is io.holunda.camunda.taskpool.collector Message Code Severity Logger* Description Meaning COLLECTOR-001 INFO Task commands will be collected. COLLECTOR-002 INFO Task commands not be collected. COLLECTOR-005 DEBUG .process.definition Process definition collecting has been disabled by property, skipping ${command.processDefinitionId}. COLLECTOR-006 DEBUG .process.instance Process instance collecting has been disabled by property, skipping ${command.processInstanceId}. COLLECTOR-007 DEBUG .process.variable Process variable collecting has been disabled by property, skipping ${command.processInstanceId}. COLLECTOR-008 DEBUG .task Task command collecting is disabled by property, would have enriched and sent command $command. ENRICHER-001 INFO Task commands will be enriched with process variables. ENRICHER-002 INFO Task commands will not be enriched. ENRICHER-003 INFO Task commands will be enriched by a custom enricher. ENRICHER-004 DEBUG .task.enricher Could not enrich variables from running execution ${command.sourceReference.executionId}, since it doesn't exist (anymore).","title":"Message codes"},{"location":"reference-guide/components/common-datapool-sender.html","text":"Datapool Collector # Purpose # Datapool collector is a component usually deployed as a part of the process application (but not necessary) that is responsible for collecting the Business Data Events fired by the application in order to allow for creation of a business data projection. In doing so, it collects and transmits it to Datapool Core. Features # Provides an API to submit arbitrary changes of business entities Provides an API to track changes (aka. Audit Log) Authorization on business entries Transmission of business entries commands Usage and configuration # <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> datapool-collector </artifactId> <version> ${camunda-taskpool.version} </version> </dependency> Then activate the datapool collector by providing the annotation on any Spring Configuration: @Configuration @EnableDataEntryCollector class MyDataEntryCollectorConfiguration { } Command transmission # In order to control sending of commands to command gateway, the command sender activation property camunda.taskpool.dataentry.sender.enabled (default is true ) is available. If disabled, the command sender will log any command instead of sending it to the command gateway. In addition you can control by the property camunda.taskpool.dataentry.sender.type if you want to use the default command sender or provide your own implementation. The default provided command sender (type: simple ) just sends the commands synchronously using Axon Command Bus. TIP: If you want to implement a custom command sending, please provide your own implementation of the interface DataEntryCommandSender (register a Spring Component of the type) and set the property camunda.taskpool.dataentry.sender.type to custom . Handling command transmission # The commands sent by the Datapool Collector are received by Command Handlers. The latter may accept or reject commands, depending on the state of the aggregate and other components. The SimpleDataEntryCommandSender is informed about the command outcome. By default, it will log the outcome to console (success is logged in DEBUG log level, errors are using ERROR log level). In some situations it is required to take care of command outcome. A prominent example is to include a metric for command dispatching errors into monitoring. For doing so, it is possible to provide own handlers for success and error command outcome. For Data Entry Command Sender (as a part of Datapool Collector ) please provide a Spring Bean implementing the io.holunda.camunda.datapool.sender.DataEntryCommandSuccessHandler and io.holunda.camunda.datapool.sender.DataEntryCommandErrorHandler accordingly. @Bean @Primary fun dataEntryCommandSuccessHandler () = object : DataEntryCommandResultHandler { override fun apply ( commandMessage : Any , commandResultMessage : CommandResultMessage < out Any? > ) { // do something here logger . info { \"Success\" } } } @Bean @Primary fun dataEntryCommandErrorHandler () = object : DataEntryCommandErrorHandler { override fun apply ( commandMessage : Any , commandResultMessage : CommandResultMessage < out Any? > ) { // do something here logger . error { \"Error\" } } }","title":"Datapool Sender"},{"location":"reference-guide/components/common-datapool-sender.html#datapool-collector","text":"","title":"Datapool Collector"},{"location":"reference-guide/components/common-datapool-sender.html#purpose","text":"Datapool collector is a component usually deployed as a part of the process application (but not necessary) that is responsible for collecting the Business Data Events fired by the application in order to allow for creation of a business data projection. In doing so, it collects and transmits it to Datapool Core.","title":"Purpose"},{"location":"reference-guide/components/common-datapool-sender.html#features","text":"Provides an API to submit arbitrary changes of business entities Provides an API to track changes (aka. Audit Log) Authorization on business entries Transmission of business entries commands","title":"Features"},{"location":"reference-guide/components/common-datapool-sender.html#usage-and-configuration","text":"<dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> datapool-collector </artifactId> <version> ${camunda-taskpool.version} </version> </dependency> Then activate the datapool collector by providing the annotation on any Spring Configuration: @Configuration @EnableDataEntryCollector class MyDataEntryCollectorConfiguration { }","title":"Usage and configuration"},{"location":"reference-guide/components/common-datapool-sender.html#command-transmission","text":"In order to control sending of commands to command gateway, the command sender activation property camunda.taskpool.dataentry.sender.enabled (default is true ) is available. If disabled, the command sender will log any command instead of sending it to the command gateway. In addition you can control by the property camunda.taskpool.dataentry.sender.type if you want to use the default command sender or provide your own implementation. The default provided command sender (type: simple ) just sends the commands synchronously using Axon Command Bus. TIP: If you want to implement a custom command sending, please provide your own implementation of the interface DataEntryCommandSender (register a Spring Component of the type) and set the property camunda.taskpool.dataentry.sender.type to custom .","title":"Command transmission"},{"location":"reference-guide/components/common-datapool-sender.html#handling-command-transmission","text":"The commands sent by the Datapool Collector are received by Command Handlers. The latter may accept or reject commands, depending on the state of the aggregate and other components. The SimpleDataEntryCommandSender is informed about the command outcome. By default, it will log the outcome to console (success is logged in DEBUG log level, errors are using ERROR log level). In some situations it is required to take care of command outcome. A prominent example is to include a metric for command dispatching errors into monitoring. For doing so, it is possible to provide own handlers for success and error command outcome. For Data Entry Command Sender (as a part of Datapool Collector ) please provide a Spring Bean implementing the io.holunda.camunda.datapool.sender.DataEntryCommandSuccessHandler and io.holunda.camunda.datapool.sender.DataEntryCommandErrorHandler accordingly. @Bean @Primary fun dataEntryCommandSuccessHandler () = object : DataEntryCommandResultHandler { override fun apply ( commandMessage : Any , commandResultMessage : CommandResultMessage < out Any? > ) { // do something here logger . info { \"Success\" } } } @Bean @Primary fun dataEntryCommandErrorHandler () = object : DataEntryCommandErrorHandler { override fun apply ( commandMessage : Any , commandResultMessage : CommandResultMessage < out Any? > ) { // do something here logger . error { \"Error\" } } }","title":"Handling command transmission"},{"location":"reference-guide/components/common-taskpool-sender.html","text":"Taskpool Sender # Purpose # Features # Usage and configuration # <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-taskpool-sender </artifactId> <version> ${camunda-taskpool.version} </version> </dependency> Then activate the taskpool sender by providing the annotation on any Spring Configuration: @Configuration @EnableTaskpoolSender class MyDataEntryCollectorConfiguration { } Command transmission # Handling command transmission # Message codes # Please note that the logger root hierarchy is io.holunda.camunda.taskpool.sender Message Code Severity Logger* Description Meaning SENDER-001 DEBUG .gateway Sending command over gateway disabled by property. Would have sent command payload . Sending of any commands is disabled. SENDER-002 DEBUG .gateway Successfully submitted command payload . Logging the successfully sent command. SENDER-003 ERROR .gateway Sending command $commandMessage resulted in error Error sending command. SENDER-004 DEBUG .task Process task sending is disabled by property. Would have sent $command. SENDER-005 DEBUG .task Handling ${taskCommands.size} commands for task $taskId using command accumulator $accumulatorName SENDER-006 DEBUG .task Handling ${taskCommands.size} commands for task $taskId using command accumulator $accumulatorName SENDER-007 DEBUG .process.definition Process definition sending is disabled by property. Would have sent $command. SENDER-007 DEBUG .process.instance Process instance sending is disabled by property. Would have sent $command. SENDER-009 DEBUG .process.variable Process variable sending is disabled by property. Would have sent $command. SENDER-011 INFO Taskpool task commands will be distributed over command bus. SENDER-012 INFO Taskpool task command distribution is disabled by property. SENDER-013 INFO Taskpool process definition commands will be distributed over command bus. SENDER-014 INFO Taskpool process definition command distribution is disabled by property. SENDER-015 INFO Taskpool process instance commands will be distributed over command bus. SENDER-016 INFO Taskpool process instance command distribution is disabled by property. SENDER-017 INFO Taskpool process variable commands will be distributed over command bus. SENDER-018 INFO Taskpool process variable command distribution is disabled by property.","title":"Taskpool Sender"},{"location":"reference-guide/components/common-taskpool-sender.html#taskpool-sender","text":"","title":"Taskpool Sender"},{"location":"reference-guide/components/common-taskpool-sender.html#purpose","text":"","title":"Purpose"},{"location":"reference-guide/components/common-taskpool-sender.html#features","text":"","title":"Features"},{"location":"reference-guide/components/common-taskpool-sender.html#usage-and-configuration","text":"<dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-taskpool-sender </artifactId> <version> ${camunda-taskpool.version} </version> </dependency> Then activate the taskpool sender by providing the annotation on any Spring Configuration: @Configuration @EnableTaskpoolSender class MyDataEntryCollectorConfiguration { }","title":"Usage and configuration"},{"location":"reference-guide/components/common-taskpool-sender.html#command-transmission","text":"","title":"Command transmission"},{"location":"reference-guide/components/common-taskpool-sender.html#handling-command-transmission","text":"","title":"Handling command transmission"},{"location":"reference-guide/components/common-taskpool-sender.html#message-codes","text":"Please note that the logger root hierarchy is io.holunda.camunda.taskpool.sender Message Code Severity Logger* Description Meaning SENDER-001 DEBUG .gateway Sending command over gateway disabled by property. Would have sent command payload . Sending of any commands is disabled. SENDER-002 DEBUG .gateway Successfully submitted command payload . Logging the successfully sent command. SENDER-003 ERROR .gateway Sending command $commandMessage resulted in error Error sending command. SENDER-004 DEBUG .task Process task sending is disabled by property. Would have sent $command. SENDER-005 DEBUG .task Handling ${taskCommands.size} commands for task $taskId using command accumulator $accumulatorName SENDER-006 DEBUG .task Handling ${taskCommands.size} commands for task $taskId using command accumulator $accumulatorName SENDER-007 DEBUG .process.definition Process definition sending is disabled by property. Would have sent $command. SENDER-007 DEBUG .process.instance Process instance sending is disabled by property. Would have sent $command. SENDER-009 DEBUG .process.variable Process variable sending is disabled by property. Would have sent $command. SENDER-011 INFO Taskpool task commands will be distributed over command bus. SENDER-012 INFO Taskpool task command distribution is disabled by property. SENDER-013 INFO Taskpool process definition commands will be distributed over command bus. SENDER-014 INFO Taskpool process definition command distribution is disabled by property. SENDER-015 INFO Taskpool process instance commands will be distributed over command bus. SENDER-016 INFO Taskpool process instance command distribution is disabled by property. SENDER-017 INFO Taskpool process variable commands will be distributed over command bus. SENDER-018 INFO Taskpool process variable command distribution is disabled by property.","title":"Message codes"},{"location":"reference-guide/components/core-datapool.html","text":"Datapool Core # Purpose # The component is responsible for maintaining and storing the consistent state of the datapool core concept of Business Data Entry. The component receives all commands and emits events, if changes are performed on underlying entities. The event stream is used to store all changes (purely event-sourced) and should be used by all other parties interested in changes. Configuration # Component activation # In order to activate Datapool Core component, please include the following dependency to your application <dependency> <groupId> io.holunda.taskpool </grouId> <artifactId> camunda-bpm-datapool-core </artifactId> <version> ${taskpool.version} </version> </dependency> and activate its configuration by adding the following to a Spring configuration: @Configuration @EnableDataPool class MyConfiguration Revision-Aware Projection # The in-memory data entry projection is supporting revision-aware projection queries. To activate this, you need to activate the correlation of revision attributes between your data entries commands and the data entry events. To do so, please activate the correlation provider by putting the following code snippet in the application containing the Datapool Core Component: @Configuration @EnableDataPool class MyConfiguration { @Bean fun revisionAwareCorrelationDataProvider (): CorrelationDataProvider { return MultiCorrelationDataProvider < CommandMessage < Any >> ( listOf ( MessageOriginProvider (), SimpleCorrelationDataProvider ( RevisionValue . REVISION_KEY ) ) ) } } By doing so, if a command is sending revision information, it will be passed to the resulting event and will be received by the projection, so the latter will deliver revision information in query results. The use of RevisionAwareQueryGateway will allow to query for specific revisions in the data entry projection, see documentation of axon-gateway-extension project.","title":"Datapool Core"},{"location":"reference-guide/components/core-datapool.html#datapool-core","text":"","title":"Datapool Core"},{"location":"reference-guide/components/core-datapool.html#purpose","text":"The component is responsible for maintaining and storing the consistent state of the datapool core concept of Business Data Entry. The component receives all commands and emits events, if changes are performed on underlying entities. The event stream is used to store all changes (purely event-sourced) and should be used by all other parties interested in changes.","title":"Purpose"},{"location":"reference-guide/components/core-datapool.html#configuration","text":"","title":"Configuration"},{"location":"reference-guide/components/core-datapool.html#component-activation","text":"In order to activate Datapool Core component, please include the following dependency to your application <dependency> <groupId> io.holunda.taskpool </grouId> <artifactId> camunda-bpm-datapool-core </artifactId> <version> ${taskpool.version} </version> </dependency> and activate its configuration by adding the following to a Spring configuration: @Configuration @EnableDataPool class MyConfiguration","title":"Component activation"},{"location":"reference-guide/components/core-datapool.html#revision-aware-projection","text":"The in-memory data entry projection is supporting revision-aware projection queries. To activate this, you need to activate the correlation of revision attributes between your data entries commands and the data entry events. To do so, please activate the correlation provider by putting the following code snippet in the application containing the Datapool Core Component: @Configuration @EnableDataPool class MyConfiguration { @Bean fun revisionAwareCorrelationDataProvider (): CorrelationDataProvider { return MultiCorrelationDataProvider < CommandMessage < Any >> ( listOf ( MessageOriginProvider (), SimpleCorrelationDataProvider ( RevisionValue . REVISION_KEY ) ) ) } } By doing so, if a command is sending revision information, it will be passed to the resulting event and will be received by the projection, so the latter will deliver revision information in query results. The use of RevisionAwareQueryGateway will allow to query for specific revisions in the data entry projection, see documentation of axon-gateway-extension project.","title":"Revision-Aware Projection"},{"location":"reference-guide/components/core-taskpool.html","text":"Taskpool Core # Purpose # The component is responsible for maintaining and storing the consistent state of the taskpool core concepts: Task (represents a user task instance) Process Definition (represents a process definition) The component receives all commands and emits events, if changes are performed on underlying entities. The event stream is used to store all changes (purely event-sourced) and should be used by all other parties interested in changes.","title":"Taskpool Core"},{"location":"reference-guide/components/core-taskpool.html#taskpool-core","text":"","title":"Taskpool Core"},{"location":"reference-guide/components/core-taskpool.html#purpose","text":"The component is responsible for maintaining and storing the consistent state of the taskpool core concepts: Task (represents a user task instance) Process Definition (represents a process definition) The component receives all commands and emits events, if changes are performed on underlying entities. The event stream is used to store all changes (purely event-sourced) and should be used by all other parties interested in changes.","title":"Purpose"},{"location":"reference-guide/components/other-tasklist-url-resolver.html","text":"Tasklist URL Resolver # Purpose # Configuration #","title":"Tasklist URL Resolver"},{"location":"reference-guide/components/other-tasklist-url-resolver.html#tasklist-url-resolver","text":"","title":"Tasklist URL Resolver"},{"location":"reference-guide/components/other-tasklist-url-resolver.html#purpose","text":"","title":"Purpose"},{"location":"reference-guide/components/other-tasklist-url-resolver.html#configuration","text":"","title":"Configuration"},{"location":"reference-guide/components/other-variable-serializer.html","text":"Variable Serializer # Purpose # Configuration #","title":"Variable Serializer"},{"location":"reference-guide/components/other-variable-serializer.html#variable-serializer","text":"","title":"Variable Serializer"},{"location":"reference-guide/components/other-variable-serializer.html#purpose","text":"","title":"Purpose"},{"location":"reference-guide/components/other-variable-serializer.html#configuration","text":"","title":"Configuration"},{"location":"reference-guide/components/view-cockpit.html","text":"Taskpool Cockpit View # Purpose # The View Cockpit is an example showing how an operational administration cockpit can be implemented and attached to Taskpool . It shows all task events known by the taskpool and is implemented as an isolated Spring Boot application.","title":"Cockpit View"},{"location":"reference-guide/components/view-cockpit.html#taskpool-cockpit-view","text":"","title":"Taskpool Cockpit View"},{"location":"reference-guide/components/view-cockpit.html#purpose","text":"The View Cockpit is an example showing how an operational administration cockpit can be implemented and attached to Taskpool . It shows all task events known by the taskpool and is implemented as an isolated Spring Boot application.","title":"Purpose"},{"location":"reference-guide/components/view-form-url-resolver.html","text":"Form URL Resolver # Purpose # Configuration #","title":"Form URL Resolver"},{"location":"reference-guide/components/view-form-url-resolver.html#form-url-resolver","text":"","title":"Form URL Resolver"},{"location":"reference-guide/components/view-form-url-resolver.html#purpose","text":"","title":"Purpose"},{"location":"reference-guide/components/view-form-url-resolver.html#configuration","text":"","title":"Configuration"},{"location":"reference-guide/components/view-mongo.html","text":"Mongo View # Purpose # The Mongo View is component responsible for creating read-projections of tasks and business data entries. It implements the Taskpool and Datapool View API and persists the projection as document collections in a Mongo database. Features # stores JSON document representation of enriched tasks, process definitions and business data entries provides single query API provides subscription query API (reactive) switchable subscription query API (AxonServer or MongoDB ChangeStream) Configuration options # In order to activate the Mongo implementation, please include the following dependency on your classpath: <dependency> <groupId> io.holunda.taskpool </groupId> <artifactId> camunda-bpm-taskpool-view-mongo </artifactId> <version> ${taskpool.version} </version> </dependency> The implementation relies on Spring Data Mongo and needs to activate those. Please add the following annotation to any class marked as Spring Configuration loaded during initialization: @Configuration @EnableTaskPoolMongoView @Import ({ org . springframework . boot . autoconfigure . mongo . MongoAutoConfiguration . class , org . springframework . boot . autoconfigure . mongo . MongoReactiveAutoConfiguration . class , org . springframework . boot . autoconfigure . data . mongo . MongoDataAutoConfiguration . class , org . springframework . boot . autoconfigure . data . mongo . MongoReactiveDataAutoConfiguration . class }) public class MyViewConfiguration { } In addition, configure a Mongo connection to database called tasks-payload using application.properties or application.yaml : spring: data: mongodb: database: tasks-payload host: localhost port: 27017 The view implementation provides runtime details using standard logging facility. If you want to increase the logging level, please setup it e.g. in your application.yaml : logging.level.io.holunda.camunda.taskpool.view.mongo: DEBUG Depending on your setup, you might want to use Axon Query Bus for subscription queries or not. MongoDB provides a change stream if run in a replication set. Using the property camunda.taskpool.view.mongo.change-tracking-mode you can control, whether you use subscription query based on Axon Query Bus (value EVENT_HANDLER , default) or based on Mongo Change Stream (value CHANGE_STREAM ). If you are not interested in publication of any subscription queries you might choose to disable it by setting the option to value NONE . Collections # The Mongo View uses several collections to store the results. These are: data-entries: collection for business data entries processes: collection for process definitions tasks: collection for user tasks tracking-tokens: collection for Axon Tracking Tokens Data Entries Collection # The data entries collection stores the business data entries in a uniform Datapool format. Here is an example: { \"_id\" : \"io.holunda.camunda.taskpool.example.ApprovalRequest#2db47ced-83d4-4c74-a644-44dd738935f8\" , \"entryType\" : \"io.holunda.camunda.taskpool.example.ApprovalRequest\" , \"payload\" : { \"amount\" : \"900.00\" , \"subject\" : \"Advanced training\" , \"currency\" : \"EUR\" , \"id\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"applicant\" : \"hulk\" }, \"correlations\" : {}, \"type\" : \"Approval Request\" , \"name\" : \"AR 2db47ced-83d4-4c74-a644-44dd738935f8\" , \"applicationName\" : \"example-process-approval\" , \"description\" : \"Advanced training\" , \"state\" : \"Submitted\" , \"statusType\" : \"IN_PROGRESS\" , \"authorizedUsers\" : [ \"gonzo\" , \"hulk\" ], \"authorizedGroups\" : [], \"protocol\" : [ { \"time\" : ISODa te ( \"2019-08-21T09:12:54.779Z\" ) , \"statusType\" : \"PRELIMINARY\" , \"state\" : \"Draft\" , \"username\" : \"gonzo\" , \"logMessage\" : \"Draft created.\" , \"logDetails\" : \"Request draft on behalf of hulk created.\" }, { \"time\" : ISODa te ( \"2019-08-21T09:12:55.060Z\" ) , \"statusType\" : \"IN_PROGRESS\" , \"state\" : \"Submitted\" , \"username\" : \"gonzo\" , \"logMessage\" : \"New approval request submitted.\" } ] } Tasks Collections # Tasks are stored in the following format (an example): { \"_id\" : \"dc1abe54-c3f3-11e9-86e8-4ab58cfe8f17\" , \"sourceReference\" : { \"_id\" : \"dc173bca-c3f3-11e9-86e8-4ab58cfe8f17\" , \"executionId\" : \"dc1a9742-c3f3-11e9-86e8-4ab58cfe8f17\" , \"definitionId\" : \"process_approve_request:1:91f2ff26-a64b-11e9-b117-3e6d125b91e2\" , \"definitionKey\" : \"process_approve_request\" , \"name\" : \"Request Approval\" , \"applicationName\" : \"example-process-approval\" , \"_class\" : \"process\" }, \"taskDefinitionKey\" : \"user_approve_request\" , \"payload\" : { \"request\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"originator\" : \"gonzo\" }, \"correlations\" : { \"io:holunda:camunda:taskpool:example:ApprovalRequest\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"io:holunda:camunda:taskpool:example:User\" : \"gonzo\" }, \"dataEntriesRefs\" : [ \"io.holunda.camunda.taskpool.example.ApprovalRequest#2db47ced-83d4-4c74-a644-44dd738935f8\" , \"io.holunda.camunda.taskpool.example.User#gonzo\" ], \"businessKey\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"name\" : \"Approve Request\" , \"description\" : \"Please approve request 2db47ced-83d4-4c74-a644-44dd738935f8 from gonzo on behalf of hulk.\" , \"formKey\" : \"approve-request\" , \"priority\" : 23 , \"createTime\" : ISODa te ( \"2019-08-21T09:12:54.872Z\" ) , \"candidateUsers\" : [ \"fozzy\" , \"gonzo\" ], \"candidateGroups\" : [], \"dueDate\" : ISODa te ( \"2019-06-26T07:55:00.000Z\" ) , \"followUpDate\" : ISODa te ( \"2023-06-26T07:55:00.000Z\" ) , \"deleted\" : false } Process Collection # Process definition collection allows for storage of startable process definitions, deployed in a Camunda Engine. This information is in particular interesting, if you are building a process-starter component and want to react dynamically on processes deployed in your landscape. { \"_id\" : \"process_approve_request:1:91f2ff26-a64b-11e9-b117-3e6d125b91e2\" , \"processDefinitionKey\" : \"process_approve_request\" , \"processDefinitionVersion\" : 1 , \"applicationName\" : \"example-process-approval\" , \"processName\" : \"Request Approval\" , \"processDescription\" : \"This is a wonderful process.\" , \"formKey\" : \"start-approval\" , \"startableFromTasklist\" : true , \"candidateStarterUsers\" : [], \"candidateStarterGroups\" : [ \"muppetshow\" , \"avengers\" ] } Tracking Token Collection # The Axon Tracking Token reflects the index of the event processed by the Mongo View and is stored in the following format: { \"_id\" : Objec t Id( \"5d2b45d6a9ca33042abea23b\" ) , \"processorName\" : \"io.holunda.camunda.taskpool.view.mongo.service\" , \"segment\" : 0 , \"owner\" : \"18524@blackstar\" , \"timestamp\" : NumberLo n g( 1566379093564 ) , \"token\" : { \"$binary\" : \"PG9yZy5heG9uZnJhbWV3b3JrLmV2ZW50aGFuZGxpbmcuR2xvYmFsU2VxdWVuY2VUcmFja2luZ1Rva2VuPjxnbG9iYWxJbmRleD40NDwvZ2xvYmFsSW5kZXg+PC9vcmcuYXhvbmZyYW1ld29yay5ldmVudGhhbmRsaW5nLkdsb2JhbFNlcXVlbmNlVHJhY2tpbmdUb2tlbj4=\" , \"$type\" : \"00\" }, \"tokenType\" : \"org.axonframework.eventhandling.GlobalSequenceTrackingToken\" }","title":"Mongo DB View"},{"location":"reference-guide/components/view-mongo.html#mongo-view","text":"","title":"Mongo View"},{"location":"reference-guide/components/view-mongo.html#purpose","text":"The Mongo View is component responsible for creating read-projections of tasks and business data entries. It implements the Taskpool and Datapool View API and persists the projection as document collections in a Mongo database.","title":"Purpose"},{"location":"reference-guide/components/view-mongo.html#features","text":"stores JSON document representation of enriched tasks, process definitions and business data entries provides single query API provides subscription query API (reactive) switchable subscription query API (AxonServer or MongoDB ChangeStream)","title":"Features"},{"location":"reference-guide/components/view-mongo.html#configuration-options","text":"In order to activate the Mongo implementation, please include the following dependency on your classpath: <dependency> <groupId> io.holunda.taskpool </groupId> <artifactId> camunda-bpm-taskpool-view-mongo </artifactId> <version> ${taskpool.version} </version> </dependency> The implementation relies on Spring Data Mongo and needs to activate those. Please add the following annotation to any class marked as Spring Configuration loaded during initialization: @Configuration @EnableTaskPoolMongoView @Import ({ org . springframework . boot . autoconfigure . mongo . MongoAutoConfiguration . class , org . springframework . boot . autoconfigure . mongo . MongoReactiveAutoConfiguration . class , org . springframework . boot . autoconfigure . data . mongo . MongoDataAutoConfiguration . class , org . springframework . boot . autoconfigure . data . mongo . MongoReactiveDataAutoConfiguration . class }) public class MyViewConfiguration { } In addition, configure a Mongo connection to database called tasks-payload using application.properties or application.yaml : spring: data: mongodb: database: tasks-payload host: localhost port: 27017 The view implementation provides runtime details using standard logging facility. If you want to increase the logging level, please setup it e.g. in your application.yaml : logging.level.io.holunda.camunda.taskpool.view.mongo: DEBUG Depending on your setup, you might want to use Axon Query Bus for subscription queries or not. MongoDB provides a change stream if run in a replication set. Using the property camunda.taskpool.view.mongo.change-tracking-mode you can control, whether you use subscription query based on Axon Query Bus (value EVENT_HANDLER , default) or based on Mongo Change Stream (value CHANGE_STREAM ). If you are not interested in publication of any subscription queries you might choose to disable it by setting the option to value NONE .","title":"Configuration options"},{"location":"reference-guide/components/view-mongo.html#collections","text":"The Mongo View uses several collections to store the results. These are: data-entries: collection for business data entries processes: collection for process definitions tasks: collection for user tasks tracking-tokens: collection for Axon Tracking Tokens","title":"Collections"},{"location":"reference-guide/components/view-mongo.html#data-entries-collection","text":"The data entries collection stores the business data entries in a uniform Datapool format. Here is an example: { \"_id\" : \"io.holunda.camunda.taskpool.example.ApprovalRequest#2db47ced-83d4-4c74-a644-44dd738935f8\" , \"entryType\" : \"io.holunda.camunda.taskpool.example.ApprovalRequest\" , \"payload\" : { \"amount\" : \"900.00\" , \"subject\" : \"Advanced training\" , \"currency\" : \"EUR\" , \"id\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"applicant\" : \"hulk\" }, \"correlations\" : {}, \"type\" : \"Approval Request\" , \"name\" : \"AR 2db47ced-83d4-4c74-a644-44dd738935f8\" , \"applicationName\" : \"example-process-approval\" , \"description\" : \"Advanced training\" , \"state\" : \"Submitted\" , \"statusType\" : \"IN_PROGRESS\" , \"authorizedUsers\" : [ \"gonzo\" , \"hulk\" ], \"authorizedGroups\" : [], \"protocol\" : [ { \"time\" : ISODa te ( \"2019-08-21T09:12:54.779Z\" ) , \"statusType\" : \"PRELIMINARY\" , \"state\" : \"Draft\" , \"username\" : \"gonzo\" , \"logMessage\" : \"Draft created.\" , \"logDetails\" : \"Request draft on behalf of hulk created.\" }, { \"time\" : ISODa te ( \"2019-08-21T09:12:55.060Z\" ) , \"statusType\" : \"IN_PROGRESS\" , \"state\" : \"Submitted\" , \"username\" : \"gonzo\" , \"logMessage\" : \"New approval request submitted.\" } ] }","title":"Data Entries Collection"},{"location":"reference-guide/components/view-mongo.html#tasks-collections","text":"Tasks are stored in the following format (an example): { \"_id\" : \"dc1abe54-c3f3-11e9-86e8-4ab58cfe8f17\" , \"sourceReference\" : { \"_id\" : \"dc173bca-c3f3-11e9-86e8-4ab58cfe8f17\" , \"executionId\" : \"dc1a9742-c3f3-11e9-86e8-4ab58cfe8f17\" , \"definitionId\" : \"process_approve_request:1:91f2ff26-a64b-11e9-b117-3e6d125b91e2\" , \"definitionKey\" : \"process_approve_request\" , \"name\" : \"Request Approval\" , \"applicationName\" : \"example-process-approval\" , \"_class\" : \"process\" }, \"taskDefinitionKey\" : \"user_approve_request\" , \"payload\" : { \"request\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"originator\" : \"gonzo\" }, \"correlations\" : { \"io:holunda:camunda:taskpool:example:ApprovalRequest\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"io:holunda:camunda:taskpool:example:User\" : \"gonzo\" }, \"dataEntriesRefs\" : [ \"io.holunda.camunda.taskpool.example.ApprovalRequest#2db47ced-83d4-4c74-a644-44dd738935f8\" , \"io.holunda.camunda.taskpool.example.User#gonzo\" ], \"businessKey\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"name\" : \"Approve Request\" , \"description\" : \"Please approve request 2db47ced-83d4-4c74-a644-44dd738935f8 from gonzo on behalf of hulk.\" , \"formKey\" : \"approve-request\" , \"priority\" : 23 , \"createTime\" : ISODa te ( \"2019-08-21T09:12:54.872Z\" ) , \"candidateUsers\" : [ \"fozzy\" , \"gonzo\" ], \"candidateGroups\" : [], \"dueDate\" : ISODa te ( \"2019-06-26T07:55:00.000Z\" ) , \"followUpDate\" : ISODa te ( \"2023-06-26T07:55:00.000Z\" ) , \"deleted\" : false }","title":"Tasks Collections"},{"location":"reference-guide/components/view-mongo.html#process-collection","text":"Process definition collection allows for storage of startable process definitions, deployed in a Camunda Engine. This information is in particular interesting, if you are building a process-starter component and want to react dynamically on processes deployed in your landscape. { \"_id\" : \"process_approve_request:1:91f2ff26-a64b-11e9-b117-3e6d125b91e2\" , \"processDefinitionKey\" : \"process_approve_request\" , \"processDefinitionVersion\" : 1 , \"applicationName\" : \"example-process-approval\" , \"processName\" : \"Request Approval\" , \"processDescription\" : \"This is a wonderful process.\" , \"formKey\" : \"start-approval\" , \"startableFromTasklist\" : true , \"candidateStarterUsers\" : [], \"candidateStarterGroups\" : [ \"muppetshow\" , \"avengers\" ] }","title":"Process Collection"},{"location":"reference-guide/components/view-mongo.html#tracking-token-collection","text":"The Axon Tracking Token reflects the index of the event processed by the Mongo View and is stored in the following format: { \"_id\" : Objec t Id( \"5d2b45d6a9ca33042abea23b\" ) , \"processorName\" : \"io.holunda.camunda.taskpool.view.mongo.service\" , \"segment\" : 0 , \"owner\" : \"18524@blackstar\" , \"timestamp\" : NumberLo n g( 1566379093564 ) , \"token\" : { \"$binary\" : \"PG9yZy5heG9uZnJhbWV3b3JrLmV2ZW50aGFuZGxpbmcuR2xvYmFsU2VxdWVuY2VUcmFja2luZ1Rva2VuPjxnbG9iYWxJbmRleD40NDwvZ2xvYmFsSW5kZXg+PC9vcmcuYXhvbmZyYW1ld29yay5ldmVudGhhbmRsaW5nLkdsb2JhbFNlcXVlbmNlVHJhY2tpbmdUb2tlbj4=\" , \"$type\" : \"00\" }, \"tokenType\" : \"org.axonframework.eventhandling.GlobalSequenceTrackingToken\" }","title":"Tracking Token Collection"},{"location":"reference-guide/components/view-simple.html","text":"In-Memory View # The In-Memory View is component responsible for creating read-projections of tasks and business data entries. It implements the Taskpool and Datapool View API and persists the projection in memory. The projection is transient and relies on event replay on every application start. It is good for demonstration purposes if the number of events is manageable small, but will fail to delivery high performance results on a large number of items. Features # uses concurrent hash maps to store the read model provides single query API provides subscription query API (reactive) relies on event replay and transient token store Configuration options # In order to activate the in-memory implementation, please include the following dependency on your classpath: <dependency> <groupId> io.holunda.taskpool </groupId> <artifactId> camunda-bpm-taskpool-view-simple </artifactId> <version> ${taskpool.version} </version> </dependency> Then, add the following annotation to any class marked as Spring Configuration loaded during initialization: @Configuration @EnableTaskPoolSimpleView public class MyViewConfiguration { } The view implementation provides runtime details using standard logging facility. If you want to increase the logging level, please setup it e.g. in your application.yaml : logging.level.io.holunda.camunda.taskpool.view.simple: DEBUG","title":"In-Memory View"},{"location":"reference-guide/components/view-simple.html#in-memory-view","text":"The In-Memory View is component responsible for creating read-projections of tasks and business data entries. It implements the Taskpool and Datapool View API and persists the projection in memory. The projection is transient and relies on event replay on every application start. It is good for demonstration purposes if the number of events is manageable small, but will fail to delivery high performance results on a large number of items.","title":"In-Memory View"},{"location":"reference-guide/components/view-simple.html#features","text":"uses concurrent hash maps to store the read model provides single query API provides subscription query API (reactive) relies on event replay and transient token store","title":"Features"},{"location":"reference-guide/components/view-simple.html#configuration-options","text":"In order to activate the in-memory implementation, please include the following dependency on your classpath: <dependency> <groupId> io.holunda.taskpool </groupId> <artifactId> camunda-bpm-taskpool-view-simple </artifactId> <version> ${taskpool.version} </version> </dependency> Then, add the following annotation to any class marked as Spring Configuration loaded during initialization: @Configuration @EnableTaskPoolSimpleView public class MyViewConfiguration { } The view implementation provides runtime details using standard logging facility. If you want to increase the logging level, please setup it e.g. in your application.yaml : logging.level.io.holunda.camunda.taskpool.view.simple: DEBUG","title":"Configuration options"},{"location":"reference-guide/configuration/index.html","text":"This is a root of configuration reference guide.","title":"Configuration Overview"}]}