{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Why should I use this? # You are building a process application or platform that comprises multiple process engines? You are building a custom task list for your Camunda Platform 7 process application? You want to include custom business object attributes to your tasks in the task list? You have performance issues with your task list? You need to provide a view on the business objects processed by your processes? You want a customized, business-driven audit log for your processes and changes to the business objects? If you can answer one of the previous questions with yes, Polyflow's libraries might help you. How to start? # We provide documentation for different people and different tasks. A good starting point is the Introduction . You might want to look at Reference Guide containing a Working Example and details about Usage Scenarios . Get in touch # If you are missing a feature, have a question regarding usage or deployment, you should definitely get in touch with us. There are various ways to do so:","title":"Home"},{"location":"index.html#why-should-i-use-this","text":"You are building a process application or platform that comprises multiple process engines? You are building a custom task list for your Camunda Platform 7 process application? You want to include custom business object attributes to your tasks in the task list? You have performance issues with your task list? You need to provide a view on the business objects processed by your processes? You want a customized, business-driven audit log for your processes and changes to the business objects? If you can answer one of the previous questions with yes, Polyflow's libraries might help you.","title":"Why should I use this?"},{"location":"index.html#how-to-start","text":"We provide documentation for different people and different tasks. A good starting point is the Introduction . You might want to look at Reference Guide containing a Working Example and details about Usage Scenarios .","title":"How to start?"},{"location":"index.html#get-in-touch","text":"If you are missing a feature, have a question regarding usage or deployment, you should definitely get in touch with us. There are various ways to do so:","title":"Get in touch"},{"location":"developer-guide/contribution.html","text":"There are several ways in which you may contribute to this project. File issues Submit a pull requests Found a bug or missing feature? # Please file an issue in our issue tracking system. Submit a Pull Request # If you found a solution to an open issue and implemented it, we would be happy to add your contribution in the code base. For doing so, please create a pull request. Prior to that, please make sure you: rebase against the develop branch stick to project coding conventions added test cases for the problem you are solving added docs, describing the change generally comply with codacy report","title":"Contribution"},{"location":"developer-guide/contribution.html#found-a-bug-or-missing-feature","text":"Please file an issue in our issue tracking system.","title":"Found a bug or missing feature?"},{"location":"developer-guide/contribution.html#submit-a-pull-request","text":"If you found a solution to an open issue and implemented it, we would be happy to add your contribution in the code base. For doing so, please create a pull request. Prior to that, please make sure you: rebase against the develop branch stick to project coding conventions added test cases for the problem you are solving added docs, describing the change generally comply with codacy report","title":"Submit a Pull Request"},{"location":"developer-guide/project-setup.html","text":"If you are interested in developing and building the project please follow the following instruction. Version control # To get sources of the project, please execute: git clone https://github.com/holunda-io/camunda-bpm-taskpool.git cd camunda-bpm-taskpool We are using gitflow in our git SCM. That means that you should start from develop branch, create a feature/<name> out of it and once it is completed create a pull request containing it. Please squash your commits before submitting and use semantic commit messages, if possible. Project Build # Perform the following steps to get a development setup up and running. ./mvnw clean install Integration Tests # By default, the build command will ignore the run of failsafe Maven plugin executing the integration tests (usual JUnit tests with class names ending with ITest). In order to run integration tests, please call from your command line: ./mvnw integration-test failsafe:verify -Pitest -DskipFrontend Project build modes and profiles # Camunda Version # You can choose the used Camunda version by specifying the profile camunda-ee or camunda-ce . The default version is a Community Edition. Specify -Pcamunda-ee to switch to Camunda Enterprise edition. This will require a valid Camunda license. You can put it into a file ~/.camunda/license.txt and it will be detected automatically. Skip Frontend # Note Components for production use of Polyflow are backend components only. Frontend components are only created for examples and demonstration purpose. If you are interested in backend only, specify the -DskipFrontend switch. This will accelerate the build significantly. Build Documentation # We are using MkDocs for generation of a static site documentation and rely on Markdown as much as possible. MkDocs is a written in Python 3 and needs to be installed on your machine. For the installation please run the following command from your command line: python3 -m pip install --upgrade pip python3 -m pip install -r ./docs/requirements.txt For creation of documentation, please run: mkdocs build The docs are generated into site directory. Note If you want to develop your docs in 'live' mode, run mkdocs serve and access the http://localhost:8000/ from your browser. Examples # Polyflow provides a series of examples demonstrating different features of the library. By default, the examples are built during the project build. If you want to skip the examples, please add the following parameter to your command line or disable the examples module in your IDE. ./mvnw clean package -DskipExamples Local Start # Important If you want to run examples locally, you will need docker and docker-compose . Pre-requirements # Before starting the example applications, make sure the required infrastructure is set up and running. Please run the following from your command line: ./.docker/setup.sh This will create required docker volumes and network. Start containers # In order to operate, the distributed example applications will require several containers. These are: Axon Server PostgreSQL Database Mongo Database (if used in projection) Please start the required containers executing the corresponding command from examples/scenarios/distributed-axon-server : cd ./examples/scenarios/distributed-axon-server docker-compose up Starting application (distributed scenario) # For the distributed scenario, the containers from the previous section needs to be started. To start applications, either use your IDE and create two run configurations for the classes (in this order): io.holunda.polyflow.example.process.platform.ExampleTaskpoolApplicationDistributedWithAxonServer io.holunda.polyflow.example.process.approval.ExampleProcessApplicationDistributedWithAxonServer Alternatively, you can run them from the command line: ./mvnw spring-boot:run -f examples/scenarios/distributed-axon-server/taskpool-application ./mvnw spring-boot:run -f examples/scenarios/distributed-axon-server/process-application Continuous Integration # Travis CI is building all branches on commit hook. In addition, a private-hosted Jenkins CI is used to build the releases. Release Management # Release management has been setup for use of Sonatype Nexus (= Maven Central) What modules get deployed to repository # Every module is enabled by default. If you want to change this, please provide the property <maven.deploy.skip> true </maven.deploy.skip> inside the corresponding pom.xml . Currently, all examples are EXCLUDED from publication into Maven Central. Trigger new release # Warning This operation requires special permissions. We use gitflow for development (see A successful git branching model for more details). You could use gitflow with native git commands, but then you would have to change the versions in the poms manually. Therefore, we use the mvn gitflow plugin , which handles this and other things nicely. You can build a release with: ./mvnw gitflow:release-start ./mvnw gitflow:release-finish This will update the versions in the pom.xml s accordingly and push the release tag to the master branch and update the develop branch for the new development version. Trigger a deploy # Warning This operation requires special permissions. Currently, CI allows for deployment of artifacts to Maven Central and is executed using github actions. This means, that a push to master branch will start the corresponding build job, and if successful the artifacts will get into Staging Repositories of OSS Sonatype without manual intervention. Run deploy from local machine # Warning This operation requires special permissions. If you still want to execute the deployment from your local machine, you need to have GPG keys at place and to execute the following command on the master branch: export GPG_KEYNAME = \"<keyname>\" export GPG_PASSPHRASE = \"<secret>\" ./mvnw clean deploy -B -DskipTests -DskipExamples -Prelease -Dgpg.keyname = $GPG_KEYNAME -Dgpg.passphrase = $GPG_PASSPHRASE Release to public repositories # Warning This operation requires special permissions. The deployment job will publish the artifacts to Nexus OSS staging repositories. Currently, all snapshots get into OSS Sonatype Snapshot repository and all releases to Maven Central automatically.","title":"Project Setup"},{"location":"developer-guide/project-setup.html#version-control","text":"To get sources of the project, please execute: git clone https://github.com/holunda-io/camunda-bpm-taskpool.git cd camunda-bpm-taskpool We are using gitflow in our git SCM. That means that you should start from develop branch, create a feature/<name> out of it and once it is completed create a pull request containing it. Please squash your commits before submitting and use semantic commit messages, if possible.","title":"Version control"},{"location":"developer-guide/project-setup.html#project-build","text":"Perform the following steps to get a development setup up and running. ./mvnw clean install","title":"Project Build"},{"location":"developer-guide/project-setup.html#integration-tests","text":"By default, the build command will ignore the run of failsafe Maven plugin executing the integration tests (usual JUnit tests with class names ending with ITest). In order to run integration tests, please call from your command line: ./mvnw integration-test failsafe:verify -Pitest -DskipFrontend","title":"Integration Tests"},{"location":"developer-guide/project-setup.html#project-build-modes-and-profiles","text":"","title":"Project build modes and profiles"},{"location":"developer-guide/project-setup.html#camunda-version","text":"You can choose the used Camunda version by specifying the profile camunda-ee or camunda-ce . The default version is a Community Edition. Specify -Pcamunda-ee to switch to Camunda Enterprise edition. This will require a valid Camunda license. You can put it into a file ~/.camunda/license.txt and it will be detected automatically.","title":"Camunda Version"},{"location":"developer-guide/project-setup.html#skip-frontend","text":"Note Components for production use of Polyflow are backend components only. Frontend components are only created for examples and demonstration purpose. If you are interested in backend only, specify the -DskipFrontend switch. This will accelerate the build significantly.","title":"Skip Frontend"},{"location":"developer-guide/project-setup.html#build-documentation","text":"We are using MkDocs for generation of a static site documentation and rely on Markdown as much as possible. MkDocs is a written in Python 3 and needs to be installed on your machine. For the installation please run the following command from your command line: python3 -m pip install --upgrade pip python3 -m pip install -r ./docs/requirements.txt For creation of documentation, please run: mkdocs build The docs are generated into site directory. Note If you want to develop your docs in 'live' mode, run mkdocs serve and access the http://localhost:8000/ from your browser.","title":"Build Documentation"},{"location":"developer-guide/project-setup.html#examples","text":"Polyflow provides a series of examples demonstrating different features of the library. By default, the examples are built during the project build. If you want to skip the examples, please add the following parameter to your command line or disable the examples module in your IDE. ./mvnw clean package -DskipExamples","title":"Examples"},{"location":"developer-guide/project-setup.html#local-start","text":"Important If you want to run examples locally, you will need docker and docker-compose .","title":"Local Start"},{"location":"developer-guide/project-setup.html#pre-requirements","text":"Before starting the example applications, make sure the required infrastructure is set up and running. Please run the following from your command line: ./.docker/setup.sh This will create required docker volumes and network.","title":"Pre-requirements"},{"location":"developer-guide/project-setup.html#start-containers","text":"In order to operate, the distributed example applications will require several containers. These are: Axon Server PostgreSQL Database Mongo Database (if used in projection) Please start the required containers executing the corresponding command from examples/scenarios/distributed-axon-server : cd ./examples/scenarios/distributed-axon-server docker-compose up","title":"Start containers"},{"location":"developer-guide/project-setup.html#starting-application-distributed-scenario","text":"For the distributed scenario, the containers from the previous section needs to be started. To start applications, either use your IDE and create two run configurations for the classes (in this order): io.holunda.polyflow.example.process.platform.ExampleTaskpoolApplicationDistributedWithAxonServer io.holunda.polyflow.example.process.approval.ExampleProcessApplicationDistributedWithAxonServer Alternatively, you can run them from the command line: ./mvnw spring-boot:run -f examples/scenarios/distributed-axon-server/taskpool-application ./mvnw spring-boot:run -f examples/scenarios/distributed-axon-server/process-application","title":"Starting application (distributed scenario)"},{"location":"developer-guide/project-setup.html#continuous-integration","text":"Travis CI is building all branches on commit hook. In addition, a private-hosted Jenkins CI is used to build the releases.","title":"Continuous Integration"},{"location":"developer-guide/project-setup.html#release-management","text":"Release management has been setup for use of Sonatype Nexus (= Maven Central)","title":"Release Management"},{"location":"developer-guide/project-setup.html#what-modules-get-deployed-to-repository","text":"Every module is enabled by default. If you want to change this, please provide the property <maven.deploy.skip> true </maven.deploy.skip> inside the corresponding pom.xml . Currently, all examples are EXCLUDED from publication into Maven Central.","title":"What modules get deployed to repository"},{"location":"developer-guide/project-setup.html#trigger-new-release","text":"Warning This operation requires special permissions. We use gitflow for development (see A successful git branching model for more details). You could use gitflow with native git commands, but then you would have to change the versions in the poms manually. Therefore, we use the mvn gitflow plugin , which handles this and other things nicely. You can build a release with: ./mvnw gitflow:release-start ./mvnw gitflow:release-finish This will update the versions in the pom.xml s accordingly and push the release tag to the master branch and update the develop branch for the new development version.","title":"Trigger new release"},{"location":"developer-guide/project-setup.html#trigger-a-deploy","text":"Warning This operation requires special permissions. Currently, CI allows for deployment of artifacts to Maven Central and is executed using github actions. This means, that a push to master branch will start the corresponding build job, and if successful the artifacts will get into Staging Repositories of OSS Sonatype without manual intervention.","title":"Trigger a deploy"},{"location":"developer-guide/project-setup.html#run-deploy-from-local-machine","text":"Warning This operation requires special permissions. If you still want to execute the deployment from your local machine, you need to have GPG keys at place and to execute the following command on the master branch: export GPG_KEYNAME = \"<keyname>\" export GPG_PASSPHRASE = \"<secret>\" ./mvnw clean deploy -B -DskipTests -DskipExamples -Prelease -Dgpg.keyname = $GPG_KEYNAME -Dgpg.passphrase = $GPG_PASSPHRASE","title":"Run deploy from local machine"},{"location":"developer-guide/project-setup.html#release-to-public-repositories","text":"Warning This operation requires special permissions. The deployment job will publish the artifacts to Nexus OSS staging repositories. Currently, all snapshots get into OSS Sonatype Snapshot repository and all releases to Maven Central automatically.","title":"Release to public repositories"},{"location":"examples/index.html","text":"We provide the following examples to demonstrate the features of Polyflow: Example Approval","title":"Examples Overview"},{"location":"examples/example-approval.html","text":"Along with library modules several example modules and applications are provided, demonstrating the main features of the solution. This includes a series of example applications for usage in different Usage Scenarios . They all share the same business process described in the next section. Business context: Approval # Take a look on the process model above. Imagine you are building a system that responsible for management of all approval requests in the company. Using this system, you can submit requests which then get eventually approved or rejected. Sometimes, the approver doesn't approve or reject, but returns the request back to the originator for correction (that is the person, who submitted the request). Then, the originator can amend the request and resubmit it or cancel the request. An approval request is modelled in the following way. The subject describes what the request is about, the applicant is the person whom it is about (can be different from originator ). Finally, the amount and currency denote the costs of the request. All requests must be stored for compliance purposes. The request is initially created in DRAFT mode. It gets to state IN PROGRESS as soon as the process is started and will eventually get to ACCEPTED or REJECTED as a final state. For sample purposes two groups of users are created: The Muppet Show ( Kermit , Piggy , Gonzo and Fozzy ) and The Avengers ( Ironman , Hulk ). Gonzo and Fozzy are responsible for approvals. Process Run # Let's play the following run through this process model. Ironman submits an Advanced Training request on behalf of Hulk The request costs are provided in wrong currency and Gonzo returns the request to Ironman for correction (EUR instead of USD) Ironman changes the currency to USD and re-submits the request Gonzo is out of office, so Fozzy takes over and approves the request Running Examples # To run the example please consult the Usage Scenarios section. Note Since the process application includes Camunda BPM engine, you can use the standard Camunda webapps by navigating to http://localhost:8080/camunda/app/ . The default user and password are admin / admin . Story board # The following storyboard can be used to understand the mechanics behind the provided implementation: TIP: In this storyboard, we assume you started the single node scenario and the application runs locally on http://localhost:8080. Please adjust the URLs accordingly, if you started differently. To start the approval process for a given request open your browser and navigate to the Example Tasklist : http://localhost:8080/polyflow/ . Please note that the selected user is Ironman . Open the menu ( Start new... ) in and select 'Request Approval'. You should see the start form for the example approval process. Select Advanced Training from one of predefined templates and click Start . The start form will disappear and redirect back to the empty Tasklist . Since you are still acting as Ironman there are nothing you can do here. Please switch the user to Gonzo in the top right corner and you should see the user task Approve Request from process Request Approval . Examine the task details by clicking Data tab in Details column. You can see the data of the request correlated to the current process instance. Click on the task name and you will see the user task form of the Approve Request task. Select the option Return request to originator and click complete. Switch to Workpieces and you should see the request business object. Examine the approval request by clicking Data , Audit and Description tabs in Details column. Change user back to Ironman and switch back to the Tasklist and open the Amend request task. Change the currency to USD and re-submit the request. Change user back to Fozzy , open the Approve Request task and approve the request by selecting the appropriate option. Switch to Workpieces and you should still see the request business object, even after the process is finished. Examine the approval request by clicking Data , Audit and Description tabs in Details column.","title":"Example Approval"},{"location":"examples/example-approval.html#business-context-approval","text":"Take a look on the process model above. Imagine you are building a system that responsible for management of all approval requests in the company. Using this system, you can submit requests which then get eventually approved or rejected. Sometimes, the approver doesn't approve or reject, but returns the request back to the originator for correction (that is the person, who submitted the request). Then, the originator can amend the request and resubmit it or cancel the request. An approval request is modelled in the following way. The subject describes what the request is about, the applicant is the person whom it is about (can be different from originator ). Finally, the amount and currency denote the costs of the request. All requests must be stored for compliance purposes. The request is initially created in DRAFT mode. It gets to state IN PROGRESS as soon as the process is started and will eventually get to ACCEPTED or REJECTED as a final state. For sample purposes two groups of users are created: The Muppet Show ( Kermit , Piggy , Gonzo and Fozzy ) and The Avengers ( Ironman , Hulk ). Gonzo and Fozzy are responsible for approvals.","title":"Business context: Approval"},{"location":"examples/example-approval.html#process-run","text":"Let's play the following run through this process model. Ironman submits an Advanced Training request on behalf of Hulk The request costs are provided in wrong currency and Gonzo returns the request to Ironman for correction (EUR instead of USD) Ironman changes the currency to USD and re-submits the request Gonzo is out of office, so Fozzy takes over and approves the request","title":"Process Run"},{"location":"examples/example-approval.html#running-examples","text":"To run the example please consult the Usage Scenarios section. Note Since the process application includes Camunda BPM engine, you can use the standard Camunda webapps by navigating to http://localhost:8080/camunda/app/ . The default user and password are admin / admin .","title":"Running Examples"},{"location":"examples/example-approval.html#story-board","text":"The following storyboard can be used to understand the mechanics behind the provided implementation: TIP: In this storyboard, we assume you started the single node scenario and the application runs locally on http://localhost:8080. Please adjust the URLs accordingly, if you started differently. To start the approval process for a given request open your browser and navigate to the Example Tasklist : http://localhost:8080/polyflow/ . Please note that the selected user is Ironman . Open the menu ( Start new... ) in and select 'Request Approval'. You should see the start form for the example approval process. Select Advanced Training from one of predefined templates and click Start . The start form will disappear and redirect back to the empty Tasklist . Since you are still acting as Ironman there are nothing you can do here. Please switch the user to Gonzo in the top right corner and you should see the user task Approve Request from process Request Approval . Examine the task details by clicking Data tab in Details column. You can see the data of the request correlated to the current process instance. Click on the task name and you will see the user task form of the Approve Request task. Select the option Return request to originator and click complete. Switch to Workpieces and you should see the request business object. Examine the approval request by clicking Data , Audit and Description tabs in Details column. Change user back to Ironman and switch back to the Tasklist and open the Amend request task. Change the currency to USD and re-submit the request. Change user back to Fozzy , open the Approve Request task and approve the request by selecting the appropriate option. Switch to Workpieces and you should still see the request business object, even after the process is finished. Examine the approval request by clicking Data , Audit and Description tabs in Details column.","title":"Story board"},{"location":"examples/example-components/index.html","text":"For demonstration purposes we built several example components and reuse them to demonstrate the Example application in different Usage Scenarios . These components are not part of a polyflow distribution and serve demonstration purposes only. They still show implementation of components which needs to be implemented by you and are not part of the provided library. If you are interested in the source code, please visit the Polyflow Repository . Process Application Example Components # To show the integration of Polyflow components into a Process Application, the process discussed in Example Approval application has been implemented. Process Application Frontend Process Application Backend Process Platform Example Components # To show the integration of Polyflow components into a Process Platform, a simple task list and a workpieces view (archive view for business objects) has been implemented. Process Platform Frontend Process Platform Backend Shared Example Components # Components used by other example components. User Management","title":"Example Components Overview"},{"location":"examples/example-components/index.html#process-application-example-components","text":"To show the integration of Polyflow components into a Process Application, the process discussed in Example Approval application has been implemented. Process Application Frontend Process Application Backend","title":"Process Application Example Components"},{"location":"examples/example-components/index.html#process-platform-example-components","text":"To show the integration of Polyflow components into a Process Platform, a simple task list and a workpieces view (archive view for business objects) has been implemented. Process Platform Frontend Process Platform Backend","title":"Process Platform Example Components"},{"location":"examples/example-components/index.html#shared-example-components","text":"Components used by other example components. User Management","title":"Shared Example Components"},{"location":"examples/example-components/pa-backend.html","text":"The process application backend is implementing the process described in the Example application . It demonstrates a typical three-tier application, following the Boundary-Control-Entity pattern. Boundary Tier # The REST API is defined using OpenAPI specification and is implemented by Spring MVC controllers. It defines of four logical parts: Environment Controller Request Controller Approve Task Controller Amend Task Controller Control Tier # The control tier is implemented using stateless Spring Beans and orchestrated by the Camunda BPM Process. Typical JavaDelegate and ExecutionListener are used as a glue layer. Business services of this layer are responsible for the integration with Datapool Components to reflect the status of the Request business entity. The Camunda BPM Engine is configured to use the TaskCollector to integrate with remaining components of the camunda-bpm-taskpool . Entity Tier # The entity tier is implemented using Spring Data JPA, using Hibernate entities. Application data and process engine data is stored using a RDBMS.","title":"Process Application Backend"},{"location":"examples/example-components/pa-backend.html#boundary-tier","text":"The REST API is defined using OpenAPI specification and is implemented by Spring MVC controllers. It defines of four logical parts: Environment Controller Request Controller Approve Task Controller Amend Task Controller","title":"Boundary Tier"},{"location":"examples/example-components/pa-backend.html#control-tier","text":"The control tier is implemented using stateless Spring Beans and orchestrated by the Camunda BPM Process. Typical JavaDelegate and ExecutionListener are used as a glue layer. Business services of this layer are responsible for the integration with Datapool Components to reflect the status of the Request business entity. The Camunda BPM Engine is configured to use the TaskCollector to integrate with remaining components of the camunda-bpm-taskpool .","title":"Control Tier"},{"location":"examples/example-components/pa-backend.html#entity-tier","text":"The entity tier is implemented using Spring Data JPA, using Hibernate entities. Application data and process engine data is stored using a RDBMS.","title":"Entity Tier"},{"location":"examples/example-components/pa-frontend.html","text":"The process application frontend is implementing the user task forms and business object views for the example application. It is built as a typical Angular Single Page Application (SPA) and provides views for both user tasks and the business objects. It communicates with process application backend via REST API, defined in the latter. The user primarily interacts with the process platform which seamlessly integrates with the process applications. Usually, it provides integration points for user-task embedding / UI-composition. Unfortunately, this topic strongly depends on the frontend technology and is not a subject we want to demonstrate in this example. For simplicity, we built a very simple example, skipping the UI composition / integration entirely. The navigation between the process platform and process application is just as simple as a full page navigation of a hyperlink.","title":"Process Application Frontend"},{"location":"examples/example-components/pp-backend.html","text":"The purpose of the example process platform backend is to demonstrate how process agnostic parts of the process solution can be implemented.","title":"Process Platform Backend"},{"location":"examples/example-components/pp-frontend.html","text":"The example process platform frontend provides example implementation of two views: Example Tasklist Example Workpieces List Example Tasklist # Example Tasklist is a simple implementation of task inbox for a single user. It provides the following features: Lists tasks in the system for selected user Allows for switching users (faking different user login for demonstration purposes) Tasks include information about the process, name, description, create time, due date, priority and assignment. Tasks include process data (from process instance) Tasks include correlated business data (correlated via variable from process instance) The list of tasks is sortable The list of tasks is pageable (7 items per page) Allows claiming / unclaiming Provides a deeplink to the user task form provided by the process application Allows starting new process instances Here is, how it looks like showing task descriptions: you can optionally show the business data correlated with user task: Example Workpieces List # The example workpieces list is provides a list of business objects / workpieces that are currently processed by the processes even after the process has already been finished. It provides the following features: Lists business objects in the system for selected user Allows for switching users (faking different user login for demonstration purposes) Business objects include information about the type, status (with sub status), name, details Business objects include details about contained data Business objects include audit log with all state changes The list is pageable (7 items per page) Business object view Provides a deeplink to the business object view provided by the process application Here is, how it looks like showing the audit log:","title":"Process Platform Frontend"},{"location":"examples/example-components/pp-frontend.html#example-tasklist","text":"Example Tasklist is a simple implementation of task inbox for a single user. It provides the following features: Lists tasks in the system for selected user Allows for switching users (faking different user login for demonstration purposes) Tasks include information about the process, name, description, create time, due date, priority and assignment. Tasks include process data (from process instance) Tasks include correlated business data (correlated via variable from process instance) The list of tasks is sortable The list of tasks is pageable (7 items per page) Allows claiming / unclaiming Provides a deeplink to the user task form provided by the process application Allows starting new process instances Here is, how it looks like showing task descriptions: you can optionally show the business data correlated with user task:","title":"Example Tasklist"},{"location":"examples/example-components/pp-frontend.html#example-workpieces-list","text":"The example workpieces list is provides a list of business objects / workpieces that are currently processed by the processes even after the process has already been finished. It provides the following features: Lists business objects in the system for selected user Allows for switching users (faking different user login for demonstration purposes) Business objects include information about the type, status (with sub status), name, details Business objects include details about contained data Business objects include audit log with all state changes The list is pageable (7 items per page) Business object view Provides a deeplink to the business object view provided by the process application Here is, how it looks like showing the audit log:","title":"Example Workpieces List"},{"location":"examples/example-components/user-management.html","text":"Usually, a central user management like a Single-Sign-On (SSO) is a part deployed into the process application landscape. This is responsible for authentication and authorization of the user and is required to control the role-based access to user tasks. In our example application, we disable any security checks to avoid the unneeded complexity. In doing so, we implemented a trivial user store holding some pre-defined users used in example components and allow to simply switch users from the frontend by choosing a different user from the drop-down list. It is integrated with the example frontends by passing around the user id along with the requests and provide a way to resolve the user by that id. Technically speaking, the user id can be used to retrieve the permissions of the user and check them in the backend.","title":"User Management"},{"location":"examples/scenarios/index.html","text":"Depending on your requirements and infrastructure available several deployment scenarios of the components is possible. The simplest setup is to run all components on a single node. A more advanced scenario is to distribute components and connect them. In doing so, one of the challenging issues for distribution and connecting microservices is a setup of messaging technology supporting required message exchange patterns (MEPs) for a CQRS system. Because of different semantics of commands, events and queries and additional requirements of event-sourced persistence a special implementation of command bus, event bus and event store is required. In particular, two scenarios can be distinguished: using Axon Server or using a different distribution technology. The provided Example application is implemented several times demonstrating the following usage scenarios: Single Node Scenario Distributed Scenario using Axon Server It is a good idea to understand the single node scenario first and then move on to more elaborated scenarios.","title":"Scenarios Overview"},{"location":"examples/scenarios/distributed-axon-server.html","text":"This example is demonstrating the usage of the Camunda BPM Taskpool with components distributed with help of Axon Server. It provides two applications for demonstration purposes: the process application and the process platform. Both applications are built as SpringBoot applications. The following configuration is used in the distributed scenario with Axon Server: Bus distribution is provided by Axon Server Connector (command bus, event bus, query bus) Axon Server is used as Event Store Postgresql is used as a database for: Camunda BPM Engine Process Application Datasource Mongo is used as persistence for projection view ( mongo-view ) System Requirements # JDK 11 Docker Docker Compose Preparations # Before you begin, please build the entire project with mvn clean install from the command line in the project root directory. You will need some backing services (Axon Server, PostgreSQL, MongoDB) and you can easily start them locally by using the provided docker-compose.yml file. Before you start change the directory to examples/scenarios/distributed-axon-server and run a preparation script .docker/setup.sh . You can do it with the following code from your command line (you need to do it once): cd examples/scenarios/distributed-axon-server .docker/setup.sh Now, start required containers. The easiest way to do so is to run: docker-compose up -d To verify it is running, open your browser http://localhost:8024/ . You should see the Axon Server administration console. Start # The demo application consists of several Maven modules. In order to start the example, you will need to start only two of them in the following order: taskpool-application (process platform) process-application (example process application) The modules can be started by running from command line in the examples/scenarios/distributed-axon-server directory using Maven or start the packaged application using: java -jar taskpool-application/target/*.jar java -jar process-application/target/*.jar Useful URLs # Process Platform # http://localhost:8081/polyflow/tasks http://localhost:8081/polyflow/archive http://localhost:8081/swagger-ui/ Process Application # http://localhost:8080/camunda/app/ http://localhost:8080/swagger-ui/","title":"Distributed using Axon Server"},{"location":"examples/scenarios/distributed-axon-server.html#system-requirements","text":"JDK 11 Docker Docker Compose","title":"System Requirements"},{"location":"examples/scenarios/distributed-axon-server.html#preparations","text":"Before you begin, please build the entire project with mvn clean install from the command line in the project root directory. You will need some backing services (Axon Server, PostgreSQL, MongoDB) and you can easily start them locally by using the provided docker-compose.yml file. Before you start change the directory to examples/scenarios/distributed-axon-server and run a preparation script .docker/setup.sh . You can do it with the following code from your command line (you need to do it once): cd examples/scenarios/distributed-axon-server .docker/setup.sh Now, start required containers. The easiest way to do so is to run: docker-compose up -d To verify it is running, open your browser http://localhost:8024/ . You should see the Axon Server administration console.","title":"Preparations"},{"location":"examples/scenarios/distributed-axon-server.html#start","text":"The demo application consists of several Maven modules. In order to start the example, you will need to start only two of them in the following order: taskpool-application (process platform) process-application (example process application) The modules can be started by running from command line in the examples/scenarios/distributed-axon-server directory using Maven or start the packaged application using: java -jar taskpool-application/target/*.jar java -jar process-application/target/*.jar","title":"Start"},{"location":"examples/scenarios/distributed-axon-server.html#useful-urls","text":"","title":"Useful URLs"},{"location":"examples/scenarios/distributed-axon-server.html#process-platform","text":"http://localhost:8081/polyflow/tasks http://localhost:8081/polyflow/archive http://localhost:8081/swagger-ui/","title":"Process Platform"},{"location":"examples/scenarios/distributed-axon-server.html#process-application","text":"http://localhost:8080/camunda/app/ http://localhost:8080/swagger-ui/","title":"Process Application"},{"location":"examples/scenarios/single-node.html","text":"This example demonstrates the usage of the Camunda BPM Taskpool deployed in one single node and is built as a SpringBoot application described in the Deployment section. System Requirements # JDK 11 Preparations # Before you begin, please build the entire project with ./mvnw clean install from the command line in the project root directory. Start # The demo application consists of one Maven module which can be started by running from command line in the examples/scenarios/single-node directory using Maven. Alternatively you can start the packaged application using: java -jar target/*.jar Useful URLs # http://localhost:8080/taskpool/ http://localhost:8080/swagger-ui/ http://localhost:8080/camunda/app/tasklist/default/","title":"Single Node"},{"location":"examples/scenarios/single-node.html#system-requirements","text":"JDK 11","title":"System Requirements"},{"location":"examples/scenarios/single-node.html#preparations","text":"Before you begin, please build the entire project with ./mvnw clean install from the command line in the project root directory.","title":"Preparations"},{"location":"examples/scenarios/single-node.html#start","text":"The demo application consists of one Maven module which can be started by running from command line in the examples/scenarios/single-node directory using Maven. Alternatively you can start the packaged application using: java -jar target/*.jar","title":"Start"},{"location":"examples/scenarios/single-node.html#useful-urls","text":"http://localhost:8080/taskpool/ http://localhost:8080/swagger-ui/ http://localhost:8080/camunda/app/tasklist/default/","title":"Useful URLs"},{"location":"getting-started/index.html","text":"This guide is describing steps required to configure an existing Camunda BPM Spring Boot Process Application and connect to existing Process Platform . Note The following steps assume that you have already choosen one of the distribution scenarios and setup the Core components . This is a pre-requirement for the following steps to work. Add dependency to Polyflow integration starter # Apart from the example application, you might be interested in integrating Polyflow Taskpool and Datapool into your existing application. To do so, you need to enable your Camunda BPM process engine to use the library. For doing so, add the polyflow-integration-camunda-bpm-engine-parent library. In Maven, add the following dependency to your pom.xml : <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-camunda-bpm-springboot-starter </artifactId> <version> ${polyflow.version} </version> </dependency> Activate Polyflow Support # Now, find your SpringBoot application class and add an annotation to it: @SpringBootApplication @EnableTaskpoolEngineSupport public class MyApplication { public static void main ( String ... args ) { SpringApplication . run ( MyApplication . class , args ); } } Configure your Polyflow provisioning # Finally, add the following block to your application.yml : camunda : bpm : default-serialization-format : application/json history-level : full polyflow : integration : client : camunda : application-name : ${spring.application.name} # default collector : camunda : application-name : ${spring.application.name} # default process-instance : enabled : true process-definition : enabled : true process-variable : enabled : true task : enabled : true enricher : type : processVariables sender : enabled : true data-entry : enabled : true type : simple application-name : ${spring.application.name} # default process-definition : enabled : true process-instance : enabled : true process-variable : enabled : true task : enabled : true type : tx send-within-transaction : true # Must be set to true in single node scenario. form-url-resolver : defaultTaskTemplate : \"/tasks/${formKey}/${id}?userId=%userId%\" defaultApplicationTemplate : \"http://localhost:${server.port}/${applicationName}\" defaultProcessTemplate : \"/${formKey}?userId=%userId%\" Now, start your process engine. If you run into a user task, you should see on the console how this is passed to task pool. For more details on the configuration of different options, please consult the Polyflow Components sections.","title":"Getting Started"},{"location":"getting-started/index.html#add-dependency-to-polyflow-integration-starter","text":"Apart from the example application, you might be interested in integrating Polyflow Taskpool and Datapool into your existing application. To do so, you need to enable your Camunda BPM process engine to use the library. For doing so, add the polyflow-integration-camunda-bpm-engine-parent library. In Maven, add the following dependency to your pom.xml : <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-camunda-bpm-springboot-starter </artifactId> <version> ${polyflow.version} </version> </dependency>","title":"Add dependency to Polyflow integration starter"},{"location":"getting-started/index.html#activate-polyflow-support","text":"Now, find your SpringBoot application class and add an annotation to it: @SpringBootApplication @EnableTaskpoolEngineSupport public class MyApplication { public static void main ( String ... args ) { SpringApplication . run ( MyApplication . class , args ); } }","title":"Activate Polyflow Support"},{"location":"getting-started/index.html#configure-your-polyflow-provisioning","text":"Finally, add the following block to your application.yml : camunda : bpm : default-serialization-format : application/json history-level : full polyflow : integration : client : camunda : application-name : ${spring.application.name} # default collector : camunda : application-name : ${spring.application.name} # default process-instance : enabled : true process-definition : enabled : true process-variable : enabled : true task : enabled : true enricher : type : processVariables sender : enabled : true data-entry : enabled : true type : simple application-name : ${spring.application.name} # default process-definition : enabled : true process-instance : enabled : true process-variable : enabled : true task : enabled : true type : tx send-within-transaction : true # Must be set to true in single node scenario. form-url-resolver : defaultTaskTemplate : \"/tasks/${formKey}/${id}?userId=%userId%\" defaultApplicationTemplate : \"http://localhost:${server.port}/${applicationName}\" defaultProcessTemplate : \"/${formKey}?userId=%userId%\" Now, start your process engine. If you run into a user task, you should see on the console how this is passed to task pool. For more details on the configuration of different options, please consult the Polyflow Components sections.","title":"Configure your Polyflow provisioning"},{"location":"introduction/index.html","text":"Over the last years, we built various process applications and whole process platforms for our customers using a modern process engine - Camunda Platform 7. In doing so, we were observed common requirements, in particular with respect to task-oriented frontend applications and were able to extract them. These were basic requirements independent of the used frontend technology, and it turned out that some issues occurred every time during the implementation. These were: Coping with performance issues of Camunda Platform 7 engine when it comes to big amounts of tasks to be shown Creating high-performance custom queries for preloading process variables for tasks Creating high-performance custom queries to preload business data associated with the running process instances High-performance re-ordering (sorting) of user tasks High-performance retrieval of tasks from several process engines Repetitive queries with the same result Creating a custom view on the business data items handled during the process execution Creating a custom audit log for the changes performed on the business data items In our projects we developed solutions to those requirements and gathered experience in applying different approaches for that. Some issues listed above result from the fact that data on a single user task is being read much more often than written, depending on the user count. For systems with a big amount of users this becomes a serious performance issue and needs to be addressed. A possible solution to most of those issues is to create a special component which has a read-optimized representation of user tasks. Such a component acts as a cache for tasks and allows for serving a high amount of queries without any performance impact to the process engine itself at the costs of loosing strong consistency (and working with eventual-consistent task list). Another component might provide additional business data related to the process tasks. We successfully applied this approach at multiple customers but identified the high initial invest as a main drawback of the solution. The goal of this project is to provide such components as free and open source libraries, to be used as a foundation for creation of process platforms for Camunda Platform 7 and other engines. They can also be used as an integration layer for custom process applications, custom user task lists and other components of process automation solutions.","title":"Motivation and Goal"},{"location":"introduction/concepts.html","text":"There are many scenarios in which the usage of a process engine as a component inside the orchestration layer makes a lot of sense. Depending on the scenario the resulting architecture of your application may vary. The following section explains the core concepts and building blocks of the architecture we want to support and address by the Polyflow libraries. The 10,000 feet view # The two main building blocks of the solution are Process Application and Process Platform . Sometimes you unite them inside the same deployment unit, but we differentiate them to make their responsibilities more clear. A Process Application implements the main business logic of the solution. It integrates the process engine that is responsible for execution the processes. During this execution user tasks are created and performed by the user and the business data objects are modified. For this purpose, the process application provides user interfaces for user tasks and business data operations. A Process Platform serves as an integration point of one or multiple process applications. It might integrate with a company's Single Sign-On solution and Identity Management, be part of Intranet portal solution. It provides process agnostic user task list and business object list . Task-oriented applications # The main idea of a task-oriented solution is to model the underlying business process and to split the user interaction into parts represented by the user tasks . Every user task is an abstraction of an operation needed to be performed by the user. Usually, they include some sort of call-to-action including the input fields to be able to input user's decision. Examples of user tasks are Confirm Order , Verify Quotation , Validate Document . User experience plays a significant role in acceptance of the overall solution. In order to access the user task a special UI, called user task form is used. Every user task form is presenting only that limited part of the overall information to the user which is required to complete the user task. This limitation is important in order to avoid distraction and foster focus on the user task. Since there might be multiple process instances running concurrently, a user might see multiple user tasks in the same time. A special view listing all user tasks available for a user is called task list . The application of different user task assignment strategies may be useful to get optimal processing. Along with user tasks forms , representing the actual work the user has to complete, a data-oriented view on business processes is a common requirement. It concentrates on the data being processed and display the business data entities involved in the business processes (sometimes called Workpieces ). Depending on your application, business data entities might be created before the running through business processes and usually the lifecycle of them spans over the business process execution. Examples of business data entities are Order , Shipment or Document . In order to display the state of an individual business data entity a special Business Data Form is designed. And since there are multiple of them in the overall application, a special view to search and list them, a so-called Business Entry List or Workpieces List is developed. Sometimes you are interested in business data entries in a particular processing status and develop a special view for them, for example: Current Workpiece List or Archive List .","title":"Concepts"},{"location":"introduction/concepts.html#the-10000-feet-view","text":"The two main building blocks of the solution are Process Application and Process Platform . Sometimes you unite them inside the same deployment unit, but we differentiate them to make their responsibilities more clear. A Process Application implements the main business logic of the solution. It integrates the process engine that is responsible for execution the processes. During this execution user tasks are created and performed by the user and the business data objects are modified. For this purpose, the process application provides user interfaces for user tasks and business data operations. A Process Platform serves as an integration point of one or multiple process applications. It might integrate with a company's Single Sign-On solution and Identity Management, be part of Intranet portal solution. It provides process agnostic user task list and business object list .","title":"The 10,000 feet view"},{"location":"introduction/concepts.html#task-oriented-applications","text":"The main idea of a task-oriented solution is to model the underlying business process and to split the user interaction into parts represented by the user tasks . Every user task is an abstraction of an operation needed to be performed by the user. Usually, they include some sort of call-to-action including the input fields to be able to input user's decision. Examples of user tasks are Confirm Order , Verify Quotation , Validate Document . User experience plays a significant role in acceptance of the overall solution. In order to access the user task a special UI, called user task form is used. Every user task form is presenting only that limited part of the overall information to the user which is required to complete the user task. This limitation is important in order to avoid distraction and foster focus on the user task. Since there might be multiple process instances running concurrently, a user might see multiple user tasks in the same time. A special view listing all user tasks available for a user is called task list . The application of different user task assignment strategies may be useful to get optimal processing. Along with user tasks forms , representing the actual work the user has to complete, a data-oriented view on business processes is a common requirement. It concentrates on the data being processed and display the business data entities involved in the business processes (sometimes called Workpieces ). Depending on your application, business data entities might be created before the running through business processes and usually the lifecycle of them spans over the business process execution. Examples of business data entities are Order , Shipment or Document . In order to display the state of an individual business data entity a special Business Data Form is designed. And since there are multiple of them in the overall application, a special view to search and list them, a so-called Business Entry List or Workpieces List is developed. Sometimes you are interested in business data entries in a particular processing status and develop a special view for them, for example: Current Workpiece List or Archive List .","title":"Task-oriented applications"},{"location":"introduction/deployment.html","text":"Several deployment scenarios of the components are possible depending on your requirements and available infrastructure. The simplest setup is to run all components on a single node. A more advanced scenario is to distribute components over the network and connect them. In doing so, one of the challenging issues for distribution and connecting microservices is a setup of messaging technology supporting required message exchange patterns (MEPs) for a CQRS system. Because of different semantics of commands, events and queries and additional requirements of event-sourced persistence a special implementation of command bus, event bus and event store is required. In particular, two scenarios can be distinguished: using Axon Server or using a different distribution technology. Single node deployment # The easiest scenario is the Single Node Deployment . It provides all functional features of the Polyflow library, but is not addressing any of performance, scalability, autonomy and reliability requirements. It works almost without additional infrastructure and is ideal to start with. In a single node scenario the following configuration is used: All buses are local (command bus, event bus, query bus) Camunda BPM Integration components, Core components and View components are all deployed in the same node JPA-Based event storage is used, persisting the domain events in a RDBMS, along with Camunda-specific DB tables. Simple (In-memory) View or JPA view is used to provide query projections of taskpool and datapool Check the following diagram for more details: Multiple node deployment # The more advanced scenario is to separate the Process Platform components from Process Application components , compare the concepts section . Especially, it is helpful if you intend to build a central Process Platform and multiple Process applications using it. In general, this is one of the main use cases for Polyflow framework itself, but the distribution aspects adds technical complexity to the resulting architecture. Especially, following the architecture blueprint of Axon Framework, the three buses (command bus, event bus and query bus) needs to be distributed and act as connecting infrastructure between components. Distribution using Axon Server # Axon Server provides an implementation for this requirement leading to a distributed buses and a central Event Store. It is easy to use, easy to configure and easy to run. If you need a HA setup, you will need the enterprise license of Axon Server. Essentially, if you don't have another HA ready-to use messaging, this scenario might be your way to go. This scenario supports: central Process Platform components (including task pool and data pool and their projections) free choice for projection persistence (since Axon Server supports event replay) no direct synchronous communication between Process Platform and Process Application is required (e.g. via REST, since it is routed via command, event and query bus) The following diagram depicts the distribution of the components and the messaging: Distribution without Axon Server # If you already have another messaging at place, like Kafka or RabbitMQ, you might skip the usage of Axon Server. In doing so, you will be responsible for distribution of events and will need to surrender some features. This scenario supports: distributed task pool / data pool view MUST be persistent (no replay supported) direct communication between task list / engines required (addressing, routing) concurrent access to engines might become a problem (no unit of work guarantees) The following diagram depicts the distribution of the components and the messaging. The following diagram depicts the task run from Process Application to the end user, consuming it via Tasklist API connected via Kafka and using Mongo DB for persistence of the query model. The Camunda BPM Taskpool Collector component listens to Camunda events, collects all relevant events that happen in a single transaction and registers a transaction synchronization to process them beforeCommit . Just before the transaction is committed, the collected events are accumulated and sent as Axon Commands through the CommandGateway . The Taskpool Core processes those commands and issues Axon Events which are stored in Axon's database tables within the same transaction. The transaction commit finishes. If anything goes wrong before this point, the transaction rolls back, and it is as though nothing ever happened. In the Axon Kafka Extension , a TrackingEventProcessor polls for events and sees them as soon as the transaction that created them is committed. It sends each event to Kafka and waits for an acknowledgment from Kafka. If sending fails or times out, the event processor goes into error mode and retries until it succeeds. This can lead to events being published to Kafka more than once but guarantees at-least-once delivery. Within the Tasklist API, the Axon Kafka Extension polls the events from Kafka and another TrackingEventProcessor forwards them to the TaskPoolMongoService where they are processed to update the Mongo DB accordingly. When a user queries the Tasklist API for tasks, two things happen: Firstly, the Mongo DB is queried for the current state of tasks for this user and these tasks are returned. Secondly, the Tasklist API subscribes to any changes to the Mongo DB. These changes are filtered for relevance to the user and relevant changes are returned after the current state as an infinite stream until the request is cancelled or interrupted for some reason. From Process Application to Kafka # From Kafka to Tasklist API #","title":"Deployment Strategies"},{"location":"introduction/deployment.html#single-node-deployment","text":"The easiest scenario is the Single Node Deployment . It provides all functional features of the Polyflow library, but is not addressing any of performance, scalability, autonomy and reliability requirements. It works almost without additional infrastructure and is ideal to start with. In a single node scenario the following configuration is used: All buses are local (command bus, event bus, query bus) Camunda BPM Integration components, Core components and View components are all deployed in the same node JPA-Based event storage is used, persisting the domain events in a RDBMS, along with Camunda-specific DB tables. Simple (In-memory) View or JPA view is used to provide query projections of taskpool and datapool Check the following diagram for more details:","title":"Single node deployment"},{"location":"introduction/deployment.html#multiple-node-deployment","text":"The more advanced scenario is to separate the Process Platform components from Process Application components , compare the concepts section . Especially, it is helpful if you intend to build a central Process Platform and multiple Process applications using it. In general, this is one of the main use cases for Polyflow framework itself, but the distribution aspects adds technical complexity to the resulting architecture. Especially, following the architecture blueprint of Axon Framework, the three buses (command bus, event bus and query bus) needs to be distributed and act as connecting infrastructure between components.","title":"Multiple node deployment"},{"location":"introduction/deployment.html#distribution-using-axon-server","text":"Axon Server provides an implementation for this requirement leading to a distributed buses and a central Event Store. It is easy to use, easy to configure and easy to run. If you need a HA setup, you will need the enterprise license of Axon Server. Essentially, if you don't have another HA ready-to use messaging, this scenario might be your way to go. This scenario supports: central Process Platform components (including task pool and data pool and their projections) free choice for projection persistence (since Axon Server supports event replay) no direct synchronous communication between Process Platform and Process Application is required (e.g. via REST, since it is routed via command, event and query bus) The following diagram depicts the distribution of the components and the messaging:","title":"Distribution using Axon Server"},{"location":"introduction/deployment.html#distribution-without-axon-server","text":"If you already have another messaging at place, like Kafka or RabbitMQ, you might skip the usage of Axon Server. In doing so, you will be responsible for distribution of events and will need to surrender some features. This scenario supports: distributed task pool / data pool view MUST be persistent (no replay supported) direct communication between task list / engines required (addressing, routing) concurrent access to engines might become a problem (no unit of work guarantees) The following diagram depicts the distribution of the components and the messaging. The following diagram depicts the task run from Process Application to the end user, consuming it via Tasklist API connected via Kafka and using Mongo DB for persistence of the query model. The Camunda BPM Taskpool Collector component listens to Camunda events, collects all relevant events that happen in a single transaction and registers a transaction synchronization to process them beforeCommit . Just before the transaction is committed, the collected events are accumulated and sent as Axon Commands through the CommandGateway . The Taskpool Core processes those commands and issues Axon Events which are stored in Axon's database tables within the same transaction. The transaction commit finishes. If anything goes wrong before this point, the transaction rolls back, and it is as though nothing ever happened. In the Axon Kafka Extension , a TrackingEventProcessor polls for events and sees them as soon as the transaction that created them is committed. It sends each event to Kafka and waits for an acknowledgment from Kafka. If sending fails or times out, the event processor goes into error mode and retries until it succeeds. This can lead to events being published to Kafka more than once but guarantees at-least-once delivery. Within the Tasklist API, the Axon Kafka Extension polls the events from Kafka and another TrackingEventProcessor forwards them to the TaskPoolMongoService where they are processed to update the Mongo DB accordingly. When a user queries the Tasklist API for tasks, two things happen: Firstly, the Mongo DB is queried for the current state of tasks for this user and these tasks are returned. Secondly, the Tasklist API subscribes to any changes to the Mongo DB. These changes are filtered for relevance to the user and relevant changes are returned after the current state as an infinite stream until the request is cancelled or interrupted for some reason.","title":"Distribution without Axon Server"},{"location":"introduction/deployment.html#from-process-application-to-kafka","text":"","title":"From Process Application to Kafka"},{"location":"introduction/deployment.html#from-kafka-to-tasklist-api","text":"","title":"From Kafka to Tasklist API"},{"location":"introduction/features.html","text":"Taskpool # A task list is an application that shows a list of tasks for each individual user, based on the user's profile, roles and authorizations. Polyflow's taskpool library provides a backend from which task lists can be served. Note If you are using taskpool with Camunda Platform 7, it can be seen as a replacement resp. add-on for Camunda's TaskService . The taskpool library provides the following features: Task mirroring: provides a list of tasks in the system including all standard task attributes provided by the process engine Include additional attributes that are important for processing Reacts on all task life cycle events fired by the process engine, automatically publishes user tasks to the taskpool High performance queries: creates read-optimized projections including task-, process- and business data Centralized task list: running several Camunda BPM Engines in several applications is a common use case for larger companies. From the user's perspective, it is not feasible to login to several task lists and check for relevant user tasks. The demand for a centralized task list can be addressed by using the central taskpool component to which tasks from several process engines are transmitted over the network. Data enrichment: all scenarios, in which the data is not stored in the process payload, result in a cascade of queries executed after fetching the tasks. In contrast to that, the usage of the taskpool library with a data enrichment plugin mechanism allows for caching additional business data along with the task information. Datapool # Each process instance works on one or more business objects and a business object's lifecycle usually spans a longer period of time than the process instance runtime. It's a common requirement to search for these business objects (independently of process tasks) and get a list of these objects including their current statuses (e.g. DRAFT, IN_PROGRESS, COMPLETED). The datapool library provides the necessary features to implement a high-performance Business Object View: Business object API providing additional attributes important for processing Business object modification API for creating an audit log (aka business object history) Authorization API for business objects Process Definition Pool # A process repository provides a list of running instances and a list of process definitions deployed in the process engines connected to the library. It provides the following features: List of startable process definitions (including URLs to start forms) List of running process instances Reacts on life cycle events of process instance Process Instance Pool # All process instances started, suspended, resumed, completed aor deleted in the process engine are reflected in the process instance pool component. Process Variable Pool # Along with business data entities being modified during the execution of the business processes, the business process instance itself holds a collection of so-called process variables, representing the state of the execution. In contrast to the business data entities, their lifecycle is bound to the lifecycle of the business process instance. For different reasons the requirement might exist to have rapid access to the process variables of a running process instance, which is provided by the taskpool library. Integration # Generic task sender Generic data entry sender Camunda BPM collector (tasks, process definitions, process instances, process variables)","title":"Features"},{"location":"introduction/features.html#taskpool","text":"A task list is an application that shows a list of tasks for each individual user, based on the user's profile, roles and authorizations. Polyflow's taskpool library provides a backend from which task lists can be served. Note If you are using taskpool with Camunda Platform 7, it can be seen as a replacement resp. add-on for Camunda's TaskService . The taskpool library provides the following features: Task mirroring: provides a list of tasks in the system including all standard task attributes provided by the process engine Include additional attributes that are important for processing Reacts on all task life cycle events fired by the process engine, automatically publishes user tasks to the taskpool High performance queries: creates read-optimized projections including task-, process- and business data Centralized task list: running several Camunda BPM Engines in several applications is a common use case for larger companies. From the user's perspective, it is not feasible to login to several task lists and check for relevant user tasks. The demand for a centralized task list can be addressed by using the central taskpool component to which tasks from several process engines are transmitted over the network. Data enrichment: all scenarios, in which the data is not stored in the process payload, result in a cascade of queries executed after fetching the tasks. In contrast to that, the usage of the taskpool library with a data enrichment plugin mechanism allows for caching additional business data along with the task information.","title":"Taskpool"},{"location":"introduction/features.html#datapool","text":"Each process instance works on one or more business objects and a business object's lifecycle usually spans a longer period of time than the process instance runtime. It's a common requirement to search for these business objects (independently of process tasks) and get a list of these objects including their current statuses (e.g. DRAFT, IN_PROGRESS, COMPLETED). The datapool library provides the necessary features to implement a high-performance Business Object View: Business object API providing additional attributes important for processing Business object modification API for creating an audit log (aka business object history) Authorization API for business objects","title":"Datapool"},{"location":"introduction/features.html#process-definition-pool","text":"A process repository provides a list of running instances and a list of process definitions deployed in the process engines connected to the library. It provides the following features: List of startable process definitions (including URLs to start forms) List of running process instances Reacts on life cycle events of process instance","title":"Process Definition Pool"},{"location":"introduction/features.html#process-instance-pool","text":"All process instances started, suspended, resumed, completed aor deleted in the process engine are reflected in the process instance pool component.","title":"Process Instance Pool"},{"location":"introduction/features.html#process-variable-pool","text":"Along with business data entities being modified during the execution of the business processes, the business process instance itself holds a collection of so-called process variables, representing the state of the execution. In contrast to the business data entities, their lifecycle is bound to the lifecycle of the business process instance. For different reasons the requirement might exist to have rapid access to the process variables of a running process instance, which is provided by the taskpool library.","title":"Process Variable Pool"},{"location":"introduction/features.html#integration","text":"Generic task sender Generic data entry sender Camunda BPM collector (tasks, process definitions, process instances, process variables)","title":"Integration"},{"location":"introduction/solution-architecture.html","text":"General Idea # The implementation of a single (small) process application can be easily done using the process engine library itself (for example like Camunda BPM). If the solution becomes larger, for example by setting up multiple engines for different processes or if the load on a single process engine becomes unmanageable, it is worth to separate the solution into process specific and process agnostic parts. We call the process specific part of the solution \"Process Application\" and the process agnostic part \"Process Platform\", as described in the concept section . Based on the assumption of the asymmetric read/write characteristics of task-oriented process applications, we decided to apply the Command Query Responsibility Segregation (CQRS) pattern for the architectural design. As a result, we supply components to collect the information from the process engines and create a read-optimized projections with user tasks and correlated business data. The components can be easily integrated into process applications and be used as foundation to build parts of the process platform. Design Decisions # We decided to build the library as a collection of loosely-coupled components which can be used during the construction of the process automation solution in different ways, depending on your Usage Scenario . The process platform is a central application consisting of business process independent components like a central user management, task inbox (aka task list), a business object view, audit logs and others. One or many process applications integrate with the process platform by implementing individual business processes and provide user tasks and business data changes to it. They may also ship application frontends, which are integrated into/with the frontends of the process platform, including business object views, user task forms and other required pieces. The following diagram depicts the overall logical architecture: Implementation Decisions # The components are implemented using Kotlin programming language and rely on SpringBoot as execution environment. They make a massive use of Axon Framework as a basis of the CQRS implementation. In addition, we rely on event sourcing (ES) as persistence implementation. Due to the usage of the Axon Framework, you can choose the Event Store implementation, based on the available technology. Available options for the Event Store are Axon Server, JDBC, JPA or Mongo. The read-optimized projections use an internal API for constructing query models. Several query models are available: In-memory, Mongo and JPA are choices matching different requirements of the scenarios of process applications and process platform. All query models implement the same public API allowing an easy exchange of the implementation depending on available technology or changing requirements. Several integration components are available for collecting information from process engines and other third-party applications. Along with generic collectors for integrations of custom components, a specialized integration component for Camunda BPM engine is provided. It seamlessly integrates with the Camunda BPM engine using Camunda Engine's Plug-in mechanism and automatically delivers process definitions, process instances, process variables from the process engine. In addition, it allows enrichment of user tasks with custom process data. By doing so, the Camunda Integration component allows to include more information into user tasks and allows the task read projections to outperform the Camunda Task Service, providing more features and higher performance. The following figure demonstrates the architecture of the Camunda Integration component. The loosely-coupled nature of the Polyflow framework supports different deployment strategies. It can be used in the context of a single process engine as well as in the context of a process landscape with multiple engines. In doing so, the Polyflow components can be used in a central system for collection, storage and provisioning of the user tasks and business data for the entire process landscape. The usage of the generic integration components allows for integration of heterogeneous process engines and other task and business data systems. For more details, please check the documentation on Deployment Scenarios .","title":"Solution Architecture"},{"location":"introduction/solution-architecture.html#general-idea","text":"The implementation of a single (small) process application can be easily done using the process engine library itself (for example like Camunda BPM). If the solution becomes larger, for example by setting up multiple engines for different processes or if the load on a single process engine becomes unmanageable, it is worth to separate the solution into process specific and process agnostic parts. We call the process specific part of the solution \"Process Application\" and the process agnostic part \"Process Platform\", as described in the concept section . Based on the assumption of the asymmetric read/write characteristics of task-oriented process applications, we decided to apply the Command Query Responsibility Segregation (CQRS) pattern for the architectural design. As a result, we supply components to collect the information from the process engines and create a read-optimized projections with user tasks and correlated business data. The components can be easily integrated into process applications and be used as foundation to build parts of the process platform.","title":"General Idea"},{"location":"introduction/solution-architecture.html#design-decisions","text":"We decided to build the library as a collection of loosely-coupled components which can be used during the construction of the process automation solution in different ways, depending on your Usage Scenario . The process platform is a central application consisting of business process independent components like a central user management, task inbox (aka task list), a business object view, audit logs and others. One or many process applications integrate with the process platform by implementing individual business processes and provide user tasks and business data changes to it. They may also ship application frontends, which are integrated into/with the frontends of the process platform, including business object views, user task forms and other required pieces. The following diagram depicts the overall logical architecture:","title":"Design Decisions"},{"location":"introduction/solution-architecture.html#implementation-decisions","text":"The components are implemented using Kotlin programming language and rely on SpringBoot as execution environment. They make a massive use of Axon Framework as a basis of the CQRS implementation. In addition, we rely on event sourcing (ES) as persistence implementation. Due to the usage of the Axon Framework, you can choose the Event Store implementation, based on the available technology. Available options for the Event Store are Axon Server, JDBC, JPA or Mongo. The read-optimized projections use an internal API for constructing query models. Several query models are available: In-memory, Mongo and JPA are choices matching different requirements of the scenarios of process applications and process platform. All query models implement the same public API allowing an easy exchange of the implementation depending on available technology or changing requirements. Several integration components are available for collecting information from process engines and other third-party applications. Along with generic collectors for integrations of custom components, a specialized integration component for Camunda BPM engine is provided. It seamlessly integrates with the Camunda BPM engine using Camunda Engine's Plug-in mechanism and automatically delivers process definitions, process instances, process variables from the process engine. In addition, it allows enrichment of user tasks with custom process data. By doing so, the Camunda Integration component allows to include more information into user tasks and allows the task read projections to outperform the Camunda Task Service, providing more features and higher performance. The following figure demonstrates the architecture of the Camunda Integration component. The loosely-coupled nature of the Polyflow framework supports different deployment strategies. It can be used in the context of a single process engine as well as in the context of a process landscape with multiple engines. In doing so, the Polyflow components can be used in a central system for collection, storage and provisioning of the user tasks and business data for the entire process landscape. The usage of the generic integration components allows for integration of heterogeneous process engines and other task and business data systems. For more details, please check the documentation on Deployment Scenarios .","title":"Implementation Decisions"},{"location":"reference-guide/index.html","text":"This reference guide is a primary source of information in order to understand how Polyflow components are used and how to configure them. It is divided into tow major sections: Components Configuration","title":"Reference Overview"},{"location":"reference-guide/components/index.html","text":"We decided to build a library as a collection of loose-coupled components which can be used during the construction of the process automation solution. In doing so, we provide Process Engine Integration Components which are intended to be deployed as a part of the process application. In addition, we provide Process Platform Components which serve as a building blocks for the process platform itself. Process Engine Integration Components # The Process Engine Integration Components are designed to be a part of process application deployment and react on engine changes / interact with the engine. These are split into common components which are independent of the used product and framework-dependent adapters: Common Integration Components # Datapool Sender Taskpool Sender Camunda BPM Integration Components # Camunda BPM Engine Interaction Client Camunda BPM Engine Taskpool Collector Camunda BPM Engine Taskpool Spring Boot Starter Process Platform Components # Process Platform Components are designed to build the process platform. The platform provides common functionality used by all process applications like common user management, unified task list and so on. Core Components # Core Components are responsible for the processing of commands about user tasks, process instances, process variables, business data items and form an event stream consumed by the view components. Depending on the scenario, they can be deployed either within the process application, process platform or even completely separately. Taskpool Core Datapool Core View Components # View Components are responsible for creation of a unified read-only projection of process definitions, process instances, process variables, user tasks and business data items. They are typically deployed as a part of the process platform. View API View API Client In-Memory View JPA View Mongo DB View Property Form URL Resolver Other Components # Variable Serializer Tasklist URL Resolver Bus Jackson","title":"Component Overview"},{"location":"reference-guide/components/index.html#process-engine-integration-components","text":"The Process Engine Integration Components are designed to be a part of process application deployment and react on engine changes / interact with the engine. These are split into common components which are independent of the used product and framework-dependent adapters:","title":"Process Engine Integration Components"},{"location":"reference-guide/components/index.html#common-integration-components","text":"Datapool Sender Taskpool Sender","title":"Common Integration Components"},{"location":"reference-guide/components/index.html#camunda-bpm-integration-components","text":"Camunda BPM Engine Interaction Client Camunda BPM Engine Taskpool Collector Camunda BPM Engine Taskpool Spring Boot Starter","title":"Camunda BPM Integration Components"},{"location":"reference-guide/components/index.html#process-platform-components","text":"Process Platform Components are designed to build the process platform. The platform provides common functionality used by all process applications like common user management, unified task list and so on.","title":"Process Platform Components"},{"location":"reference-guide/components/index.html#core-components","text":"Core Components are responsible for the processing of commands about user tasks, process instances, process variables, business data items and form an event stream consumed by the view components. Depending on the scenario, they can be deployed either within the process application, process platform or even completely separately. Taskpool Core Datapool Core","title":"Core Components"},{"location":"reference-guide/components/index.html#view-components","text":"View Components are responsible for creation of a unified read-only projection of process definitions, process instances, process variables, user tasks and business data items. They are typically deployed as a part of the process platform. View API View API Client In-Memory View JPA View Mongo DB View Property Form URL Resolver","title":"View Components"},{"location":"reference-guide/components/index.html#other-components","text":"Variable Serializer Tasklist URL Resolver Bus Jackson","title":"Other Components"},{"location":"reference-guide/components/camunda-interaction-client.html","text":"Camunda Engine Interaction Client # Purpose # This component performs changes delivered by Camunda Interaction Events on Camunda BPM engine. The following Camunda Interaction Events are supported: Claim User Task Unclaim User Task Defer User Task Undefer User Task Complete User Task Usage and configuration # To use Camunda Engine Interaction Client please add the following artifact to your classpath: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-tasklist-url-resolver </artifactId> </dependency> In your application.yml configure the application name of your process engine, to receive commands: polyflow: integration: client: camunda: application-name: my-process-application # defaults to ${spring.application.name}","title":"Camunda BPM Engine Interaction Client"},{"location":"reference-guide/components/camunda-interaction-client.html#camunda-engine-interaction-client","text":"","title":"Camunda Engine Interaction Client"},{"location":"reference-guide/components/camunda-interaction-client.html#purpose","text":"This component performs changes delivered by Camunda Interaction Events on Camunda BPM engine. The following Camunda Interaction Events are supported: Claim User Task Unclaim User Task Defer User Task Undefer User Task Complete User Task","title":"Purpose"},{"location":"reference-guide/components/camunda-interaction-client.html#usage-and-configuration","text":"To use Camunda Engine Interaction Client please add the following artifact to your classpath: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-tasklist-url-resolver </artifactId> </dependency> In your application.yml configure the application name of your process engine, to receive commands: polyflow: integration: client: camunda: application-name: my-process-application # defaults to ${spring.application.name}","title":"Usage and configuration"},{"location":"reference-guide/components/camunda-starter.html","text":"Purpose # The Camunda Engine Taskpool Support SpringBoot Starter is a convenience module providing a single module dependency to be included in the process application. It includes all process application modules and provides meaningful defaults for their options. Configuration # In order to enable the starter, please put the following dependency on your class path: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-integration-camunda-bpm-springboot-starter </artifactId> </dependency> The included TaskpoolEngineSupportConfiguration is a SpringBoot AutoConfiguration that configures the required components. If you want to configure it manually, please add the @EnableTaskpoolEngineSupport annotation on any @Configuration annotated class of your SpringBoot application. The @EnableTaskpoolEngineSupport annotation has the same effect as the following block of annotations: @EnableCamundaTaskpoolCollector @EnableDataEntrySender public class MyApplication { //... }","title":"Camunda BPM Engine Taskpool SpringBoot Starter"},{"location":"reference-guide/components/camunda-starter.html#purpose","text":"The Camunda Engine Taskpool Support SpringBoot Starter is a convenience module providing a single module dependency to be included in the process application. It includes all process application modules and provides meaningful defaults for their options.","title":"Purpose"},{"location":"reference-guide/components/camunda-starter.html#configuration","text":"In order to enable the starter, please put the following dependency on your class path: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-integration-camunda-bpm-springboot-starter </artifactId> </dependency> The included TaskpoolEngineSupportConfiguration is a SpringBoot AutoConfiguration that configures the required components. If you want to configure it manually, please add the @EnableTaskpoolEngineSupport annotation on any @Configuration annotated class of your SpringBoot application. The @EnableTaskpoolEngineSupport annotation has the same effect as the following block of annotations: @EnableCamundaTaskpoolCollector @EnableDataEntrySender public class MyApplication { //... }","title":"Configuration"},{"location":"reference-guide/components/camunda-taskpool-collector.html","text":"Taskpool Collector # Purpose # Taskpool Collector is a component deployed as a part of the process application (aside with Camunda BPM Engine) that is responsible for collecting information from the Camunda BPM Engine. It detects the intent of the operations executed inside the engine and creates the corresponding commands for the Taskpool. The commands are enriched with data and transmitted to other taskpool components (via Axon Command Bus). In the following description, we use the terms event and command . Event denotes an entity received from Camunda BPM Engine (from delegate event listener or from history event listener) which is passed over to the Taskpool Collector using internal Spring eventing mechanism. The Taskpool Collector converts the series of such events into a Taskpool Command - an entity carrying an intent of change inside the Taskpool core. Please note that event has another meaning in CQRS/ES systems and other components of the Taskpool, but in the context of Taskpool collector an event always originates from Spring eventing. Features # Collection of process definitions Collection of process instance events Collection of process variable change events Collection of task events and history events Creation of task engine commands Enrichment of task engine commands with process variables Attachment of correlation information to task engine commands Transmission of commands to Axon command bus Provision of properties for process application Architecture # The Taskpool Collector consists of several components which can be divided into the following groups: Event collectors receive are responsible for gathering information and form commands Processor performs the command enrichment with payload and data correlation Command senders are responsible for accumulating commands and sending them to Command Gateway Usage and configuration # In order to enable collector component, include the Maven dependency to your process application: <dependency> <groupId> io.holunda.polyflow <groupId> <artifactId> polyflow-camunda-bpm-taskpool-collector </artifactId> <version> ${camunda-taskpool.version} </version> <dependency> Then activate the taskpool collector by providing the annotation on any Spring Configuration: @Configuration @Import ( CamundaTaskpoolCollectorConfiguration . class ) class MyProcessApplicationConfiguration { } Event collection # Taskpool Collector registers Spring Event Listener to the following events, fired by Camunda Eventing Engine Plugin: DelegateTask events: create assign delete complete HistoryEvent events: HistoricTaskInstanceEvent HistoricIdentityLinkLogEvent HistoricProcessInstanceEventEntity HistoricVariableUpdateEventEntity ** HistoricDetailVariableInstanceUpdateEntity The events are transformed into corresponding commands and passed over to the processor layer. Task commands enrichment # Alongside with attributes received from the Camunda BPM engine, the engine task commands can be enriched with additional attributes. There are three enrichment modes available controlled by the polyflow.integration.collector.camunda.task.enricher.type property: no : No enrichment takes place process-variables : Enrichment of engine task commands with process variables custom : User provides own implementation Process variable enrichment # In particular cases, the data enclosed into task attributes is not sufficient for the task list or other user-related components. The information may be available as process variables and need to be attached to the task in the taskpool. This is where Process Variable Task Enricher can be used. For this purpose, active it, setting the property polyflow.integration.collector.camunda.task.enricher.type to process-variables and the enricher will put process variables into the task payload. You can control what variables will be put into task command payload by providing the Process Variables Filter. The ProcessVariablesFilter is a Spring bean holding a list of individual VariableFilter - at most one per process definition key and optionally one without process definition key (a global filter). If the filter is not provded, a default filter is used which is an empty EXCLUDE filter, resulting in all process variables being attached to the user task. A VariableFilter can be of the following type: TaskVariableFilter : INCLUDE : task-level include filter, denoting a list of variables to be added for the task defined in the filter. EXCLUDE : task-level exclude filter, denoting a list of variables to be ignored for the task defined in the filter. All other variables are included. ProcessVariableFilter with process definition key: INCLUDE : process-level include filter, denoting a list of variables to be added for all tasks of the process. EXCLUDE : process-level exclude filter, denoting a list of variables to be ignored for all tasks of the process. ProcessVariableFilter without process definition key: INCLUDE : global include filter, denoting a list of variables to be added for all tasks of all processes for which no dedicated ProcessVariableFilter is defined. EXCLUDE : global exclude filter, denoting a list of variables to be ignored for all tasks of all processes for which no dedicated ProcessVariableFilter is defined. Here is an example, how the process variable filter can configure the enrichment: @Configuration public class MyTaskCollectorConfiguration { @Bean public ProcessVariablesFilter myProcessVariablesFilter () { return new ProcessVariablesFilter ( // define a variable filter for every process new VariableFilter [] { // define for every process definition // either a TaskVariableFilter or ProcessVariableFilter new TaskVariableFilter ( ProcessApproveRequest . KEY , // filter type FilterType . INCLUDE , ImmutableMap . < String , List < String >> builder () // define a variable filter for every task of the process . put ( ProcessApproveRequest . Elements . APPROVE_REQUEST , Lists . newArrayList ( ProcessApproveRequest . Variables . REQUEST_ID , ProcessApproveRequest . Variables . ORIGINATOR ) ) // and again . put ( ProcessApproveRequest . Elements . AMEND_REQUEST , Lists . newArrayList ( ProcessApproveRequest . Variables . REQUEST_ID , ProcessApproveRequest . Variables . COMMENT , ProcessApproveRequest . Variables . APPLICANT ) ). build () ), // optionally add a global filter for all processes // for that no individual filter was created new ProcessVariableFilter ( FilterType . INCLUDE , Lists . newArrayList ( CommonProcessVariables . CUSTOMER_ID )) } ); } } Note If you want to implement a custom enrichment, please provide your own implementation of the interface VariablesEnricher (register a Spring Component of the type) and set the property polyflow.integration.collector.camunda.task.enricher.type to custom . Data Correlation # Apart from task payload attached by the enricher, the so-called Correlation with data entries can be configured. The data correlation allows to attach one or several references (that is a pair of values entryType and entryId ) of business data entry(ies) to a task. In the projection (which is used for querying of tasks) this correlations is be resolved and the information from business data events can be shown together with task information. The correlation to data events can be configured by providing a ProcessVariablesCorrelator bean. Here is an example how this can be done: @Bean fun processVariablesCorrelator () = ProcessVariablesCorrelator ( // define correlation for every process ProcessVariableCorrelation ( ProcessApproveRequest . KEY , mapOf ( // define a correlation for every task needed ProcessApproveRequest . Elements . APPROVE_REQUEST to mapOf ( ProcessApproveRequest . Variables . REQUEST_ID to BusinessDataEntry . REQUEST ) ), // define a correlation globally (for the whole process) mapOf ( ProcessApproveRequest . Variables . REQUEST_ID to BusinessDataEntry . REQUEST ) ) ) The process variable correlator holds a list of process variable correlations - one for every process definition key. Every ProcessVariableCorrelation configures for all tasks or for an individual task by providing a so-called correlation map. A correlation map is keyed by the name of a process variable inside Camunda Process Engine and holds the type of business data entry as value. Here is an example. Imagine the process instance is storing the id of an approval request in a process variable called varRequestId . The system responsible for storing approval requests fires data entry events supplying the data and using the entry type io.my.approvalRequest and the id of the request as entryId . In order to create a correlation in task task_approve_request of the process_approval_process we would provide the following configuration of the correlator: @Bean fun processVariablesCorrelator () = ProcessVariablesCorrelator ( ProcessVariableCorrelation ( \"process_approval_process\" , mapOf ( \"task_approve_request\" to mapOf ( // process variable 'varRequestId' holds the id of a data entry of type 'io.my.approvalRequest' \"varRequestId\" to \"io.my.approvalRequest\" ) ) ) ) If the process instance now contains the approval request id \"4711\" in the process variable varRequestId and the process reaches the task task_approve_request , the task will get the following correlation created (here written in JSON): \"correlations\" : [ { \"entryType\" : \"approvalRequest\" , \"entryId\" : \"4711\" } ] Command aggregation # In order to control sending of commands to command sender, the command sender activation property polyflow.integration.sender.task.enabled is available. If disabled, the command sender will log any command instead of aggregating sending it to the command gateway. In addition, you can control by the property polyflow.integration.task.sender.type if you want to use the default command sender or provide your own implementation. The default provided command sender (type: tx ) is collects all task commands during one transaction, group them by task id and accumulates by creating one command reflecting the intent of the task operation. It uses Axon Command Bus (encapsulated by the AxonCommandListGateway for sending the result over to the Axon command gateway. TIP: If you want to implement a custom command sending, please provide your own implementation of the interface EngineTaskCommandSender (register a Spring Component of the type) and set the property polyflow.integration.task.sender.type to custom . The Spring event listeners receiving events from the Camunda Engine plugin are called before the engine commits the transaction. Since all processing inside collector component and enricher is performed synchronously, the sender must waits until transaction to be successfully committed before sending any commands to the Command Gateway. Otherwise, on any error the transaction would be rolled-back and the command would create an inconsistency between the taskpool and the engine. Depending on your deployment scenario, you may want to control the exact point in time when the commands are sent to command gateway. The property polyflow.integration.task.sender.send-within-transaction is designed to influence this. If set to true , the commands are sent before the process engine transaction is committed, otherwise commands are sent after the process engine transaction is committed. Warning Never send commands over remote messaging before the transaction is committed, since you may produce unexpected results if Camunda fails to commit the transaction. Handling command transmission # The commands sent via gateway (e.g. AxonCommandListGateway ) are received by Command Handlers. The latter may accept or reject commands, depending on the state of the aggregate and other components. The AxonCommandListGateway is informed about the command outcome. By default, it will log the outcome to console (success is logged in DEBUG log level, errors are using ERROR log level). In some situations it is required to take care of command outcome. A prominent example is to include a metric for command dispatching errors into monitoring. For doing so, it is possible to provide own handlers for success and error command outcome. For this purpose, please provide a Spring Bean implementing the CommandSuccessHandler and CommandErrorHandler accordingly. Here is an example, how such a handler may look like: @Bean @Primary fun taskCommandErrorHandler (): TaskCommandErrorHandler = object : LoggingTaskCommandErrorHandler ( logger ) { override fun apply ( commandMessage : Any , commandResultMessage : CommandResultMessage < out Any? > ) { logger . info { \"<--------- CUSTOM ERROR HANDLER REPORT --------->\" } super . apply ( commandMessage , commandResultMessage ) logger . info { \"<------------------- END ----------------------->\" } } } Message codes # Please note that the logger root hierarchy is io.holunda.camunda.taskpool.collector Message Code Severity Logger* Description Meaning COLLECTOR-001 INFO Task commands will be collected. COLLECTOR-002 INFO Task commands not be collected. COLLECTOR-005 DEBUG .process.definition Process definition collecting has been disabled by property, skipping ${command.processDefinitionId}. COLLECTOR-006 DEBUG .process.instance Process instance collecting has been disabled by property, skipping ${command.processInstanceId}. COLLECTOR-007 DEBUG .process.variable Process variable collecting has been disabled by property, skipping ${command.processInstanceId}. COLLECTOR-008 DEBUG .task Task command collecting is disabled by property, would have enriched and sent command $command. ENRICHER-001 INFO Task commands will be enriched with process variables. ENRICHER-002 INFO Task commands will not be enriched. ENRICHER-003 INFO Task commands will be enriched by a custom enricher. ENRICHER-004 DEBUG .task.enricher Could not enrich variables from running execution ${command.sourceReference.executionId}, since it doesn't exist (anymore).","title":"Camunda BPM Engine Taskpool Collector"},{"location":"reference-guide/components/camunda-taskpool-collector.html#taskpool-collector","text":"","title":"Taskpool Collector"},{"location":"reference-guide/components/camunda-taskpool-collector.html#purpose","text":"Taskpool Collector is a component deployed as a part of the process application (aside with Camunda BPM Engine) that is responsible for collecting information from the Camunda BPM Engine. It detects the intent of the operations executed inside the engine and creates the corresponding commands for the Taskpool. The commands are enriched with data and transmitted to other taskpool components (via Axon Command Bus). In the following description, we use the terms event and command . Event denotes an entity received from Camunda BPM Engine (from delegate event listener or from history event listener) which is passed over to the Taskpool Collector using internal Spring eventing mechanism. The Taskpool Collector converts the series of such events into a Taskpool Command - an entity carrying an intent of change inside the Taskpool core. Please note that event has another meaning in CQRS/ES systems and other components of the Taskpool, but in the context of Taskpool collector an event always originates from Spring eventing.","title":"Purpose"},{"location":"reference-guide/components/camunda-taskpool-collector.html#features","text":"Collection of process definitions Collection of process instance events Collection of process variable change events Collection of task events and history events Creation of task engine commands Enrichment of task engine commands with process variables Attachment of correlation information to task engine commands Transmission of commands to Axon command bus Provision of properties for process application","title":"Features"},{"location":"reference-guide/components/camunda-taskpool-collector.html#architecture","text":"The Taskpool Collector consists of several components which can be divided into the following groups: Event collectors receive are responsible for gathering information and form commands Processor performs the command enrichment with payload and data correlation Command senders are responsible for accumulating commands and sending them to Command Gateway","title":"Architecture"},{"location":"reference-guide/components/camunda-taskpool-collector.html#usage-and-configuration","text":"In order to enable collector component, include the Maven dependency to your process application: <dependency> <groupId> io.holunda.polyflow <groupId> <artifactId> polyflow-camunda-bpm-taskpool-collector </artifactId> <version> ${camunda-taskpool.version} </version> <dependency> Then activate the taskpool collector by providing the annotation on any Spring Configuration: @Configuration @Import ( CamundaTaskpoolCollectorConfiguration . class ) class MyProcessApplicationConfiguration { }","title":"Usage and configuration"},{"location":"reference-guide/components/camunda-taskpool-collector.html#event-collection","text":"Taskpool Collector registers Spring Event Listener to the following events, fired by Camunda Eventing Engine Plugin: DelegateTask events: create assign delete complete HistoryEvent events: HistoricTaskInstanceEvent HistoricIdentityLinkLogEvent HistoricProcessInstanceEventEntity HistoricVariableUpdateEventEntity ** HistoricDetailVariableInstanceUpdateEntity The events are transformed into corresponding commands and passed over to the processor layer.","title":"Event collection"},{"location":"reference-guide/components/camunda-taskpool-collector.html#task-commands-enrichment","text":"Alongside with attributes received from the Camunda BPM engine, the engine task commands can be enriched with additional attributes. There are three enrichment modes available controlled by the polyflow.integration.collector.camunda.task.enricher.type property: no : No enrichment takes place process-variables : Enrichment of engine task commands with process variables custom : User provides own implementation","title":"Task commands enrichment"},{"location":"reference-guide/components/camunda-taskpool-collector.html#process-variable-enrichment","text":"In particular cases, the data enclosed into task attributes is not sufficient for the task list or other user-related components. The information may be available as process variables and need to be attached to the task in the taskpool. This is where Process Variable Task Enricher can be used. For this purpose, active it, setting the property polyflow.integration.collector.camunda.task.enricher.type to process-variables and the enricher will put process variables into the task payload. You can control what variables will be put into task command payload by providing the Process Variables Filter. The ProcessVariablesFilter is a Spring bean holding a list of individual VariableFilter - at most one per process definition key and optionally one without process definition key (a global filter). If the filter is not provded, a default filter is used which is an empty EXCLUDE filter, resulting in all process variables being attached to the user task. A VariableFilter can be of the following type: TaskVariableFilter : INCLUDE : task-level include filter, denoting a list of variables to be added for the task defined in the filter. EXCLUDE : task-level exclude filter, denoting a list of variables to be ignored for the task defined in the filter. All other variables are included. ProcessVariableFilter with process definition key: INCLUDE : process-level include filter, denoting a list of variables to be added for all tasks of the process. EXCLUDE : process-level exclude filter, denoting a list of variables to be ignored for all tasks of the process. ProcessVariableFilter without process definition key: INCLUDE : global include filter, denoting a list of variables to be added for all tasks of all processes for which no dedicated ProcessVariableFilter is defined. EXCLUDE : global exclude filter, denoting a list of variables to be ignored for all tasks of all processes for which no dedicated ProcessVariableFilter is defined. Here is an example, how the process variable filter can configure the enrichment: @Configuration public class MyTaskCollectorConfiguration { @Bean public ProcessVariablesFilter myProcessVariablesFilter () { return new ProcessVariablesFilter ( // define a variable filter for every process new VariableFilter [] { // define for every process definition // either a TaskVariableFilter or ProcessVariableFilter new TaskVariableFilter ( ProcessApproveRequest . KEY , // filter type FilterType . INCLUDE , ImmutableMap . < String , List < String >> builder () // define a variable filter for every task of the process . put ( ProcessApproveRequest . Elements . APPROVE_REQUEST , Lists . newArrayList ( ProcessApproveRequest . Variables . REQUEST_ID , ProcessApproveRequest . Variables . ORIGINATOR ) ) // and again . put ( ProcessApproveRequest . Elements . AMEND_REQUEST , Lists . newArrayList ( ProcessApproveRequest . Variables . REQUEST_ID , ProcessApproveRequest . Variables . COMMENT , ProcessApproveRequest . Variables . APPLICANT ) ). build () ), // optionally add a global filter for all processes // for that no individual filter was created new ProcessVariableFilter ( FilterType . INCLUDE , Lists . newArrayList ( CommonProcessVariables . CUSTOMER_ID )) } ); } } Note If you want to implement a custom enrichment, please provide your own implementation of the interface VariablesEnricher (register a Spring Component of the type) and set the property polyflow.integration.collector.camunda.task.enricher.type to custom .","title":"Process variable enrichment"},{"location":"reference-guide/components/camunda-taskpool-collector.html#data-correlation","text":"Apart from task payload attached by the enricher, the so-called Correlation with data entries can be configured. The data correlation allows to attach one or several references (that is a pair of values entryType and entryId ) of business data entry(ies) to a task. In the projection (which is used for querying of tasks) this correlations is be resolved and the information from business data events can be shown together with task information. The correlation to data events can be configured by providing a ProcessVariablesCorrelator bean. Here is an example how this can be done: @Bean fun processVariablesCorrelator () = ProcessVariablesCorrelator ( // define correlation for every process ProcessVariableCorrelation ( ProcessApproveRequest . KEY , mapOf ( // define a correlation for every task needed ProcessApproveRequest . Elements . APPROVE_REQUEST to mapOf ( ProcessApproveRequest . Variables . REQUEST_ID to BusinessDataEntry . REQUEST ) ), // define a correlation globally (for the whole process) mapOf ( ProcessApproveRequest . Variables . REQUEST_ID to BusinessDataEntry . REQUEST ) ) ) The process variable correlator holds a list of process variable correlations - one for every process definition key. Every ProcessVariableCorrelation configures for all tasks or for an individual task by providing a so-called correlation map. A correlation map is keyed by the name of a process variable inside Camunda Process Engine and holds the type of business data entry as value. Here is an example. Imagine the process instance is storing the id of an approval request in a process variable called varRequestId . The system responsible for storing approval requests fires data entry events supplying the data and using the entry type io.my.approvalRequest and the id of the request as entryId . In order to create a correlation in task task_approve_request of the process_approval_process we would provide the following configuration of the correlator: @Bean fun processVariablesCorrelator () = ProcessVariablesCorrelator ( ProcessVariableCorrelation ( \"process_approval_process\" , mapOf ( \"task_approve_request\" to mapOf ( // process variable 'varRequestId' holds the id of a data entry of type 'io.my.approvalRequest' \"varRequestId\" to \"io.my.approvalRequest\" ) ) ) ) If the process instance now contains the approval request id \"4711\" in the process variable varRequestId and the process reaches the task task_approve_request , the task will get the following correlation created (here written in JSON): \"correlations\" : [ { \"entryType\" : \"approvalRequest\" , \"entryId\" : \"4711\" } ]","title":"Data Correlation"},{"location":"reference-guide/components/camunda-taskpool-collector.html#command-aggregation","text":"In order to control sending of commands to command sender, the command sender activation property polyflow.integration.sender.task.enabled is available. If disabled, the command sender will log any command instead of aggregating sending it to the command gateway. In addition, you can control by the property polyflow.integration.task.sender.type if you want to use the default command sender or provide your own implementation. The default provided command sender (type: tx ) is collects all task commands during one transaction, group them by task id and accumulates by creating one command reflecting the intent of the task operation. It uses Axon Command Bus (encapsulated by the AxonCommandListGateway for sending the result over to the Axon command gateway. TIP: If you want to implement a custom command sending, please provide your own implementation of the interface EngineTaskCommandSender (register a Spring Component of the type) and set the property polyflow.integration.task.sender.type to custom . The Spring event listeners receiving events from the Camunda Engine plugin are called before the engine commits the transaction. Since all processing inside collector component and enricher is performed synchronously, the sender must waits until transaction to be successfully committed before sending any commands to the Command Gateway. Otherwise, on any error the transaction would be rolled-back and the command would create an inconsistency between the taskpool and the engine. Depending on your deployment scenario, you may want to control the exact point in time when the commands are sent to command gateway. The property polyflow.integration.task.sender.send-within-transaction is designed to influence this. If set to true , the commands are sent before the process engine transaction is committed, otherwise commands are sent after the process engine transaction is committed. Warning Never send commands over remote messaging before the transaction is committed, since you may produce unexpected results if Camunda fails to commit the transaction.","title":"Command aggregation"},{"location":"reference-guide/components/camunda-taskpool-collector.html#handling-command-transmission","text":"The commands sent via gateway (e.g. AxonCommandListGateway ) are received by Command Handlers. The latter may accept or reject commands, depending on the state of the aggregate and other components. The AxonCommandListGateway is informed about the command outcome. By default, it will log the outcome to console (success is logged in DEBUG log level, errors are using ERROR log level). In some situations it is required to take care of command outcome. A prominent example is to include a metric for command dispatching errors into monitoring. For doing so, it is possible to provide own handlers for success and error command outcome. For this purpose, please provide a Spring Bean implementing the CommandSuccessHandler and CommandErrorHandler accordingly. Here is an example, how such a handler may look like: @Bean @Primary fun taskCommandErrorHandler (): TaskCommandErrorHandler = object : LoggingTaskCommandErrorHandler ( logger ) { override fun apply ( commandMessage : Any , commandResultMessage : CommandResultMessage < out Any? > ) { logger . info { \"<--------- CUSTOM ERROR HANDLER REPORT --------->\" } super . apply ( commandMessage , commandResultMessage ) logger . info { \"<------------------- END ----------------------->\" } } }","title":"Handling command transmission"},{"location":"reference-guide/components/camunda-taskpool-collector.html#message-codes","text":"Please note that the logger root hierarchy is io.holunda.camunda.taskpool.collector Message Code Severity Logger* Description Meaning COLLECTOR-001 INFO Task commands will be collected. COLLECTOR-002 INFO Task commands not be collected. COLLECTOR-005 DEBUG .process.definition Process definition collecting has been disabled by property, skipping ${command.processDefinitionId}. COLLECTOR-006 DEBUG .process.instance Process instance collecting has been disabled by property, skipping ${command.processInstanceId}. COLLECTOR-007 DEBUG .process.variable Process variable collecting has been disabled by property, skipping ${command.processInstanceId}. COLLECTOR-008 DEBUG .task Task command collecting is disabled by property, would have enriched and sent command $command. ENRICHER-001 INFO Task commands will be enriched with process variables. ENRICHER-002 INFO Task commands will not be enriched. ENRICHER-003 INFO Task commands will be enriched by a custom enricher. ENRICHER-004 DEBUG .task.enricher Could not enrich variables from running execution ${command.sourceReference.executionId}, since it doesn't exist (anymore).","title":"Message codes"},{"location":"reference-guide/components/common-datapool-sender.html","text":"Datapool Collector # Purpose # Datapool collector is a component usually deployed as a part of the process application (but not necessary) that is responsible for collecting the Business Data Events fired by the application in order to allow for creation of a business data projection. In doing so, it collects and transmits it to Datapool Core. Features # Provides an API to submit arbitrary changes of business entities Provides an API to track changes (aka. Audit Log) Authorization on business entries Transmission of business entries commands Usage and configuration # <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> datapool-collector </artifactId> <version> ${camunda-taskpool.version} </version> </dependency> Then activate the datapool collector by providing the annotation on any Spring Configuration: @Configuration @EnableDataEntrySender class MyDataEntryCollectorConfiguration { } Command transmission # In order to control sending of commands to command gateway, the command sender activation property polyflow.integration.sender.data-entry.enabled (default is true ) is available. If disabled, the command sender will log any command instead of sending it to the command gateway. In addition, you can control by the property polyflow.integration.sender.data-entry.type if you want to use the default command sender or provide your own implementation. The default provided command sender (type: simple ) just sends the commands synchronously using Axon Command Bus. TIP: If you want to implement a custom command sending, please provide your own implementation of the interface DataEntryCommandSender (register a Spring Component of the type) and set the property polyflow.integration.sender.data-entry.type to custom . Handling command transmission # The commands sent by the Datapool Collector are received by Command Handlers. The latter may accept or reject commands, depending on the state of the aggregate and other components. The SimpleDataEntryCommandSender is informed about the command outcome. By default, it will log the outcome to console (success is logged in DEBUG log level, errors are using ERROR log level). In some situations it is required to take care of command outcome. A prominent example is to include a metric for command dispatching errors into monitoring. For doing so, it is possible to provide own handlers for success and error command outcome. For Data Entry Command Sender (as a part of Datapool Collector ) please provide a Spring Bean implementing the io.holunda.polyflow.datapool.sender.DataEntryCommandSuccessHandler and io.holunda.polyflow.datapool.sender.DataEntryCommandErrorHandler accordingly. @Bean @Primary fun dataEntryCommandSuccessHandler () = object : DataEntryCommandResultHandler { override fun apply ( commandMessage : Any , commandResultMessage : CommandResultMessage < out Any? > ) { // do something here logger . info { \"Success\" } } } @Bean @Primary fun dataEntryCommandErrorHandler () = object : DataEntryCommandErrorHandler { override fun apply ( commandMessage : Any , commandResultMessage : CommandResultMessage < out Any? > ) { // do something here logger . error { \"Error\" } } }","title":"Datapool Sender"},{"location":"reference-guide/components/common-datapool-sender.html#datapool-collector","text":"","title":"Datapool Collector"},{"location":"reference-guide/components/common-datapool-sender.html#purpose","text":"Datapool collector is a component usually deployed as a part of the process application (but not necessary) that is responsible for collecting the Business Data Events fired by the application in order to allow for creation of a business data projection. In doing so, it collects and transmits it to Datapool Core.","title":"Purpose"},{"location":"reference-guide/components/common-datapool-sender.html#features","text":"Provides an API to submit arbitrary changes of business entities Provides an API to track changes (aka. Audit Log) Authorization on business entries Transmission of business entries commands","title":"Features"},{"location":"reference-guide/components/common-datapool-sender.html#usage-and-configuration","text":"<dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> datapool-collector </artifactId> <version> ${camunda-taskpool.version} </version> </dependency> Then activate the datapool collector by providing the annotation on any Spring Configuration: @Configuration @EnableDataEntrySender class MyDataEntryCollectorConfiguration { }","title":"Usage and configuration"},{"location":"reference-guide/components/common-datapool-sender.html#command-transmission","text":"In order to control sending of commands to command gateway, the command sender activation property polyflow.integration.sender.data-entry.enabled (default is true ) is available. If disabled, the command sender will log any command instead of sending it to the command gateway. In addition, you can control by the property polyflow.integration.sender.data-entry.type if you want to use the default command sender or provide your own implementation. The default provided command sender (type: simple ) just sends the commands synchronously using Axon Command Bus. TIP: If you want to implement a custom command sending, please provide your own implementation of the interface DataEntryCommandSender (register a Spring Component of the type) and set the property polyflow.integration.sender.data-entry.type to custom .","title":"Command transmission"},{"location":"reference-guide/components/common-datapool-sender.html#handling-command-transmission","text":"The commands sent by the Datapool Collector are received by Command Handlers. The latter may accept or reject commands, depending on the state of the aggregate and other components. The SimpleDataEntryCommandSender is informed about the command outcome. By default, it will log the outcome to console (success is logged in DEBUG log level, errors are using ERROR log level). In some situations it is required to take care of command outcome. A prominent example is to include a metric for command dispatching errors into monitoring. For doing so, it is possible to provide own handlers for success and error command outcome. For Data Entry Command Sender (as a part of Datapool Collector ) please provide a Spring Bean implementing the io.holunda.polyflow.datapool.sender.DataEntryCommandSuccessHandler and io.holunda.polyflow.datapool.sender.DataEntryCommandErrorHandler accordingly. @Bean @Primary fun dataEntryCommandSuccessHandler () = object : DataEntryCommandResultHandler { override fun apply ( commandMessage : Any , commandResultMessage : CommandResultMessage < out Any? > ) { // do something here logger . info { \"Success\" } } } @Bean @Primary fun dataEntryCommandErrorHandler () = object : DataEntryCommandErrorHandler { override fun apply ( commandMessage : Any , commandResultMessage : CommandResultMessage < out Any? > ) { // do something here logger . error { \"Error\" } } }","title":"Handling command transmission"},{"location":"reference-guide/components/common-taskpool-sender.html","text":"Taskpool Sender # Purpose # Features # Usage and configuration # <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-taskpool-sender </artifactId> <version> ${camunda-taskpool.version} </version> </dependency> Then activate the taskpool sender by providing the annotation on any Spring Configuration: @Configuration @EnableTaskpoolSender class MyDataEntryCollectorConfiguration { } Command transmission # Handling command transmission # Message codes # Please note that the logger root hierarchy is io.holunda.camunda.taskpool.sender Message Code Severity Logger* Description Meaning SENDER-001 DEBUG .gateway Sending command over gateway disabled by property. Would have sent command payload . Sending of any commands is disabled. SENDER-002 DEBUG .gateway Successfully submitted command payload . Logging the successfully sent command. SENDER-003 ERROR .gateway Sending command $commandMessage resulted in error Error sending command. SENDER-004 DEBUG .task Process task sending is disabled by property. Would have sent $command. SENDER-005 DEBUG .task Handling ${taskCommands.size} commands for task $taskId using command accumulator $accumulatorName SENDER-006 DEBUG .task Handling ${taskCommands.size} commands for task $taskId using command accumulator $accumulatorName SENDER-007 DEBUG .process.definition Process definition sending is disabled by property. Would have sent $command. SENDER-007 DEBUG .process.instance Process instance sending is disabled by property. Would have sent $command. SENDER-009 DEBUG .process.variable Process variable sending is disabled by property. Would have sent $command. SENDER-011 INFO Taskpool task commands will be distributed over command bus. SENDER-012 INFO Taskpool task command distribution is disabled by property. SENDER-013 INFO Taskpool process definition commands will be distributed over command bus. SENDER-014 INFO Taskpool process definition command distribution is disabled by property. SENDER-015 INFO Taskpool process instance commands will be distributed over command bus. SENDER-016 INFO Taskpool process instance command distribution is disabled by property. SENDER-017 INFO Taskpool process variable commands will be distributed over command bus. SENDER-018 INFO Taskpool process variable command distribution is disabled by property.","title":"Taskpool Sender"},{"location":"reference-guide/components/common-taskpool-sender.html#taskpool-sender","text":"","title":"Taskpool Sender"},{"location":"reference-guide/components/common-taskpool-sender.html#purpose","text":"","title":"Purpose"},{"location":"reference-guide/components/common-taskpool-sender.html#features","text":"","title":"Features"},{"location":"reference-guide/components/common-taskpool-sender.html#usage-and-configuration","text":"<dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-taskpool-sender </artifactId> <version> ${camunda-taskpool.version} </version> </dependency> Then activate the taskpool sender by providing the annotation on any Spring Configuration: @Configuration @EnableTaskpoolSender class MyDataEntryCollectorConfiguration { }","title":"Usage and configuration"},{"location":"reference-guide/components/common-taskpool-sender.html#command-transmission","text":"","title":"Command transmission"},{"location":"reference-guide/components/common-taskpool-sender.html#handling-command-transmission","text":"","title":"Handling command transmission"},{"location":"reference-guide/components/common-taskpool-sender.html#message-codes","text":"Please note that the logger root hierarchy is io.holunda.camunda.taskpool.sender Message Code Severity Logger* Description Meaning SENDER-001 DEBUG .gateway Sending command over gateway disabled by property. Would have sent command payload . Sending of any commands is disabled. SENDER-002 DEBUG .gateway Successfully submitted command payload . Logging the successfully sent command. SENDER-003 ERROR .gateway Sending command $commandMessage resulted in error Error sending command. SENDER-004 DEBUG .task Process task sending is disabled by property. Would have sent $command. SENDER-005 DEBUG .task Handling ${taskCommands.size} commands for task $taskId using command accumulator $accumulatorName SENDER-006 DEBUG .task Handling ${taskCommands.size} commands for task $taskId using command accumulator $accumulatorName SENDER-007 DEBUG .process.definition Process definition sending is disabled by property. Would have sent $command. SENDER-007 DEBUG .process.instance Process instance sending is disabled by property. Would have sent $command. SENDER-009 DEBUG .process.variable Process variable sending is disabled by property. Would have sent $command. SENDER-011 INFO Taskpool task commands will be distributed over command bus. SENDER-012 INFO Taskpool task command distribution is disabled by property. SENDER-013 INFO Taskpool process definition commands will be distributed over command bus. SENDER-014 INFO Taskpool process definition command distribution is disabled by property. SENDER-015 INFO Taskpool process instance commands will be distributed over command bus. SENDER-016 INFO Taskpool process instance command distribution is disabled by property. SENDER-017 INFO Taskpool process variable commands will be distributed over command bus. SENDER-018 INFO Taskpool process variable command distribution is disabled by property.","title":"Message codes"},{"location":"reference-guide/components/core-datapool.html","text":"Datapool Core # Purpose # The component is responsible for maintaining and storing the consistent state of the datapool core concept of Business Data Entry. The component receives all commands and emits events, if changes are performed on underlying entities. The event stream is used to store all changes (purely event-sourced) and should be used by all other parties interested in changes. Configuration # Component activation # In order to activate Datapool Core component, please include the following dependency to your application <dependency> <groupId> io.holunda.taskpool </grouId> <artifactId> camunda-bpm-datapool-core </artifactId> <version> ${taskpool.version} </version> </dependency> and activate its configuration by adding the following to a Spring configuration: @Configuration @EnablePolyflowDataPool class MyConfiguration Revision-Aware Projection # The in-memory data entry projection is supporting revision-aware projection queries. To activate this, you need to activate the correlation of revision attributes between your data entries commands and the data entry events. To do so, please activate the correlation provider by putting the following code snippet in the application containing the Datapool Core Component: @Configuration @EnablePolyflowDataPool class MyConfiguration { @Bean fun revisionAwareCorrelationDataProvider (): CorrelationDataProvider { return MultiCorrelationDataProvider < CommandMessage < Any >> ( listOf ( MessageOriginProvider (), SimpleCorrelationDataProvider ( RevisionValue . REVISION_KEY ) ) ) } } By doing so, if a command is sending revision information, it will be passed to the resulting event and will be received by the projection, so the latter will deliver revision information in query results. The use of RevisionAwareQueryGateway will allow querying for specific revisions in the data entry projection, see documentation of axon-gateway-extension project. Strategies to optimize data entry access # The Business Data Entry is implemented using an Aggregate pattern and the corresponding projection as a part of the view component. By default, the Datapool Sender is used to send commands expressing the change of a Business Data Entry to the Aggregate Root , which is emitting the corresponding event. If you are dealing with Business Data Entries with a very long lifetime, the number of events emitted by the Aggregate Root may become large and impacts the load time of it (it is event-sourced). To improve the load time of the aggregate, we developed two strategies which can be applied: special repository and snapshotting. Snapshotting uses standard Axon Snapshotting and will use the latest snapshot and additionally apply the events emitted after the last snapshot instead of replaying all events ever emitted by the aggregate. The special repository uses the fact that Data Entry Aggregate Root state is not changed by update events and the first event it emits during creation already contains everything the aggregates require during loading. In order to select the strategy best matching your use case, please consult the configuration section .","title":"Datapool Core"},{"location":"reference-guide/components/core-datapool.html#datapool-core","text":"","title":"Datapool Core"},{"location":"reference-guide/components/core-datapool.html#purpose","text":"The component is responsible for maintaining and storing the consistent state of the datapool core concept of Business Data Entry. The component receives all commands and emits events, if changes are performed on underlying entities. The event stream is used to store all changes (purely event-sourced) and should be used by all other parties interested in changes.","title":"Purpose"},{"location":"reference-guide/components/core-datapool.html#configuration","text":"","title":"Configuration"},{"location":"reference-guide/components/core-datapool.html#component-activation","text":"In order to activate Datapool Core component, please include the following dependency to your application <dependency> <groupId> io.holunda.taskpool </grouId> <artifactId> camunda-bpm-datapool-core </artifactId> <version> ${taskpool.version} </version> </dependency> and activate its configuration by adding the following to a Spring configuration: @Configuration @EnablePolyflowDataPool class MyConfiguration","title":"Component activation"},{"location":"reference-guide/components/core-datapool.html#revision-aware-projection","text":"The in-memory data entry projection is supporting revision-aware projection queries. To activate this, you need to activate the correlation of revision attributes between your data entries commands and the data entry events. To do so, please activate the correlation provider by putting the following code snippet in the application containing the Datapool Core Component: @Configuration @EnablePolyflowDataPool class MyConfiguration { @Bean fun revisionAwareCorrelationDataProvider (): CorrelationDataProvider { return MultiCorrelationDataProvider < CommandMessage < Any >> ( listOf ( MessageOriginProvider (), SimpleCorrelationDataProvider ( RevisionValue . REVISION_KEY ) ) ) } } By doing so, if a command is sending revision information, it will be passed to the resulting event and will be received by the projection, so the latter will deliver revision information in query results. The use of RevisionAwareQueryGateway will allow querying for specific revisions in the data entry projection, see documentation of axon-gateway-extension project.","title":"Revision-Aware Projection"},{"location":"reference-guide/components/core-datapool.html#strategies-to-optimize-data-entry-access","text":"The Business Data Entry is implemented using an Aggregate pattern and the corresponding projection as a part of the view component. By default, the Datapool Sender is used to send commands expressing the change of a Business Data Entry to the Aggregate Root , which is emitting the corresponding event. If you are dealing with Business Data Entries with a very long lifetime, the number of events emitted by the Aggregate Root may become large and impacts the load time of it (it is event-sourced). To improve the load time of the aggregate, we developed two strategies which can be applied: special repository and snapshotting. Snapshotting uses standard Axon Snapshotting and will use the latest snapshot and additionally apply the events emitted after the last snapshot instead of replaying all events ever emitted by the aggregate. The special repository uses the fact that Data Entry Aggregate Root state is not changed by update events and the first event it emits during creation already contains everything the aggregates require during loading. In order to select the strategy best matching your use case, please consult the configuration section .","title":"Strategies to optimize data entry access"},{"location":"reference-guide/components/core-taskpool.html","text":"Taskpool Core # Purpose # The component is responsible for maintaining and storing the consistent state of the taskpool core concepts: Task (represents a user task instance) Process Definition (represents a process definition) The component receives all commands and emits events, if changes are performed on underlying entities. The event stream is used to store all changes (purely event-sourced) and should be used by all other parties interested in changes.","title":"Taskpool Core"},{"location":"reference-guide/components/core-taskpool.html#taskpool-core","text":"","title":"Taskpool Core"},{"location":"reference-guide/components/core-taskpool.html#purpose","text":"The component is responsible for maintaining and storing the consistent state of the taskpool core concepts: Task (represents a user task instance) Process Definition (represents a process definition) The component receives all commands and emits events, if changes are performed on underlying entities. The event stream is used to store all changes (purely event-sourced) and should be used by all other parties interested in changes.","title":"Purpose"},{"location":"reference-guide/components/other-bus-jackson.html","text":"Bus Jackson # Purpose # The component is a helper component if you configure your Axon busses (command, event, query) to use Jackson for serialization of messages. It provides helper Jackson Modules to configure serialization of classes used by Polyflow. Configuration and Usage # To use the component, please add the following dependency to your classpath <dependency> <groupId> io.holunda.taskpool </grouId> <artifactId> polyflow-bus-jackson </artifactId> </dependency> Inside your Object Mapper configuration call import io.holunda.polyflow.bus.jackson.configureTaskpoolJacksonObjectMapper class MyConfiguration { @Bean fun objectMapper (): ObjectMapper { return ObjectMapper (). configureTaskpoolJacksonObjectMapper () } }","title":"Bus Jackson"},{"location":"reference-guide/components/other-bus-jackson.html#bus-jackson","text":"","title":"Bus Jackson"},{"location":"reference-guide/components/other-bus-jackson.html#purpose","text":"The component is a helper component if you configure your Axon busses (command, event, query) to use Jackson for serialization of messages. It provides helper Jackson Modules to configure serialization of classes used by Polyflow.","title":"Purpose"},{"location":"reference-guide/components/other-bus-jackson.html#configuration-and-usage","text":"To use the component, please add the following dependency to your classpath <dependency> <groupId> io.holunda.taskpool </grouId> <artifactId> polyflow-bus-jackson </artifactId> </dependency> Inside your Object Mapper configuration call import io.holunda.polyflow.bus.jackson.configureTaskpoolJacksonObjectMapper class MyConfiguration { @Bean fun objectMapper (): ObjectMapper { return ObjectMapper (). configureTaskpoolJacksonObjectMapper () } }","title":"Configuration and Usage"},{"location":"reference-guide/components/other-tasklist-url-resolver.html","text":"Tasklist URL Resolver # Purpose # The Tasklist URL Resolver is a helper component helping to provide the URL of the task list for other components. It is not use by other components, but is helpful, if you complete tasks using SPA on the side of the process application and needs a redirection target resolution of the task list after completion. Usage and Configuration # To use Tasklist URL Resolver please add the following artifact to your classpath: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-tasklist-url-resolver </artifactId> </dependency> In your application.yml either configure the property for the static tasklist URL: polyflow: integration: tasklist: tasklist-url: http://my-task-list.application.url/ or provide your own implementation of the TasklistUrlResolver interface as Spring Bean in your configuration: import java.beans.BeanProperty ; @Configuration class MyConfiguration { @Bean public TasklistUrlResolver myTasklistUrlResolver () { return MyTasklistUrlResolver (); } } .","title":"Tasklist URL Resolver"},{"location":"reference-guide/components/other-tasklist-url-resolver.html#tasklist-url-resolver","text":"","title":"Tasklist URL Resolver"},{"location":"reference-guide/components/other-tasklist-url-resolver.html#purpose","text":"The Tasklist URL Resolver is a helper component helping to provide the URL of the task list for other components. It is not use by other components, but is helpful, if you complete tasks using SPA on the side of the process application and needs a redirection target resolution of the task list after completion.","title":"Purpose"},{"location":"reference-guide/components/other-tasklist-url-resolver.html#usage-and-configuration","text":"To use Tasklist URL Resolver please add the following artifact to your classpath: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-tasklist-url-resolver </artifactId> </dependency> In your application.yml either configure the property for the static tasklist URL: polyflow: integration: tasklist: tasklist-url: http://my-task-list.application.url/ or provide your own implementation of the TasklistUrlResolver interface as Spring Bean in your configuration: import java.beans.BeanProperty ; @Configuration class MyConfiguration { @Bean public TasklistUrlResolver myTasklistUrlResolver () { return MyTasklistUrlResolver (); } } .","title":"Usage and Configuration"},{"location":"reference-guide/components/other-variable-serializer.html","text":"Variable Serializer # Purpose # Configuration #","title":"Variable Serializer"},{"location":"reference-guide/components/other-variable-serializer.html#variable-serializer","text":"","title":"Variable Serializer"},{"location":"reference-guide/components/other-variable-serializer.html#purpose","text":"","title":"Purpose"},{"location":"reference-guide/components/other-variable-serializer.html#configuration","text":"","title":"Configuration"},{"location":"reference-guide/components/view-api-client.html","text":"Purpose # The Polyflow View API Client is a client for the users of the task-pool and the data-pool query API. It provides simple components which can be used in order to query the configured views. By doing so, it defines an easy-to-use API for callers . Usage # Pleas put the following component to you class path: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-view-api-client </artifactId> </dependency> The components available are: io.holunda.polyflow.view.DataEntryQueryClient io.holunda.polyflow.view.ProcessDefinitionQueryClient io.holunda.polyflow.view.ProcessInstanceQueryClient io.holunda.polyflow.view.ProcessVariableQueryClient io.holunda.polyflow.view.TaskQueryClient If you are using Kotlin, you might like the extension functions of the QueryGateway provided by io.holunda.polyflow.view.QueryGatewayExt object.","title":"View API Client"},{"location":"reference-guide/components/view-api-client.html#purpose","text":"The Polyflow View API Client is a client for the users of the task-pool and the data-pool query API. It provides simple components which can be used in order to query the configured views. By doing so, it defines an easy-to-use API for callers .","title":"Purpose"},{"location":"reference-guide/components/view-api-client.html#usage","text":"Pleas put the following component to you class path: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-view-api-client </artifactId> </dependency> The components available are: io.holunda.polyflow.view.DataEntryQueryClient io.holunda.polyflow.view.ProcessDefinitionQueryClient io.holunda.polyflow.view.ProcessInstanceQueryClient io.holunda.polyflow.view.ProcessVariableQueryClient io.holunda.polyflow.view.TaskQueryClient If you are using Kotlin, you might like the extension functions of the QueryGateway provided by io.holunda.polyflow.view.QueryGatewayExt object.","title":"Usage"},{"location":"reference-guide/components/view-api.html","text":"Purpose # Note If you are looking for a convenient way to send out queries (API for callers ), please check the View API Client The Polyflow View API defines the interfaces for the implementers of the task-pool and the data-pool query API. It defines the main query types of the common read-projections. Its main purpose is to create a public stable API which is independent of the implementations. There are multiple implementations available: In-Memory View JPA View Mongo DB View In addition, the API supplies filtering functionality for handling requests of filtering of view results in form of attribute filters ( like attribute=value&attrubute2=value2&task.name=taskname ). Especially, it defines the main concepts like Criteria and Operator and generic query paging and sorting. Feature support matrix # Task API # The Task API allows to query for tasks handled by the task-pool. Query Type Description Payload types In-Memory JPA Mongo DB TasksForUserQuery Retrieves a list of tasks accessible by the user (filters on username and groups only) List yes yes yes TaskForIdQuery Retrieves a task by id (without any other filters) Task or null yes yes yes TasksForApplicationQuery Retrieves all tasks by given application name (without any further filters) List yes yes yes TaskWithDataEntriesForIdQuery Retrieves a task by id and correlates result with data entries, if available (Task, List ) or null yes yes yes TasksWithDataEntriesForUserQuery Retrieves a list of tasks accessible by the user and applying additional filters and correlates result with data entries, if available List<(Task, List ) yes incubation yes TaskCountByApplicationQuery Counts tasks grouped by application names, useful for monitoring List<(ApplicationName, Count)> yes no yes Process Definition API # The Process Definition API allows to query for process definitions handled by the task-pool. Query Type Description Payload types In-Memory JPA Mongo DB ProcessDefinitionsStartableByUserQuery Retrieves a list of process definitions start-able by user List yes yes yes Process Instance API # The Process Instance API allows to query for process instances handled by the task-pool. Query Type Description Payload types In-Memory JPA Mongo DB ProcessInstancesByStateQuery Retrieves a list of process instances by state (started, finished, etc) List yes yes no Process Variable API (incubation) # The Process Variable API allows to query for process variables handled by the task-pool. Warning The Process Variable API is supporting revision-aware queries, which are currently only supported by JPA and In-Memory implementations. Query Type Description Payload types In-Memory JPA Mongo DB ProcessVariablesForInstanceQuery Retrieves a list of process variables for given process instance and matching provided filters List yes no no Data Entry API # The Data Entry API allows to query for data entries handled by the data-pool. Warning The Data Entry API is supporting revision-aware queries, which are currently only supported by JPA and In-Memory implementations. Query Type Description Payload types In-Memory JPA Mongo DB DataEntriesForUserQuery Retrieves a list of data entries accessible by the user with some additional filters. List yes yes yes DataEntryForIdentityQuery Retrieves a a list by type and an optional id (without any other filters) List yes yes yes DataEntriesQuery Retrieves a list of data entries matching filters List yes yes yes Revision-aware query support # Projections can be built in a way, that they support and store event revision information transported by the event metadata. By doing so, you might send an update of the model by specifying the update revision and are waiting for the eventually consistent event delivery to the projection of this update. In order to achieve this, you might specify the minimum revision the query result must fulfill in order to match your query request. See axon-gateway-extension for more details. Please note, that not all implementations are implementing this feature. Especially, Mongo DB View is currently NOT SUPPORTING Revision Aware queries.","title":"View API"},{"location":"reference-guide/components/view-api.html#purpose","text":"Note If you are looking for a convenient way to send out queries (API for callers ), please check the View API Client The Polyflow View API defines the interfaces for the implementers of the task-pool and the data-pool query API. It defines the main query types of the common read-projections. Its main purpose is to create a public stable API which is independent of the implementations. There are multiple implementations available: In-Memory View JPA View Mongo DB View In addition, the API supplies filtering functionality for handling requests of filtering of view results in form of attribute filters ( like attribute=value&attrubute2=value2&task.name=taskname ). Especially, it defines the main concepts like Criteria and Operator and generic query paging and sorting.","title":"Purpose"},{"location":"reference-guide/components/view-api.html#feature-support-matrix","text":"","title":"Feature support matrix"},{"location":"reference-guide/components/view-api.html#task-api","text":"The Task API allows to query for tasks handled by the task-pool. Query Type Description Payload types In-Memory JPA Mongo DB TasksForUserQuery Retrieves a list of tasks accessible by the user (filters on username and groups only) List yes yes yes TaskForIdQuery Retrieves a task by id (without any other filters) Task or null yes yes yes TasksForApplicationQuery Retrieves all tasks by given application name (without any further filters) List yes yes yes TaskWithDataEntriesForIdQuery Retrieves a task by id and correlates result with data entries, if available (Task, List ) or null yes yes yes TasksWithDataEntriesForUserQuery Retrieves a list of tasks accessible by the user and applying additional filters and correlates result with data entries, if available List<(Task, List ) yes incubation yes TaskCountByApplicationQuery Counts tasks grouped by application names, useful for monitoring List<(ApplicationName, Count)> yes no yes","title":"Task API"},{"location":"reference-guide/components/view-api.html#process-definition-api","text":"The Process Definition API allows to query for process definitions handled by the task-pool. Query Type Description Payload types In-Memory JPA Mongo DB ProcessDefinitionsStartableByUserQuery Retrieves a list of process definitions start-able by user List yes yes yes","title":"Process Definition API"},{"location":"reference-guide/components/view-api.html#process-instance-api","text":"The Process Instance API allows to query for process instances handled by the task-pool. Query Type Description Payload types In-Memory JPA Mongo DB ProcessInstancesByStateQuery Retrieves a list of process instances by state (started, finished, etc) List yes yes no","title":"Process Instance API"},{"location":"reference-guide/components/view-api.html#process-variable-api-incubation","text":"The Process Variable API allows to query for process variables handled by the task-pool. Warning The Process Variable API is supporting revision-aware queries, which are currently only supported by JPA and In-Memory implementations. Query Type Description Payload types In-Memory JPA Mongo DB ProcessVariablesForInstanceQuery Retrieves a list of process variables for given process instance and matching provided filters List yes no no","title":"Process Variable API (incubation)"},{"location":"reference-guide/components/view-api.html#data-entry-api","text":"The Data Entry API allows to query for data entries handled by the data-pool. Warning The Data Entry API is supporting revision-aware queries, which are currently only supported by JPA and In-Memory implementations. Query Type Description Payload types In-Memory JPA Mongo DB DataEntriesForUserQuery Retrieves a list of data entries accessible by the user with some additional filters. List yes yes yes DataEntryForIdentityQuery Retrieves a a list by type and an optional id (without any other filters) List yes yes yes DataEntriesQuery Retrieves a list of data entries matching filters List yes yes yes","title":"Data Entry API"},{"location":"reference-guide/components/view-api.html#revision-aware-query-support","text":"Projections can be built in a way, that they support and store event revision information transported by the event metadata. By doing so, you might send an update of the model by specifying the update revision and are waiting for the eventually consistent event delivery to the projection of this update. In order to achieve this, you might specify the minimum revision the query result must fulfill in order to match your query request. See axon-gateway-extension for more details. Please note, that not all implementations are implementing this feature. Especially, Mongo DB View is currently NOT SUPPORTING Revision Aware queries.","title":"Revision-aware query support"},{"location":"reference-guide/components/view-form-url-resolver.html","text":"Purpose # Building a central Task List and Business Data Entry List requires a UI integration logic between the use case agnostic central component and use case specific forms for display of the User Tasks and Business Data Entries. There exist multiple options to provide this UI integration, like dynamic loading of UI components from the distributed use case specific application and process engines or simple redirection to those applications being able to display the requested form. In many those implementations, the process platform components like Task List and Business Data Entry List need to resolve the particular URL of the process application endpoint. The form-url-resolver component is designed exactly for this purpose, if this resolution is static and can be performed based on configuration. Configuration # In order to use the form-url-resolver please add the following dependency to your project: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-form-url-resolver </artifactId> <version> ${polyflow.version} </version> </dependency> In your configuration, please add the following annotation to enable the resolver: @EnablePropertyBasedFormUrlResolver @Configuration class MyTasklistConfiguration { } Using your application.yaml or corresponding properties file, please provide the configuration of the URLs. Te configuration is separated into provision of some defaults (for user tasks, data entries, processes or even entire applications) In general the resolution of the URL happens from most specific to most generic, see the example below. If the specific request matches the configuration (e.g. URL for the User Task with process definition task1 of the application with application name app1 ) it will be returned ( https://app1.server.io/app/forms/task1/foo/${id} ), otherwise the default for the application or even the default template is taken. polyflow : integration : form-url-resolver : defaultApplicationTemplate : \"http://localhost:8080/${applicationName}\" defaultDataEntryTemplate : \"/${entryType}/${entryId}\" defaultProcessTemplate : \"/${processDefinitionKey}/${formKey}\" defaultTaskTemplate : \"/forms/${formKey}/${id}\" applications : - app1 : url : \"https://app1.server.io/app\" tasks : - task1 : \"/forms/task1/foo/${id}\" - task2 : \"/bar/2/foo/${id}\" processes : - process1 : \"/proc-1/start\" - process2 : \"/proc/2/begin\" - app2 : url : \"https://foo.app2.com\" tasks : - otherTask1 : \"/views/task1/${id}\" - otherTask2 : \"/other/2/foo/${id}\" As you can see in the example above, the component supports simple text templating using ${} to indicate the template variable. The variables which can be used are direct attributes of the object for which the URL is resolved (see View API ). The keys in the configuration are: Value of attribute applicationName for applications Value of attribute processDefinitionKey for processes Value of attribute taskDefinitionKey for tasks Value of attribute entryType for data entries","title":"Form URL Resolver"},{"location":"reference-guide/components/view-form-url-resolver.html#purpose","text":"Building a central Task List and Business Data Entry List requires a UI integration logic between the use case agnostic central component and use case specific forms for display of the User Tasks and Business Data Entries. There exist multiple options to provide this UI integration, like dynamic loading of UI components from the distributed use case specific application and process engines or simple redirection to those applications being able to display the requested form. In many those implementations, the process platform components like Task List and Business Data Entry List need to resolve the particular URL of the process application endpoint. The form-url-resolver component is designed exactly for this purpose, if this resolution is static and can be performed based on configuration.","title":"Purpose"},{"location":"reference-guide/components/view-form-url-resolver.html#configuration","text":"In order to use the form-url-resolver please add the following dependency to your project: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-form-url-resolver </artifactId> <version> ${polyflow.version} </version> </dependency> In your configuration, please add the following annotation to enable the resolver: @EnablePropertyBasedFormUrlResolver @Configuration class MyTasklistConfiguration { } Using your application.yaml or corresponding properties file, please provide the configuration of the URLs. Te configuration is separated into provision of some defaults (for user tasks, data entries, processes or even entire applications) In general the resolution of the URL happens from most specific to most generic, see the example below. If the specific request matches the configuration (e.g. URL for the User Task with process definition task1 of the application with application name app1 ) it will be returned ( https://app1.server.io/app/forms/task1/foo/${id} ), otherwise the default for the application or even the default template is taken. polyflow : integration : form-url-resolver : defaultApplicationTemplate : \"http://localhost:8080/${applicationName}\" defaultDataEntryTemplate : \"/${entryType}/${entryId}\" defaultProcessTemplate : \"/${processDefinitionKey}/${formKey}\" defaultTaskTemplate : \"/forms/${formKey}/${id}\" applications : - app1 : url : \"https://app1.server.io/app\" tasks : - task1 : \"/forms/task1/foo/${id}\" - task2 : \"/bar/2/foo/${id}\" processes : - process1 : \"/proc-1/start\" - process2 : \"/proc/2/begin\" - app2 : url : \"https://foo.app2.com\" tasks : - otherTask1 : \"/views/task1/${id}\" - otherTask2 : \"/other/2/foo/${id}\" As you can see in the example above, the component supports simple text templating using ${} to indicate the template variable. The variables which can be used are direct attributes of the object for which the URL is resolved (see View API ). The keys in the configuration are: Value of attribute applicationName for applications Value of attribute processDefinitionKey for processes Value of attribute taskDefinitionKey for tasks Value of attribute entryType for data entries","title":"Configuration"},{"location":"reference-guide/components/view-jpa.html","text":"Purpose # The JPA View is component responsible for creating read-projections of tasks and business data entries. It currently implements Datapool View API and Taskpool API and persists the projection as entities and relations in a RDBMS using JPA. It is a useful if the JPA persistence is already used in the project setup. Features # stores representation of business data entries stores representation of process definitions stores representation of process instances provides single query API supporting single and subscription queries Configuration options # In order to activate the JPA View implementation, please include the following dependency on your classpath: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-view-jpa </artifactId> <version> ${polyflow.version} </version> </dependency> The implementation relies on Spring Data JPA and needs to activate those. @Configuration @EnablePolyflowJpaView public class MyViewConfiguration { } In addition, configure a database connection to database using application.properties or application.yaml : spring: jpa: show-sql: false open-in-view: true # disable JPA warning datasource: url: <jdbc-connnection-string> username: <db-user> password: <db-password> The JPA view uses a special facility for creating search indexes on unstructured payload. For this purpose it converts the payload into a recursive map structure (in which every primitive type is a leaf and every complex type is decomposed via the map) using Jackson ObjectMapper and then create search indexes for all property paths ( myObj1.myProperty2.myOtherEmbeddedProperty3 ) and their values. You can provide some configuration of this indexing process by the following configuration options: polyflow.view.jpa: payload-attribute-level-limit: 2 stored-items: task, data-entry, process-instance, process-definition data-entry-filters: include: myProperty2.myOtherEmbeddedProperty3, myProperty2.myOtherEmbeddedProperty2 # exclude: myProperty In the example below you see the configuration of the limit of keying depth and usage of include/exclude filters of the keys. In addition, the stored-items property is holding a set of items to be persisted to the database. The possible values of stored items are: task , data-entry , process-instance and process-definition . By setting this property, you can disable storage of items not required by your application and save space consumption of your database. The property defaults to data-entry . Entity Scan # The JPA View utilizes Spring Data repositories and Hibernate entities inside the persistence layer. As a result, it declares a @EntityScan and @EnableJpaRepositories annotations pointing at the corresponding locations. If you are using Spring Data JPA on your own, you will need to add the @EntityScan and @EnableJpaRepositores annotation pointing at your packages. In addition, please check Persistence configuration . Logging # The view implementation provides runtime details using standard logging facility. If you want to increase the logging level, please setup it e.g. in your application.yaml : logging.level: io.holunda.polyflow.view.jpa: DEBUG DB Tables # The JPA View uses several tables to store the results. These are: PLF_DATA_ENTRY : table for business data entries PLF_DATA_ENTRY_AUTHORIZATIONS : table for authorization information of data entries PLF_DATA_ENTRY_PAYLOAD_ATTRIBUTES : table for data entry attribute search index PLF_DATA_ENTRY_PROTOCOL : table for data entry protocol entry (users, groups) PLF_PROC_DEF : table for process definitions PLF_PROC_DEF_AUTHORIZATIONS : table for authorization information of process definitions PLF_PROC_INSTANCE : table for process instances PLF_TASK : table for user tasks PLF_TASK_AUTHORIZATIONS : table for authorization information of user tasks PLF_TASK_CORRELATIONS : table for user task correlation information PLF_TASK_PAYLOAD_ATTRIBUTES : table for user task attribute search index TRACKING_TOKEN : table for Axon Tracking Tokens If you are interested in DDLs for the view, feel free to generate one using the following call of Apache Maven mvn -Pgenerate-sql -f view/jpa . Currently, DDLs for the databases H2, MSSQL and PostgreSQL are generated into target/ directory.","title":"JPA View"},{"location":"reference-guide/components/view-jpa.html#purpose","text":"The JPA View is component responsible for creating read-projections of tasks and business data entries. It currently implements Datapool View API and Taskpool API and persists the projection as entities and relations in a RDBMS using JPA. It is a useful if the JPA persistence is already used in the project setup.","title":"Purpose"},{"location":"reference-guide/components/view-jpa.html#features","text":"stores representation of business data entries stores representation of process definitions stores representation of process instances provides single query API supporting single and subscription queries","title":"Features"},{"location":"reference-guide/components/view-jpa.html#configuration-options","text":"In order to activate the JPA View implementation, please include the following dependency on your classpath: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-view-jpa </artifactId> <version> ${polyflow.version} </version> </dependency> The implementation relies on Spring Data JPA and needs to activate those. @Configuration @EnablePolyflowJpaView public class MyViewConfiguration { } In addition, configure a database connection to database using application.properties or application.yaml : spring: jpa: show-sql: false open-in-view: true # disable JPA warning datasource: url: <jdbc-connnection-string> username: <db-user> password: <db-password> The JPA view uses a special facility for creating search indexes on unstructured payload. For this purpose it converts the payload into a recursive map structure (in which every primitive type is a leaf and every complex type is decomposed via the map) using Jackson ObjectMapper and then create search indexes for all property paths ( myObj1.myProperty2.myOtherEmbeddedProperty3 ) and their values. You can provide some configuration of this indexing process by the following configuration options: polyflow.view.jpa: payload-attribute-level-limit: 2 stored-items: task, data-entry, process-instance, process-definition data-entry-filters: include: myProperty2.myOtherEmbeddedProperty3, myProperty2.myOtherEmbeddedProperty2 # exclude: myProperty In the example below you see the configuration of the limit of keying depth and usage of include/exclude filters of the keys. In addition, the stored-items property is holding a set of items to be persisted to the database. The possible values of stored items are: task , data-entry , process-instance and process-definition . By setting this property, you can disable storage of items not required by your application and save space consumption of your database. The property defaults to data-entry .","title":"Configuration options"},{"location":"reference-guide/components/view-jpa.html#entity-scan","text":"The JPA View utilizes Spring Data repositories and Hibernate entities inside the persistence layer. As a result, it declares a @EntityScan and @EnableJpaRepositories annotations pointing at the corresponding locations. If you are using Spring Data JPA on your own, you will need to add the @EntityScan and @EnableJpaRepositores annotation pointing at your packages. In addition, please check Persistence configuration .","title":"Entity Scan"},{"location":"reference-guide/components/view-jpa.html#logging","text":"The view implementation provides runtime details using standard logging facility. If you want to increase the logging level, please setup it e.g. in your application.yaml : logging.level: io.holunda.polyflow.view.jpa: DEBUG","title":"Logging"},{"location":"reference-guide/components/view-jpa.html#db-tables","text":"The JPA View uses several tables to store the results. These are: PLF_DATA_ENTRY : table for business data entries PLF_DATA_ENTRY_AUTHORIZATIONS : table for authorization information of data entries PLF_DATA_ENTRY_PAYLOAD_ATTRIBUTES : table for data entry attribute search index PLF_DATA_ENTRY_PROTOCOL : table for data entry protocol entry (users, groups) PLF_PROC_DEF : table for process definitions PLF_PROC_DEF_AUTHORIZATIONS : table for authorization information of process definitions PLF_PROC_INSTANCE : table for process instances PLF_TASK : table for user tasks PLF_TASK_AUTHORIZATIONS : table for authorization information of user tasks PLF_TASK_CORRELATIONS : table for user task correlation information PLF_TASK_PAYLOAD_ATTRIBUTES : table for user task attribute search index TRACKING_TOKEN : table for Axon Tracking Tokens If you are interested in DDLs for the view, feel free to generate one using the following call of Apache Maven mvn -Pgenerate-sql -f view/jpa . Currently, DDLs for the databases H2, MSSQL and PostgreSQL are generated into target/ directory.","title":"DB Tables"},{"location":"reference-guide/components/view-mongo.html","text":"Purpose # The Mongo View is component responsible for creating read-projections of tasks and business data entries. It implements the Taskpool and Datapool View API and persists the projection as document collections in a Mongo database. Features # stores JSON document representation of enriched tasks, process definitions and business data entries provides single query API provides subscription query API (reactive) switchable subscription query API (AxonServer or MongoDB ChangeStream) Warning Mongo DB View is currently NOT SUPPORTING Revision Aware queries. Configuration options # In order to activate the Mongo implementation, please include the following dependency on your classpath: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-view-mongo </artifactId> <version> ${polyflow.version} </version> </dependency> The implementation relies on Spring Data Mongo and needs to activate those. Please add the following annotation to any class marked as Spring Configuration loaded during initialization: @Configuration @EnablePolyflowMongoView @Import ({ org . springframework . boot . autoconfigure . mongo . MongoAutoConfiguration . class , org . springframework . boot . autoconfigure . mongo . MongoReactiveAutoConfiguration . class , org . springframework . boot . autoconfigure . data . mongo . MongoDataAutoConfiguration . class , org . springframework . boot . autoconfigure . data . mongo . MongoReactiveDataAutoConfiguration . class }) public class MyViewConfiguration { } In addition, configure a Mongo connection to database called tasks-payload using application.properties or application.yaml : spring: data: mongodb: database: tasks-payload host: localhost port: 27017 The view implementation provides runtime details using standard logging facility. If you want to increase the logging level, please set up it e.g. in your application.yaml : logging.level.io.holunda.polyflow.view.mongo: DEBUG For further configuration, please check Mongo DB View Configuration Collections # The Mongo View uses several collections to store the results. These are: data-entries: collection for business data entries processes: collection for process definitions tasks: collection for user tasks tracking-tokens: collection for Axon Tracking Tokens Data Entries Collection # The data entries collection stores the business data entries in a uniform Datapool format. Here is an example: { \"_id\" : \"io.holunda.camunda.taskpool.example.ApprovalRequest#2db47ced-83d4-4c74-a644-44dd738935f8\" , \"entryType\" : \"io.holunda.camunda.taskpool.example.ApprovalRequest\" , \"payload\" : { \"amount\" : \"900.00\" , \"subject\" : \"Advanced training\" , \"currency\" : \"EUR\" , \"id\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"applicant\" : \"hulk\" }, \"correlations\" : {}, \"type\" : \"Approval Request\" , \"name\" : \"AR 2db47ced-83d4-4c74-a644-44dd738935f8\" , \"applicationName\" : \"example-process-approval\" , \"description\" : \"Advanced training\" , \"state\" : \"Submitted\" , \"statusType\" : \"IN_PROGRESS\" , \"authorizedUsers\" : [ \"gonzo\" , \"hulk\" ], \"authorizedGroups\" : [], \"protocol\" : [ { \"time\" : ISODa te ( \"2019-08-21T09:12:54.779Z\" ) , \"statusType\" : \"PRELIMINARY\" , \"state\" : \"Draft\" , \"username\" : \"gonzo\" , \"logMessage\" : \"Draft created.\" , \"logDetails\" : \"Request draft on behalf of hulk created.\" }, { \"time\" : ISODa te ( \"2019-08-21T09:12:55.060Z\" ) , \"statusType\" : \"IN_PROGRESS\" , \"state\" : \"Submitted\" , \"username\" : \"gonzo\" , \"logMessage\" : \"New approval request submitted.\" } ] } Tasks Collections # Tasks are stored in the following format (an example): { \"_id\" : \"dc1abe54-c3f3-11e9-86e8-4ab58cfe8f17\" , \"sourceReference\" : { \"_id\" : \"dc173bca-c3f3-11e9-86e8-4ab58cfe8f17\" , \"executionId\" : \"dc1a9742-c3f3-11e9-86e8-4ab58cfe8f17\" , \"definitionId\" : \"process_approve_request:1:91f2ff26-a64b-11e9-b117-3e6d125b91e2\" , \"definitionKey\" : \"process_approve_request\" , \"name\" : \"Request Approval\" , \"applicationName\" : \"example-process-approval\" , \"_class\" : \"process\" }, \"taskDefinitionKey\" : \"user_approve_request\" , \"payload\" : { \"request\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"originator\" : \"gonzo\" }, \"correlations\" : { \"io:holunda:camunda:taskpool:example:ApprovalRequest\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"io:holunda:camunda:taskpool:example:User\" : \"gonzo\" }, \"dataEntriesRefs\" : [ \"io.holunda.camunda.taskpool.example.ApprovalRequest#2db47ced-83d4-4c74-a644-44dd738935f8\" , \"io.holunda.camunda.taskpool.example.User#gonzo\" ], \"businessKey\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"name\" : \"Approve Request\" , \"description\" : \"Please approve request 2db47ced-83d4-4c74-a644-44dd738935f8 from gonzo on behalf of hulk.\" , \"formKey\" : \"approve-request\" , \"priority\" : 23 , \"createTime\" : ISODa te ( \"2019-08-21T09:12:54.872Z\" ) , \"candidateUsers\" : [ \"fozzy\" , \"gonzo\" ], \"candidateGroups\" : [], \"dueDate\" : ISODa te ( \"2019-06-26T07:55:00.000Z\" ) , \"followUpDate\" : ISODa te ( \"2023-06-26T07:55:00.000Z\" ) , \"deleted\" : false } Process Collection # Process definition collection allows for storage of startable process definitions, deployed in a Camunda Engine. This information is in particular interesting, if you are building a process-starter component and want to react dynamically on processes deployed in your landscape. { \"_id\" : \"process_approve_request:1:91f2ff26-a64b-11e9-b117-3e6d125b91e2\" , \"processDefinitionKey\" : \"process_approve_request\" , \"processDefinitionVersion\" : 1 , \"applicationName\" : \"example-process-approval\" , \"processName\" : \"Request Approval\" , \"processDescription\" : \"This is a wonderful process.\" , \"formKey\" : \"start-approval\" , \"startableFromTasklist\" : true , \"candidateStarterUsers\" : [], \"candidateStarterGroups\" : [ \"muppetshow\" , \"avengers\" ] } Tracking Token Collection # The Axon Tracking Token reflects the index of the event processed by the Mongo View and is stored in the following format: { \"_id\" : Objec t Id( \"5d2b45d6a9ca33042abea23b\" ) , \"processorName\" : \"io.holunda.camunda.taskpool.view.mongo.service\" , \"segment\" : 0 , \"owner\" : \"18524@blackstar\" , \"timestamp\" : NumberLo n g( 1566379093564 ) , \"token\" : { \"$binary\" : \"PG9yZy5heG9uZnJhbWV3b3JrLmV2ZW50aGFuZGxpbmcuR2xvYmFsU2VxdWVuY2VUcmFja2luZ1Rva2VuPjxnbG9iYWxJbmRleD40NDwvZ2xvYmFsSW5kZXg+PC9vcmcuYXhvbmZyYW1ld29yay5ldmVudGhhbmRsaW5nLkdsb2JhbFNlcXVlbmNlVHJhY2tpbmdUb2tlbj4=\" , \"$type\" : \"00\" }, \"tokenType\" : \"org.axonframework.eventhandling.GlobalSequenceTrackingToken\" }","title":"Mongo DB View"},{"location":"reference-guide/components/view-mongo.html#purpose","text":"The Mongo View is component responsible for creating read-projections of tasks and business data entries. It implements the Taskpool and Datapool View API and persists the projection as document collections in a Mongo database.","title":"Purpose"},{"location":"reference-guide/components/view-mongo.html#features","text":"stores JSON document representation of enriched tasks, process definitions and business data entries provides single query API provides subscription query API (reactive) switchable subscription query API (AxonServer or MongoDB ChangeStream) Warning Mongo DB View is currently NOT SUPPORTING Revision Aware queries.","title":"Features"},{"location":"reference-guide/components/view-mongo.html#configuration-options","text":"In order to activate the Mongo implementation, please include the following dependency on your classpath: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-view-mongo </artifactId> <version> ${polyflow.version} </version> </dependency> The implementation relies on Spring Data Mongo and needs to activate those. Please add the following annotation to any class marked as Spring Configuration loaded during initialization: @Configuration @EnablePolyflowMongoView @Import ({ org . springframework . boot . autoconfigure . mongo . MongoAutoConfiguration . class , org . springframework . boot . autoconfigure . mongo . MongoReactiveAutoConfiguration . class , org . springframework . boot . autoconfigure . data . mongo . MongoDataAutoConfiguration . class , org . springframework . boot . autoconfigure . data . mongo . MongoReactiveDataAutoConfiguration . class }) public class MyViewConfiguration { } In addition, configure a Mongo connection to database called tasks-payload using application.properties or application.yaml : spring: data: mongodb: database: tasks-payload host: localhost port: 27017 The view implementation provides runtime details using standard logging facility. If you want to increase the logging level, please set up it e.g. in your application.yaml : logging.level.io.holunda.polyflow.view.mongo: DEBUG For further configuration, please check Mongo DB View Configuration","title":"Configuration options"},{"location":"reference-guide/components/view-mongo.html#collections","text":"The Mongo View uses several collections to store the results. These are: data-entries: collection for business data entries processes: collection for process definitions tasks: collection for user tasks tracking-tokens: collection for Axon Tracking Tokens","title":"Collections"},{"location":"reference-guide/components/view-mongo.html#data-entries-collection","text":"The data entries collection stores the business data entries in a uniform Datapool format. Here is an example: { \"_id\" : \"io.holunda.camunda.taskpool.example.ApprovalRequest#2db47ced-83d4-4c74-a644-44dd738935f8\" , \"entryType\" : \"io.holunda.camunda.taskpool.example.ApprovalRequest\" , \"payload\" : { \"amount\" : \"900.00\" , \"subject\" : \"Advanced training\" , \"currency\" : \"EUR\" , \"id\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"applicant\" : \"hulk\" }, \"correlations\" : {}, \"type\" : \"Approval Request\" , \"name\" : \"AR 2db47ced-83d4-4c74-a644-44dd738935f8\" , \"applicationName\" : \"example-process-approval\" , \"description\" : \"Advanced training\" , \"state\" : \"Submitted\" , \"statusType\" : \"IN_PROGRESS\" , \"authorizedUsers\" : [ \"gonzo\" , \"hulk\" ], \"authorizedGroups\" : [], \"protocol\" : [ { \"time\" : ISODa te ( \"2019-08-21T09:12:54.779Z\" ) , \"statusType\" : \"PRELIMINARY\" , \"state\" : \"Draft\" , \"username\" : \"gonzo\" , \"logMessage\" : \"Draft created.\" , \"logDetails\" : \"Request draft on behalf of hulk created.\" }, { \"time\" : ISODa te ( \"2019-08-21T09:12:55.060Z\" ) , \"statusType\" : \"IN_PROGRESS\" , \"state\" : \"Submitted\" , \"username\" : \"gonzo\" , \"logMessage\" : \"New approval request submitted.\" } ] }","title":"Data Entries Collection"},{"location":"reference-guide/components/view-mongo.html#tasks-collections","text":"Tasks are stored in the following format (an example): { \"_id\" : \"dc1abe54-c3f3-11e9-86e8-4ab58cfe8f17\" , \"sourceReference\" : { \"_id\" : \"dc173bca-c3f3-11e9-86e8-4ab58cfe8f17\" , \"executionId\" : \"dc1a9742-c3f3-11e9-86e8-4ab58cfe8f17\" , \"definitionId\" : \"process_approve_request:1:91f2ff26-a64b-11e9-b117-3e6d125b91e2\" , \"definitionKey\" : \"process_approve_request\" , \"name\" : \"Request Approval\" , \"applicationName\" : \"example-process-approval\" , \"_class\" : \"process\" }, \"taskDefinitionKey\" : \"user_approve_request\" , \"payload\" : { \"request\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"originator\" : \"gonzo\" }, \"correlations\" : { \"io:holunda:camunda:taskpool:example:ApprovalRequest\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"io:holunda:camunda:taskpool:example:User\" : \"gonzo\" }, \"dataEntriesRefs\" : [ \"io.holunda.camunda.taskpool.example.ApprovalRequest#2db47ced-83d4-4c74-a644-44dd738935f8\" , \"io.holunda.camunda.taskpool.example.User#gonzo\" ], \"businessKey\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\" , \"name\" : \"Approve Request\" , \"description\" : \"Please approve request 2db47ced-83d4-4c74-a644-44dd738935f8 from gonzo on behalf of hulk.\" , \"formKey\" : \"approve-request\" , \"priority\" : 23 , \"createTime\" : ISODa te ( \"2019-08-21T09:12:54.872Z\" ) , \"candidateUsers\" : [ \"fozzy\" , \"gonzo\" ], \"candidateGroups\" : [], \"dueDate\" : ISODa te ( \"2019-06-26T07:55:00.000Z\" ) , \"followUpDate\" : ISODa te ( \"2023-06-26T07:55:00.000Z\" ) , \"deleted\" : false }","title":"Tasks Collections"},{"location":"reference-guide/components/view-mongo.html#process-collection","text":"Process definition collection allows for storage of startable process definitions, deployed in a Camunda Engine. This information is in particular interesting, if you are building a process-starter component and want to react dynamically on processes deployed in your landscape. { \"_id\" : \"process_approve_request:1:91f2ff26-a64b-11e9-b117-3e6d125b91e2\" , \"processDefinitionKey\" : \"process_approve_request\" , \"processDefinitionVersion\" : 1 , \"applicationName\" : \"example-process-approval\" , \"processName\" : \"Request Approval\" , \"processDescription\" : \"This is a wonderful process.\" , \"formKey\" : \"start-approval\" , \"startableFromTasklist\" : true , \"candidateStarterUsers\" : [], \"candidateStarterGroups\" : [ \"muppetshow\" , \"avengers\" ] }","title":"Process Collection"},{"location":"reference-guide/components/view-mongo.html#tracking-token-collection","text":"The Axon Tracking Token reflects the index of the event processed by the Mongo View and is stored in the following format: { \"_id\" : Objec t Id( \"5d2b45d6a9ca33042abea23b\" ) , \"processorName\" : \"io.holunda.camunda.taskpool.view.mongo.service\" , \"segment\" : 0 , \"owner\" : \"18524@blackstar\" , \"timestamp\" : NumberLo n g( 1566379093564 ) , \"token\" : { \"$binary\" : \"PG9yZy5heG9uZnJhbWV3b3JrLmV2ZW50aGFuZGxpbmcuR2xvYmFsU2VxdWVuY2VUcmFja2luZ1Rva2VuPjxnbG9iYWxJbmRleD40NDwvZ2xvYmFsSW5kZXg+PC9vcmcuYXhvbmZyYW1ld29yay5ldmVudGhhbmRsaW5nLkdsb2JhbFNlcXVlbmNlVHJhY2tpbmdUb2tlbj4=\" , \"$type\" : \"00\" }, \"tokenType\" : \"org.axonframework.eventhandling.GlobalSequenceTrackingToken\" }","title":"Tracking Token Collection"},{"location":"reference-guide/components/view-simple.html","text":"Purpose # The In-Memory View is component responsible for creating read-projections of tasks and business data entries. It implements the Taskpool and Datapool View API and persists the projection in memory. The projection is transient and relies on event replay on every application start. It is good for demonstration purposes if the number of events is manageable small, but will fail to delivery high performance results on a large number of items. Features # uses concurrent hash maps to store the read model provides single query API provides subscription query API (reactive) relies on event replay and transient token store Configuration options # In order to activate the in-memory implementation, please include the following dependency on your classpath: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-view-simple </artifactId> <version> ${polyflow.version} </version> </dependency> Then, add the following annotation to any class marked as Spring Configuration loaded during initialization: @Configuration @EnablePolyflowSimpleView public class MyViewConfiguration { } The view implementation provides runtime details using standard logging facility. If you want to increase the logging level, please setup it e.g. in your application.yaml : logging.level.io.holunda.polyflow.view.simple: DEBUG","title":"In-Memory View"},{"location":"reference-guide/components/view-simple.html#purpose","text":"The In-Memory View is component responsible for creating read-projections of tasks and business data entries. It implements the Taskpool and Datapool View API and persists the projection in memory. The projection is transient and relies on event replay on every application start. It is good for demonstration purposes if the number of events is manageable small, but will fail to delivery high performance results on a large number of items.","title":"Purpose"},{"location":"reference-guide/components/view-simple.html#features","text":"uses concurrent hash maps to store the read model provides single query API provides subscription query API (reactive) relies on event replay and transient token store","title":"Features"},{"location":"reference-guide/components/view-simple.html#configuration-options","text":"In order to activate the in-memory implementation, please include the following dependency on your classpath: <dependency> <groupId> io.holunda.polyflow </groupId> <artifactId> polyflow-view-simple </artifactId> <version> ${polyflow.version} </version> </dependency> Then, add the following annotation to any class marked as Spring Configuration loaded during initialization: @Configuration @EnablePolyflowSimpleView public class MyViewConfiguration { } The view implementation provides runtime details using standard logging facility. If you want to increase the logging level, please setup it e.g. in your application.yaml : logging.level.io.holunda.polyflow.view.simple: DEBUG","title":"Configuration options"},{"location":"reference-guide/configuration/index.html","text":"This is a root of configuration reference guide. Here are some dedicated articles: Persistence configuration Mongo DB View configuration Datapool Aggregate Tuning","title":"Configuration Overview"},{"location":"reference-guide/configuration/core-datapool-aggregate-tuning.html","text":"","title":"Datapool Aggregate Tuning"},{"location":"reference-guide/configuration/persistence.html","text":"Persistence # If you use relational databases for your Event Store of the DataPool or TaskPool or your view, using the JPA View , Axon Framework, used as a component of Polyflow will detect and autoconfigure itself. Especially, if you use Spring Data JPA or Spring JDBC, Axon auto-configuration will try to reuse it. If you are using @EntityScan annotation, you need to add Axon entities to the scan. To do so, please the following code on top of a class marked with @Configuration . @Configuration @EntityScan ( basePackageClasses = [ TokenEntry :: class , DomainEventEntry :: class , SagaEntry :: class ] ) class MyConfiguration","title":"Persistence Configuration"},{"location":"reference-guide/configuration/persistence.html#persistence","text":"If you use relational databases for your Event Store of the DataPool or TaskPool or your view, using the JPA View , Axon Framework, used as a component of Polyflow will detect and autoconfigure itself. Especially, if you use Spring Data JPA or Spring JDBC, Axon auto-configuration will try to reuse it. If you are using @EntityScan annotation, you need to add Axon entities to the scan. To do so, please the following code on top of a class marked with @Configuration . @Configuration @EntityScan ( basePackageClasses = [ TokenEntry :: class , DomainEventEntry :: class , SagaEntry :: class ] ) class MyConfiguration","title":"Persistence"},{"location":"reference-guide/configuration/view-mongo.html","text":"The use of Mongo database or a Cosmos DB in Mongo mode as a persistence for the read projection of task-pool and data-pool may require some additional configuration, depending on your scenario. Note We strongly recommend to use a clustered Mongo DB or Cosmos DB installation for persistence to avoid data loss. Configuration properties # The configuration of View Mongo is performed via application properties of the component, that includes the polyflow-view-mongo . All configuration properties have the prefix polyflow.view.mongo . Here is the example with all properties: polyflow : view : mongo : change-stream : clear-deleted-tasks : after : PT1H buffer-size : 100000 job-schedule : '@daily' job-jitter : PT1H job-timezone : UTC mode : SCHEDULED_JOB change-tracking-mode : CHANGE_STREAM indexes : token-store : false Property (prefixed by polyflow.view.mongo ) Description Value Default Example change-stream.clear-deleted-tasks.after How long should we keep deleted tasks around before clearing them Duration Duration.ZERO PT1H change-stream.clear-deleted-tasks.buffer-size While the change tracker waits for tasks that have been marked deleted to become due for clearing, it needs to buffer them. This property defines the buffer capacity. If more than [bufferSize] tasks are deleted within the time window defined by [after], the buffer will overflow and the latest task(s) will be dropped. These task(s) will not be automatically cleared in CHANGE_STREAM_SUBSCRIPTION [mode]. In BOTH [mode], the scheduled job will pick them up and clear them eventually. Only relevant if [mode] is CHANGE_STREAM_SUBSCRIPTION or BOTH . Long 10000 200 change-stream.clear-deleted-tasks.job-schedule Cron expression to configure how often the job run that clears deleted tasks should run. Only relevant if [mode] is SCHEDULED_JOB or BOTH . Cron expression @daily @hourly change-stream.clear-deleted-tasks.job-jitter The cleanup job execution time will randomly be delayed after what is determined by the cron expression by [0..this duration]. Duration PT5M PT3M change-stream.clear-deleted-tasks.timezone Time zone to use for resolving the cron expression. Default: UTC. Timezone UTC CET change-stream.clear-deleted-tasks.mode How exactly should we clear deleted tasks. see below see below see below change-tracking-mode Mode to use for event tracking. see below see below see below indexes.token-store Controls the index of the token store. Boolean true false Change Tracking Mode # The events delivering updates on view artifacts are received by the projection and cause updates in Mongo DB projections. If you are interested in delivering reactive updates (e.g. Server push, SSE, Websocket), you can control what is the trigger for the update modification. The reason for this is that Mongo DB itself is eventually consistent in replica set and the update operation will eventually be written to all nodes. If you are reading the documents by the component triggered by the reactive update, you might end up in a race condition. Set the mode for event tracking by selecting from one of the following values: Value Description EVENT_HANDLER Use Axon query bus and update subscriptions after the event has been processed. CHANGE_STREAM Use Mongo DB change stream. NONE Disable reactive updates. Clear deleted tasks # Removal of elements from collection is a costly operation in distributed Mongo DB cluster. For this purpose, if the user task gets deleted, it is marked by a deleted flag and immediately excluded from the selection. A later job is responsible for real wiping it out from the collection. The deletion mode controls how this operation is performed. Set the mode for task deletion by selecting from one of the following values: Value Description CHANGE_STREAM_SUBSCRIPTION Subscribe to the change stream and clear any tasks that are marked deleted after the duration configured in after property. SCHEDULED_JOB Run a scheduled job to clear any tasks that are marked as deleted if the deletion timestamp is at least after property in the past. The job is run according to the cron expression defined in job-schedule property. BOTH Use CHANGE_STREAM_SUBSCRIPTION and SCHEDULED_JOB . NONE The application is taking care of clearing deleted tasks, e.g. by implementing its own scheduled job or using a partial TTL index.","title":"Mongo DB View Configuration"},{"location":"reference-guide/configuration/view-mongo.html#configuration-properties","text":"The configuration of View Mongo is performed via application properties of the component, that includes the polyflow-view-mongo . All configuration properties have the prefix polyflow.view.mongo . Here is the example with all properties: polyflow : view : mongo : change-stream : clear-deleted-tasks : after : PT1H buffer-size : 100000 job-schedule : '@daily' job-jitter : PT1H job-timezone : UTC mode : SCHEDULED_JOB change-tracking-mode : CHANGE_STREAM indexes : token-store : false Property (prefixed by polyflow.view.mongo ) Description Value Default Example change-stream.clear-deleted-tasks.after How long should we keep deleted tasks around before clearing them Duration Duration.ZERO PT1H change-stream.clear-deleted-tasks.buffer-size While the change tracker waits for tasks that have been marked deleted to become due for clearing, it needs to buffer them. This property defines the buffer capacity. If more than [bufferSize] tasks are deleted within the time window defined by [after], the buffer will overflow and the latest task(s) will be dropped. These task(s) will not be automatically cleared in CHANGE_STREAM_SUBSCRIPTION [mode]. In BOTH [mode], the scheduled job will pick them up and clear them eventually. Only relevant if [mode] is CHANGE_STREAM_SUBSCRIPTION or BOTH . Long 10000 200 change-stream.clear-deleted-tasks.job-schedule Cron expression to configure how often the job run that clears deleted tasks should run. Only relevant if [mode] is SCHEDULED_JOB or BOTH . Cron expression @daily @hourly change-stream.clear-deleted-tasks.job-jitter The cleanup job execution time will randomly be delayed after what is determined by the cron expression by [0..this duration]. Duration PT5M PT3M change-stream.clear-deleted-tasks.timezone Time zone to use for resolving the cron expression. Default: UTC. Timezone UTC CET change-stream.clear-deleted-tasks.mode How exactly should we clear deleted tasks. see below see below see below change-tracking-mode Mode to use for event tracking. see below see below see below indexes.token-store Controls the index of the token store. Boolean true false","title":"Configuration properties"},{"location":"reference-guide/configuration/view-mongo.html#change-tracking-mode","text":"The events delivering updates on view artifacts are received by the projection and cause updates in Mongo DB projections. If you are interested in delivering reactive updates (e.g. Server push, SSE, Websocket), you can control what is the trigger for the update modification. The reason for this is that Mongo DB itself is eventually consistent in replica set and the update operation will eventually be written to all nodes. If you are reading the documents by the component triggered by the reactive update, you might end up in a race condition. Set the mode for event tracking by selecting from one of the following values: Value Description EVENT_HANDLER Use Axon query bus and update subscriptions after the event has been processed. CHANGE_STREAM Use Mongo DB change stream. NONE Disable reactive updates.","title":"Change Tracking Mode"},{"location":"reference-guide/configuration/view-mongo.html#clear-deleted-tasks","text":"Removal of elements from collection is a costly operation in distributed Mongo DB cluster. For this purpose, if the user task gets deleted, it is marked by a deleted flag and immediately excluded from the selection. A later job is responsible for real wiping it out from the collection. The deletion mode controls how this operation is performed. Set the mode for task deletion by selecting from one of the following values: Value Description CHANGE_STREAM_SUBSCRIPTION Subscribe to the change stream and clear any tasks that are marked deleted after the duration configured in after property. SCHEDULED_JOB Run a scheduled job to clear any tasks that are marked as deleted if the deletion timestamp is at least after property in the past. The job is run according to the cron expression defined in job-schedule property. BOTH Use CHANGE_STREAM_SUBSCRIPTION and SCHEDULED_JOB . NONE The application is taking care of clearing deleted tasks, e.g. by implementing its own scheduled job or using a partial TTL index.","title":"Clear deleted tasks"}]}