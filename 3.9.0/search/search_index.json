{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#why-should-i-use-this","title":"Why should I use this?","text":"<ul> <li>You are building a process application or platform that comprises multiple process engines?</li> <li>You are building a custom task list for your Camunda Platform 7 process application?</li> <li>You want to include custom business object attributes to your tasks in the task list?</li> <li>You have performance issues with your task list?</li> <li>You need to provide a view on the business objects processed by your processes?</li> <li>You want a customized, business-driven audit log for your processes and changes to the business objects?</li> </ul> <p>If you can answer one of the previous questions with yes, Polyflow's libraries might help you.</p>"},{"location":"index.html#how-to-start","title":"How to start?","text":"<p>We provide documentation for different people and different tasks. A good starting point is the Introduction. You might want to look at Reference Guide containing a Working Example and details about Usage Scenarios.</p>"},{"location":"index.html#get-in-touch","title":"Get in touch","text":"<p>If you are missing a feature, have a question regarding usage or deployment, you should definitely get in touch with us. There are various ways to do so:</p> <p> </p>"},{"location":"developer-guide/contribution.html","title":"Contribution","text":"<p>There are several ways in which you may contribute to this project.</p> <ul> <li>File issues</li> <li>Submit a pull requests</li> </ul>"},{"location":"developer-guide/contribution.html#found-a-bug-or-missing-feature","title":"Found a bug or missing feature?","text":"<p>Please file an issue in our issue tracking system.</p>"},{"location":"developer-guide/contribution.html#submit-a-pull-request","title":"Submit a Pull Request","text":"<p>If you found a solution to an open issue and implemented it, we would be happy to add your contribution in the code base. For doing so, please create a pull request. Prior to that, please make sure you:</p> <ul> <li>rebase against the <code>develop</code> branch</li> <li>stick to project coding conventions</li> <li>added test cases for the problem you are solving</li> <li>added docs, describing the change</li> <li>generally comply with codacy report</li> </ul>"},{"location":"developer-guide/project-setup.html","title":"Project Setup","text":"<p>If you are interested in developing and building the project please follow the following instruction.</p>"},{"location":"developer-guide/project-setup.html#version-control","title":"Version control","text":"<p>To get sources of the project, please execute:</p> <pre><code>git clone https://github.com/holunda-io/camunda-bpm-taskpool.git\ncd camunda-bpm-taskpool\n</code></pre> <p>We are using gitflow in our git SCM. That means that you should start from <code>develop</code> branch, create a <code>feature/&lt;name&gt;</code> out of it and once it is completed create a pull request containing it. Please squash your commits before submitting and use semantic commit messages, if possible.</p>"},{"location":"developer-guide/project-setup.html#project-build","title":"Project Build","text":"<p>Perform the following steps to get a development setup up and running.</p> <pre><code>./mvnw clean install\n</code></pre>"},{"location":"developer-guide/project-setup.html#integration-tests","title":"Integration Tests","text":"<p>By default, the build command will ignore the run of <code>failsafe</code> Maven plugin executing the integration tests (usual JUnit tests with class names ending with ITest). In order to run integration tests, please call from your command line:</p> <pre><code>./mvnw integration-test failsafe:verify -Pitest\n</code></pre>"},{"location":"developer-guide/project-setup.html#project-build-modes-and-profiles","title":"Project build modes and profiles","text":""},{"location":"developer-guide/project-setup.html#camunda-version","title":"Camunda Version","text":"<p>You can choose the used Camunda version by specifying the profile <code>camunda-ee</code> or <code>camunda-ce</code>. The default version is a Community Edition. Specify <code>-Pcamunda-ee</code> to switch to Camunda Enterprise edition. This will require a valid Camunda license. You can put it into a file <code>~/.camunda/license.txt</code> and it will be detected automatically.</p>"},{"location":"developer-guide/project-setup.html#generate-sql-ddl","title":"Generate SQL DDL","text":"<p>If you are using RDBMS (for example for the Polyflow JPA View or/and JPA storage of Axon Entities), you will require the SQL DDL. Consider to edit the <code>view/jpa/src/sql/persistence.xml</code> descriptor to control what to include into DDL generation. You can generate this by executing the following build command:</p> <pre><code>./mvnw -Pgenerate-sql -f view/jpa\n</code></pre>"},{"location":"developer-guide/project-setup.html#build-documentation","title":"Build Documentation","text":"<p>We are using MkDocs for generation of a static site documentation and rely on Markdown as much as possible. MkDocs is a written in Python 3 and needs to be installed on your machine. For the installation please run the following command from your command line:</p> <pre><code>python3 -m pip install --upgrade pip\npython3 -m pip install -r ./docs/requirements.txt\n</code></pre> <p>For creation of documentation, please run:</p> <pre><code>mkdocs build\n</code></pre> <p>The docs are generated into <code>site</code> directory.</p> <p>Note</p> <p>If you want to develop your docs in 'live' mode, run <code>mkdocs serve</code> and access the http://localhost:8000/ from your browser.</p>"},{"location":"developer-guide/project-setup.html#continuous-integration","title":"Continuous Integration","text":"<p>Travis CI is building all branches on commit hook. In addition, a private-hosted Jenkins CI is used to build the releases.</p>"},{"location":"developer-guide/project-setup.html#release-management","title":"Release Management","text":"<p>Release management has been set up for use of Sonatype Nexus (= Maven Central)</p>"},{"location":"developer-guide/project-setup.html#what-modules-get-deployed-to-repository","title":"What modules get deployed to repository","text":"<p>Every module is enabled by default. If you want to change this, please provide the property</p> <pre><code>&lt;maven.deploy.skip&gt;true&lt;/maven.deploy.skip&gt;\n</code></pre> <p>inside the corresponding <code>pom.xml</code>. Currently, all examples are EXCLUDED from publication into Maven Central.</p>"},{"location":"developer-guide/project-setup.html#trigger-new-release","title":"Trigger new release","text":"<p>Warning</p> <p>This operation requires special permissions.</p> <p>We use gitflow for development (see A successful git branching model for more details). You could use gitflow with native git commands, but then you would have to change the versions in the poms manually. Therefore, we use the mvn gitflow plugin, which handles this and other things nicely.</p> <p>You can build a release with:</p> <pre><code>./mvnw gitflow:release-start\n./mvnw gitflow:release-finish\n</code></pre> <p>This will update the versions in the <code>pom.xml</code> s accordingly and push the release tag to the <code>master</code> branch and update the <code>develop</code> branch for the new development version.</p>"},{"location":"developer-guide/project-setup.html#trigger-a-deploy","title":"Trigger a deploy","text":"<p>Warning</p> <p>This operation requires special permissions.</p> <p>Currently, CI allows for deployment of artifacts to Maven Central and is executed using github actions. This means, that a push to <code>master</code> branch will start the corresponding build job, and if successful the artifacts will get into <code>Staging Repositories</code> of OSS Sonatype without manual intervention.</p>"},{"location":"developer-guide/project-setup.html#run-deploy-from-local-machine","title":"Run deploy from local machine","text":"<p>Warning</p> <p>This operation requires special permissions.</p> <p>If you still want to execute the deployment from your local machine, you need to have GPG keys at place and to execute the following command on the <code>master</code> branch:</p> <pre><code>export GPG_KEYNAME=\"&lt;keyname&gt;\"\nexport GPG_PASSPHRASE=\"&lt;secret&gt;\"\n./mvnw clean deploy -B -DskipTests -DskipExamples -Prelease -Dgpg.keyname=$GPG_KEYNAME -Dgpg.passphrase=$GPG_PASSPHRASE\n</code></pre>"},{"location":"developer-guide/project-setup.html#release-to-public-repositories","title":"Release to public repositories","text":"<p>Warning</p> <p>This operation requires special permissions.</p> <p>The deployment job will publish the artifacts to Nexus OSS staging repositories. Currently, all snapshots get into OSS Sonatype Snapshot repository and all releases to Maven Central automatically.</p>"},{"location":"examples/index.html","title":"Examples Overview","text":"<p>We provide the following examples to demonstrate the features of Polyflow:</p> <ul> <li>Example Approval</li> </ul>"},{"location":"examples/example-approval.html","title":"Example Application","text":"<p>Along with library modules several example modules and applications are provided, demonstrating the main features of the solution. This includes a series of example applications for usage in different Usage Scenarios. They all share the same business process described in the next section.</p>"},{"location":"examples/example-approval.html#business-context-approval","title":"Business context: Approval","text":"<p>Take a look on the process model above. Imagine you are building a system that responsible for management of all approval requests in the company. Using this system, you can submit requests which then get eventually approved or rejected. Sometimes, the approver doesn't approve or reject, but returns the request back to the originator for correction (that is the person, who submitted the request). Then, the originator can amend the request and resubmit it or cancel the request.</p> <p>An approval request is modelled in the following way. The subject  describes what the request is about, the applicant is the person whom it is about (can be different from originator). Finally, the amount and currency denote the costs of the request. All requests must be stored for compliance purposes.</p> <p>The request is initially created in <code>DRAFT</code> mode. It gets to state <code>IN PROGRESS</code> as soon as the process is started and will eventually get to <code>ACCEPTED</code> or <code>REJECTED</code> as a final state.</p> <p>For sample purposes two groups of users are created: The Muppet Show (<code>Kermit</code>, <code>Piggy</code>, <code>Gonzo</code> and <code>Fozzy</code>) and The Avengers (<code>Ironman</code>, <code>Hulk</code>).  <code>Gonzo</code> and <code>Fozzy</code> are responsible for approvals.</p>"},{"location":"examples/example-approval.html#process-run","title":"Process Run","text":"<p>Let's play the following run through this process model.</p> <ul> <li><code>Ironman</code> submits an Advanced Training request on behalf of <code>Hulk</code></li> <li>The request costs are provided in wrong currency and <code>Gonzo</code> returns the request to <code>Ironman</code> for correction (EUR instead of USD)</li> <li><code>Ironman</code> changes the currency to USD and re-submits the request</li> <li><code>Gonzo</code> is out of office, so <code>Fozzy</code> takes over and approves the request</li> </ul>"},{"location":"examples/example-approval.html#running-examples","title":"Running Examples","text":"<p>To run the example please consult the Usage Scenarios section.</p> <p>Note</p> <p>Since the process application includes Camunda BPM engine, you can use the standard Camunda webapps by navigating to http://localhost:8080/camunda/app/. The default user and password are <code>admin / admin</code>.</p>"},{"location":"examples/example-approval.html#story-board","title":"Story board","text":"<p>The following storyboard can be used to understand the mechanics behind the provided implementation:</p> <p>TIP: In this storyboard, we assume you started the single node scenario and the application runs locally on http://localhost:8080. Please adjust the URLs accordingly, if you started differently.</p> <ul> <li> <p>To start the approval process for a given request open your browser and navigate to the <code>Example Tasklist</code>: http://localhost:8080/polyflow/. Please note that the selected user is <code>Ironman</code>.</p> </li> <li> <p>Open the menu (<code>Start new...</code>) in and select 'Request Approval'. You should see the start form for the example approval process.</p> </li> </ul> <p></p> <ul> <li> <p>Select <code>Advanced Training</code> from one of predefined templates and click Start. The start form will disappear and redirect back to the empty <code>Tasklist</code>.</p> </li> <li> <p>Since you are still acting as <code>Ironman</code> there are nothing you can do here. Please switch the user to <code>Gonzo</code> in the top right corner and you should see the user task <code>Approve Request</code> from process <code>Request Approval</code>.</p> </li> </ul> <p></p> <ul> <li>Examine the task details by clicking Data tab in Details column. You can see the data of the request correlated to the current process instance.</li> </ul> <p></p> <ul> <li>Click on the task name and you will see the user task form of the <code>Approve Request</code> task. Select the option <code>Return request to originator</code> and click complete.</li> </ul> <p></p> <ul> <li>Switch to <code>Workpieces</code> and you should see the request business object. Examine the approval request by clicking Data, Audit and Description tabs in Details column.</li> </ul> <p></p> <ul> <li>Change user back to <code>Ironman</code> and switch back to the <code>Tasklist</code> and open the <code>Amend request</code>task. Change the currency to <code>USD</code> and re-submit the request.</li> </ul> <p></p> <ul> <li> <p>Change user back to <code>Fozzy</code>, open the <code>Approve Request</code> task and approve the request by selecting the appropriate option.</p> </li> <li> <p>Switch to <code>Workpieces</code> and you should still see the request business object, even after the process is finished. Examine the approval request by clicking Data, Audit and Description tabs in Details column.</p> </li> </ul>"},{"location":"examples/example-components/index.html","title":"Example Components","text":"<p>For demonstration purposes we built several example components and reuse them to demonstrate the Example application in different Usage Scenarios. These components are not part of a polyflow distribution and serve demonstration purposes only. They still show implementation of components which needs to be implemented by you and are not part of the provided library. If you are interested in the source code, please visit the Polyflow Repository.</p>"},{"location":"examples/example-components/index.html#process-application-example-components","title":"Process Application Example Components","text":"<p>To show the integration of Polyflow components into a Process Application, the process discussed in Example Approval application has been implemented.</p> <ul> <li>Process Application Frontend</li> <li>Process Application Backend</li> </ul>"},{"location":"examples/example-components/index.html#process-platform-example-components","title":"Process Platform Example Components","text":"<p>To show the integration of Polyflow components into a Process Platform, a simple task list and a workpieces view (archive view for business objects) has been implemented.</p> <ul> <li>Process Platform Frontend</li> <li>Process Platform Backend</li> </ul>"},{"location":"examples/example-components/index.html#shared-example-components","title":"Shared Example Components","text":"<p>Components used by other example components.</p> <ul> <li>User Management</li> </ul>"},{"location":"examples/example-components/pa-backend.html","title":"Process Application Backend","text":"<p>The process application backend is implementing the process described in the Example application. It demonstrates a typical three-tier application, following the Boundary-Control-Entity pattern.</p>"},{"location":"examples/example-components/pa-backend.html#boundary-tier","title":"Boundary Tier","text":"<p>The REST API is defined using OpenAPI specification and is implemented by Spring MVC controllers. It defines of four logical parts:</p> <ul> <li>Environment Controller</li> <li>Request Controller</li> <li>Approve Task Controller</li> <li>Amend Task Controller</li> </ul>"},{"location":"examples/example-components/pa-backend.html#control-tier","title":"Control Tier","text":"<p>The control tier is implemented using stateless Spring Beans and orchestrated by the Camunda BPM Process. Typical <code>JavaDelegate</code> and <code>ExecutionListener</code> are used as a glue layer. Business services of this layer are responsible for the integration with <code>Datapool Components</code> to reflect the status of the <code>Request</code> business entity. The Camunda BPM Engine is configured to use the <code>TaskCollector</code> to integrate with remaining components of the <code>camunda-bpm-taskpool</code>.</p>"},{"location":"examples/example-components/pa-backend.html#entity-tier","title":"Entity Tier","text":"<p>The entity tier is implemented using Spring Data JPA, using Hibernate entities. Application data and process engine data is stored using a RDBMS.</p>"},{"location":"examples/example-components/pa-frontend.html","title":"Process Application Frontend","text":"<p>The process application frontend is implementing the user task forms and business object views for the example application. It is built as a typical Angular Single Page Application (SPA) and provides views for both user tasks and the business objects. It communicates with process application backend via REST API, defined in the latter.</p> <p>The user primarily interacts with the process platform which seamlessly integrates with the process applications. Usually, it provides integration points for user-task embedding / UI-composition. Unfortunately, this topic strongly depends on the frontend technology and is not a subject we want to demonstrate in this example. For simplicity, we built a very simple example, skipping the UI composition / integration entirely.</p> <p>The navigation between the process platform and process application is just as simple as a full page navigation of a hyperlink.</p>"},{"location":"examples/example-components/pp-backend.html","title":"Process Platform Backend","text":"<p>The purpose of the example process platform backend is to demonstrate how process agnostic parts of the process solution can be implemented.</p>"},{"location":"examples/example-components/pp-frontend.html","title":"Process Platform Frontend","text":"<p>The example process platform frontend provides example implementation of two views:</p> <ul> <li>Example Tasklist</li> <li>Example Workpieces List</li> </ul>"},{"location":"examples/example-components/pp-frontend.html#example-tasklist","title":"Example Tasklist","text":"<p>Example Tasklist is a simple implementation of task inbox for a single user. It provides the following features:</p> <ul> <li>Lists tasks in the system for selected user</li> <li>Allows for switching users (faking different user login for demonstration purposes)</li> <li>Tasks include information about the process, name, description, create time, due date, priority and assignment.</li> <li>Tasks include process data (from process instance)</li> <li>Tasks include correlated business data (correlated via variable from process instance)</li> <li>The list of tasks is sortable</li> <li>The list of tasks is pageable (7 items per page)</li> <li>Allows claiming / unclaiming</li> <li>Provides a deeplink to the user task form provided by the process application</li> <li>Allows starting new process instances</li> </ul> <p>Here is, how it looks like showing task descriptions:</p> <p></p> <p>you can optionally show the business data correlated with user task:</p> <p></p>"},{"location":"examples/example-components/pp-frontend.html#example-workpieces-list","title":"Example Workpieces List","text":"<p>The example workpieces list is provides a list of business objects / workpieces that are currently processed by the processes even after the process has already been finished. It provides the following features:</p> <ul> <li>Lists business objects in the system for selected user</li> <li>Allows for switching users (faking different user login for demonstration purposes)</li> <li>Business objects include information about the type, status (with sub status), name, details</li> <li>Business objects include details about contained data</li> <li>Business objects include audit log with all state changes</li> <li>The list is pageable (7 items per page)</li> <li>Business object view</li> <li>Provides a deeplink to the business object view provided by the process application</li> </ul> <p>Here is, how it looks like showing the audit log:</p> <p></p>"},{"location":"examples/example-components/user-management.html","title":"User Management","text":"<p>Usually, a central user management like a Single-Sign-On (SSO) is a part deployed into the process application landscape. This is responsible for authentication and authorization of the user and is required to control the role-based access to user tasks.</p> <p>In our example application, we disable any security checks to avoid the unneeded complexity. In doing so, we implemented a trivial user store holding some pre-defined users used in example components and allow to simply switch users from the frontend by choosing a different user from the drop-down list.</p> <p>It is integrated with the example frontends by passing around the user id along with the requests and provide a way to resolve the user by that id. Technically speaking, the user id can be used to retrieve the permissions of the user and check them in the backend.</p>"},{"location":"examples/scenarios/index.html","title":"Usage Scenarios","text":"<p>Depending on your requirements and infrastructure available several deployment scenarios of the components is possible.</p> <p>The simplest setup is to run all components on a single node. A more advanced scenario is to distribute components and connect them.</p> <p>In doing so, one of the challenging issues for distribution and connecting microservices is a setup of messaging technology supporting required message exchange patterns (MEPs) for a CQRS system. Because of different semantics of commands, events and queries and additional requirements of event-sourced persistence a special implementation of command bus, event bus and event store is required. In particular, two scenarios can be distinguished: using Axon Server or using a different distribution technology.</p> <p>The provided Example application is implemented several times demonstrating the following usage scenarios:</p> <ul> <li>Single Node Scenario</li> <li>Distributed Scenario using Axon Server</li> <li>Distributed Scenario using Axon Server with Local Polyflow Core</li> </ul> <p>It is a good idea to understand the single node scenario first and then move on to more elaborated scenarios.</p>"},{"location":"examples/scenarios/distributed-axon-server-local.html","title":"Distributed Scenario using Axon Server using Local","text":"<p>This example is demonstrating the usage of the Polyflow components distributed with help of Axon Server. It provides two applications for demonstration purposes: the process application and the process platform.  Both applications are built as SpringBoot applications.</p> <p>The following configuration is used in the distributed scenario with Axon Server:</p> <ul> <li>Bus distribution is provided by Axon Server Connector (event bus only)</li> <li>Polyflow Core Components (task pool and data pool) are deployed aside the process application</li> <li>Axon Server is used as Event Store</li> <li>Postgresql is used as a database for:<ul> <li>Camunda BPM Engine</li> <li>Process Application Datasource</li> </ul> </li> <li>JPA is used as persistence for projection view (<code>view-jpa</code>)</li> </ul>"},{"location":"examples/scenarios/distributed-axon-server-local.html#system-requirements","title":"System Requirements","text":"<ul> <li>JDK 11</li> <li>Docker</li> <li>Docker Compose</li> </ul>"},{"location":"examples/scenarios/distributed-axon-server-local.html#preparations","title":"Preparations","text":"<p>Before you begin, please build the entire project with <code>mvn clean install</code> from the command line in the project root directory.</p> <p>You will need some backing services (Axon Server, PostgreSQL) and you can easily start them locally by using the provided <code>docker-compose.yml</code> file.</p> <p>Before you start change the directory to <code>examples/scenarios/distributed-axon-server-local-polyflow</code> and run a preparation script <code>.docker/setup.sh</code>. You can do it with the following code from your command line (you need to do it once):</p> <pre><code>cd examples/scenarios/distributed-axon-server-local-polyflow\n.docker/setup.sh\n</code></pre> <p>Now, start required containers. The easiest way to do so is to run:</p> <pre><code>docker-compose up -d\n</code></pre> <p>To verify it is running, open your browser http://localhost:8024/. You should see the Axon Server administration console.</p>"},{"location":"examples/scenarios/distributed-axon-server-local.html#start","title":"Start","text":"<p>The demo application consists of several Maven modules. In order to start the example, you will need to start only two of them in the following order:</p> <ol> <li>process-platform-view-only (process platform)</li> <li>process-application-local-polyflow (example process application)</li> </ol> <p>The modules can be started by running from command line in the <code>examples/scenarios/distributed-axon-server-local-polyflow</code> directory using Maven or start the packaged application using:</p> <pre><code>java -jar process-platform-view-only/target/*.jar\njava -jar process-application-local-polyflow/target/*.jar\n</code></pre>"},{"location":"examples/scenarios/distributed-axon-server-local.html#useful-urls","title":"Useful URLs","text":""},{"location":"examples/scenarios/distributed-axon-server-local.html#process-platform","title":"Process Platform","text":"<ul> <li>http://localhost:8081/polyflow/tasks</li> <li>http://localhost:8081/polyflow/archive</li> <li>http://localhost:8081/swagger-ui/</li> </ul>"},{"location":"examples/scenarios/distributed-axon-server-local.html#process-application","title":"Process Application","text":"<ul> <li>http://localhost:8080/camunda/app/</li> <li>http://localhost:8080/swagger-ui/</li> </ul>"},{"location":"examples/scenarios/distributed-axon-server.html","title":"Distributed Scenario using Axon Server","text":"<p>This example is demonstrating the usage of the Polyflow components distributed with help of Axon Server. It provides two applications for demonstration purposes: the process application and the process platform.  Both applications are built as SpringBoot applications.</p> <p>The following configuration is used in the distributed scenario with Axon Server:</p> <ul> <li>Bus distribution is provided by Axon Server Connector (command bus, event bus, query bus)</li> <li>Axon Server is used as Event Store</li> <li>Postgresql is used as a database for:<ul> <li>Camunda BPM Engine</li> <li>Process Application Datasource</li> </ul> </li> <li>Mongo is used as persistence for projection view (<code>mongo-view</code>)</li> </ul>"},{"location":"examples/scenarios/distributed-axon-server.html#system-requirements","title":"System Requirements","text":"<ul> <li>JDK 11</li> <li>Docker</li> <li>Docker Compose</li> </ul>"},{"location":"examples/scenarios/distributed-axon-server.html#preparations","title":"Preparations","text":"<p>Before you begin, please build the entire project with <code>mvn clean install</code> from the command line in the project root directory.</p> <p>You will need some backing services (Axon Server, PostgreSQL, MongoDB) and you can easily start them locally by using the provided <code>docker-compose.yml</code> file.</p> <p>Before you start change the directory to <code>examples/scenarios/distributed-axon-server</code> and run a preparation script <code>.docker/setup.sh</code>. You can do it with the following code from your command line (you need to do it once):</p> <pre><code>cd examples/scenarios/distributed-axon-server\n.docker/setup.sh\n</code></pre> <p>Now, start required containers. The easiest way to do so is to run:</p> <pre><code>docker-compose up -d\n</code></pre> <p>To verify it is running, open your browser http://localhost:8024/. You should see the Axon Server administration console.</p>"},{"location":"examples/scenarios/distributed-axon-server.html#start","title":"Start","text":"<p>The demo application consists of several Maven modules. In order to start the example, you will need to start only two of them in the following order:</p> <ol> <li>taskpool-application (process platform)</li> <li>process-application (example process application)</li> </ol> <p>The modules can be started by running from command line in the <code>examples/scenarios/distributed-axon-server</code> directory using Maven or start the packaged application using:</p> <pre><code>java -jar taskpool-application/target/*.jar\njava -jar process-application/target/*.jar\n</code></pre>"},{"location":"examples/scenarios/distributed-axon-server.html#useful-urls","title":"Useful URLs","text":""},{"location":"examples/scenarios/distributed-axon-server.html#process-platform","title":"Process Platform","text":"<ul> <li>http://localhost:8081/polyflow/tasks</li> <li>http://localhost:8081/polyflow/archive</li> <li>http://localhost:8081/swagger-ui/</li> </ul>"},{"location":"examples/scenarios/distributed-axon-server.html#process-application","title":"Process Application","text":"<ul> <li>http://localhost:8080/camunda/app/</li> <li>http://localhost:8080/swagger-ui/</li> </ul>"},{"location":"examples/scenarios/distributed-with-kafka.html","title":"Distributed using Kafka","text":"<p>This document describes additional details to the distribution scenario without Axon Server. Especially, it is used by one of the adopters of Polyflow  (runs in production by a customer) using Apache Kafka (technically Azure Event Hubs) as an event distribution technology. </p> <p>The following diagram depicts the task run from Process Application to the end user, consuming it via Tasklist API connected via Kafka and using Mongo DB for persistence of the query model.</p> <p></p> <ul> <li>The <code>Camunda BPM Taskpool Collector</code> component listens to Camunda events, collects all relevant events that happen in a single transaction and registers a   transaction synchronization to process them <code>beforeCommit</code>. Just before the transaction is committed, the collected events are accumulated and sent as Axon   Commands through the <code>CommandGateway</code>.</li> <li>The <code>Taskpool Core</code> processes those commands and issues Axon Events which are stored in Axon's database tables within the same transaction.</li> <li>The transaction commit finishes. If anything goes wrong before this point, the transaction rolls back, and it is as though nothing ever happened.</li> <li>In the <code>Axon Kafka Extension</code>, a <code>TrackingEventProcessor</code> polls for events and sees them as soon as the transaction that created them is committed. It sends   each event to Kafka and waits for an acknowledgment from Kafka. If sending fails or times out, the event processor goes into error mode and retries until it   succeeds. This can lead to events being published to Kafka more than once but guarantees at-least-once delivery.</li> <li>Within the Tasklist API, the <code>Axon Kafka Extension</code> polls the events from Kafka and another TrackingEventProcessor forwards them to the <code>TaskPoolMongoService</code>   where they are processed to update the Mongo DB accordingly.</li> <li>When a user queries the Tasklist API for tasks, two things happen: Firstly, the Mongo DB is queried for the current state of tasks for this user and these   tasks are returned. Secondly, the Tasklist API subscribes to any changes to the Mongo DB. These changes are filtered for relevance to the user and relevant   changes are returned after the current state as an infinite stream until the request is cancelled or interrupted for some reason.</li> </ul> <p></p>"},{"location":"examples/scenarios/distributed-with-kafka.html#from-process-application-to-kafka","title":"From Process Application to Kafka","text":""},{"location":"examples/scenarios/distributed-with-kafka.html#from-kafka-to-tasklist-api","title":"From Kafka to Tasklist API","text":""},{"location":"examples/scenarios/single-node.html","title":"Scenario for running on a single node","text":"<p>This example demonstrates the usage of the Camunda BPM Taskpool deployed in one single node and is built as a SpringBoot application  described in the Deployment section.</p>"},{"location":"examples/scenarios/single-node.html#system-requirements","title":"System Requirements","text":"<ul> <li>JDK 11</li> </ul>"},{"location":"examples/scenarios/single-node.html#preparations","title":"Preparations","text":"<p>Before you begin, please build the entire project with <code>./mvnw clean install</code> from the command line in the project root directory.</p>"},{"location":"examples/scenarios/single-node.html#start","title":"Start","text":"<p>The demo application consists of one Maven module which can be started by running from command line in the <code>examples/scenarios/single-node</code> directory using Maven. Alternatively you can start the packaged application using:</p> <pre><code>java -jar target/*.jar\n</code></pre>"},{"location":"examples/scenarios/single-node.html#useful-urls","title":"Useful URLs","text":"<ul> <li>http://localhost:8080/taskpool/</li> <li>http://localhost:8080/swagger-ui/</li> <li>http://localhost:8080/camunda/app/tasklist/default/</li> </ul>"},{"location":"getting-started/index.html","title":"Integration Guide","text":"<p>This guide is describing steps required to configure an existing Camunda BPM Spring Boot Process Application and connect to existing Process Platform.</p> <p>Note</p> <p>The following steps assume that you have already chosen one of the distribution scenarios and set-up the Core components. This is a pre-requirement for the following steps to work.</p>"},{"location":"getting-started/index.html#add-dependency-to-polyflow-integration-starter","title":"Add dependency to Polyflow integration starter","text":"<p>Apart from the example application, you might be interested in integrating Polyflow Taskpool and Datapool into your existing application. To do so, you need to enable your Camunda BPM process engine to use the library. For doing so, add the <code>polyflow-integration-camunda-bpm-springboot-starter</code> library. In Maven, add the following dependency to your <code>pom.xml</code>:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.holunda.polyflow&lt;/groupId&gt;\n&lt;artifactId&gt;polyflow-camunda-bpm-springboot-starter&lt;/artifactId&gt;\n&lt;version&gt;${polyflow.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"getting-started/index.html#activate-polyflow-support","title":"Activate Polyflow Support","text":"<p>Now, find your SpringBoot application class and add an annotation to it:</p> <pre><code>@SpringBootApplication\n@EnableTaskpoolEngineSupport\npublic class MyApplication {\n\npublic static void main(String... args) {\nSpringApplication.run(MyApplication.class, args);\n}\n}\n</code></pre>"},{"location":"getting-started/index.html#configure-your-polyflow-provisioning","title":"Configure your Polyflow provisioning","text":"<p>Finally, add the following block to your <code>application.yml</code>:</p> <pre><code>camunda:\nbpm:\ndefault-serialization-format: application/json\nhistory-level: full\neventing:\ntask: false\n\npolyflow:\nintegration:\nclient:\ncamunda:\napplication-name: ${spring.application.name}  # default\ncollector:\ncamunda:\napplication-name: ${spring.application.name}  # default\nprocess-instance:\nenabled: true\nprocess-definition:\nenabled: true\nprocess-variable:\nenabled: true\ntask:\nenabled: true\nenricher:\ntype: processVariables\nsender:\nenabled: true\ndata-entry:\nenabled: true\ntype: simple\napplication-name: ${spring.application.name}  # default\nprocess-definition:\nenabled: true\nprocess-instance:\nenabled: true\nprocess-variable:\nenabled: true\ntask:\nenabled: true\ntype: tx\nsend-within-transaction: true # Must be set to true in single node scenario.\nform-url-resolver:\ndefaultTaskTemplate:  \"/tasks/${formKey}/${id}?userId=%userId%\"\ndefaultApplicationTemplate: \"http://localhost:${server.port}/${applicationName}\"\ndefaultProcessTemplate: \"/${formKey}?userId=%userId%\"\n</code></pre> <p>Now, start your process engine. If you run into a user task, you should see on the console how this is passed to task pool.</p> <p>For more details on the configuration of different options, please consult the Polyflow Components sections.</p>"},{"location":"introduction/index.html","title":"Motivation and Goals","text":"<p>Over the last years, we built various process applications and whole process platforms for our customers using a modern process engine - Camunda Platform 7. In doing so, we observed common requirements, in particular with respect to task-oriented frontend applications and were able to extract them. These were basic requirements independent of the used frontend technology, and it turned out that some issues occurred every time during the implementation.</p> <p>These were:</p> <ul> <li>Coping with performance issues of Camunda Platform 7 engine when it comes to big amounts of tasks to be shown</li> <li>Creating high-performance custom queries for preloading process variables for tasks</li> <li>Creating high-performance custom queries to preload business data associated with the running process instances</li> <li>High-performance re-ordering (sorting) of user tasks</li> <li>High-performance retrieval of tasks from multiple process engines to display in a single task list</li> <li>Repetitive queries with the same result</li> <li>Creating a custom view on the business data items handled during the process execution</li> <li>Creating a custom audit log for the changes performed on the business data items</li> </ul> <p>In our projects we developed solutions to those requirements and gathered experience in applying different approaches for that. Some issues listed above result from the fact that data on a single user task is being read much more often than written, depending on the user count. For systems with a big amount of users this becomes a serious performance issue and needs to be addressed.</p> <p>A possible solution to most of those issues is to create a special component which has a read-optimized representation of user tasks. Such a component acts as a cache for tasks and allows for serving a high amount of queries without any performance impact to the process engine itself at the costs of loosing strong consistency (and working with eventual-consistent task list). Another component might provide additional business data related to the process tasks.</p> <p>We successfully applied this approach at multiple customers but identified the high initial invest as a main drawback of the solution. The goal of this project is to provide such components as free and open source libraries, to be used as a foundation for creation of process platforms for Camunda Platform 7 and other engines. They can also be used as an integration layer for custom process applications, custom user task lists and other components of process automation solutions.</p>"},{"location":"introduction/concepts.html","title":"Concepts","text":"<p>There are many scenarios in which the usage of a process engine as a component inside the orchestration layer makes a lot of sense. Depending on the scenario the resulting architecture of your application may vary. The following section explains the core concepts and building blocks of the architecture we want to support and address by the Polyflow libraries.</p>"},{"location":"introduction/concepts.html#the-10000-feet-view","title":"The 10,000 feet view","text":"<p>The two main building blocks of the solution are Process Application and Process Platform. Sometimes you unite them inside the same deployment unit, but we differentiate them to make their responsibilities more clear.</p> <p>A Process Application implements the main business logic of the solution. It integrates the process engine that is responsible for execution the processes  and orchestrates the business functions. During this execution user tasks are created and performed by the user and the business data objects are modified.  For this purpose, the process application provides user interfaces for user tasks and business data operations.</p> <p>A Process Platform serves as an integration point of one or multiple process applications. It might integrate with a company's Single Sign-On (SSO) solution and Identity &amp; Authorization Management, be part of Intranet portal solution. It provides process agnostic user task list and business object list.</p>"},{"location":"introduction/concepts.html#task-oriented-applications","title":"Task-oriented applications","text":"<p>The core concept of a task-oriented solution is to model the underlying business process and to split the user interaction into parts represented by the user tasks. Every user task is an abstraction of an operation needed to be performed by the user of the system. Usually, they include some sort of call-to-action  including the input fields to be able to input user's decision. Examples of user tasks are Confirm Order, Verify Quotation, Validate Document.</p> <p>User experience plays a significant role in acceptance of the overall solution. In order to access the user task a special UI, called user task form is used. Every user task form is presenting only that limited part of the overall information to the user which is required to complete the user task. This limitation is important in order to avoid distraction and foster focus on the user task.</p> <p>Since there might be multiple process instances running concurrently, a user might see multiple user tasks in the same time. A special view listing all user tasks available for a user is called task list. The application of different user task assignment strategies may be useful to get optimal processing.</p> <p>Along with user tasks forms, representing the actual work the user has to complete, a data-oriented view on business processes is a common requirement. It concentrates on the data being processed and display the business data entities involved in the business processes (sometimes called Workpieces). Depending on your application, business data entities might be created before the running through business processes and usually the lifecycle of them spans over the business process execution. Examples of business data entities are Order, Shipment or Document. In order to display the state of an individual business data entity a special Business Data Form is designed.</p> <p>And since there are multiple of them in the overall application, a special view to search and list them, a so-called Business Entry List or Workpieces List is developed. Sometimes you are interested in business data entries in a particular processing status and develop a special view for them, for example: Current Workpiece List or Archive List.</p>"},{"location":"introduction/deployment.html","title":"Deployment Scenarios","text":"<p>Several deployment scenarios of the components are possible depending on your requirements and available infrastructure.</p> <p>The simplest setup is to run all components on a single node. A more advanced scenario is to distribute components over the network and connect them.</p> <p>In doing so, one of the challenging issues for distribution and connecting microservices is a setup of messaging technology supporting required message exchange patterns (MEPs) for a CQRS system. Because of different semantics of commands, events and queries and additional requirements of event-sourced persistence a special implementation of command bus, event bus and event store is required. In particular, two scenarios can be distinguished: using Axon Server or using a different distribution technology.</p>"},{"location":"introduction/deployment.html#single-node-deployment","title":"Single node deployment","text":"<p>The easiest scenario is the Single Node Deployment. It provides all functional features of the Polyflow library, but is not addressing any of performance, scalability, autonomy and reliability requirements. It works almost without additional infrastructure and is ideal to start with.</p> <p>In a single node scenario the following configuration is used:</p> <ul> <li>All buses are local (command bus, event bus, query bus)</li> <li>Camunda BPM Integration components, Core components and View components are all deployed in the same node</li> <li>JPA-Based event storage is used, persisting the domain events in a RDBMS, along with Camunda-specific DB tables.</li> <li>Simple (In-memory) View or JPA view is used to provide query projections of <code>taskpool</code> and <code>datapool</code></li> </ul> <p>Check the following diagram for more details:</p> <p></p>"},{"location":"introduction/deployment.html#multiple-node-deployment","title":"Multiple node deployment","text":"<p>The more advanced scenario is to separate the Process Platform components from Process Application components, compare the concepts section. Especially, it is helpful if you intend to build a central Process Platform and multiple Process applications using it.</p> <p>In general, this is one of the main use cases for Polyflow framework itself, but the distribution aspects adds technical complexity to the resulting architecture. Especially, following the architecture blueprint of Axon Framework, the three buses (Command bus, Event bus and Query bus) needs to be distributed and act as connecting infrastructure between components.</p>"},{"location":"introduction/deployment.html#distribution-using-axon-server-core-component-as-part-of-process-platform","title":"Distribution using Axon Server (core component as part of process platform)","text":"<p>Requirements:</p> <ul> <li>Event Store</li> <li>distributed command bus</li> <li>distributed event bus</li> </ul> <p>Axon Server provides an implementation for this requirement leading to a distributed buses and a central Event Store. It is easy to use, easy to configure and easy to run. If you need a HA setup, you will need the Enterprise license of Axon Server. Essentially, if you don't have another HA ready-to use messaging, this scenario might be your way to go.</p> <p>This scenario supports:</p> <ul> <li>central Process Platform components (core components and their projections)</li> <li>free choice for projection persistence (since Axon Server supports event replay)</li> <li>no direct synchronous communication between Process Platform and Process Application is required (e.g. via REST, since it is routed via command, event   and query bus)</li> <li>central components should to be HA available </li> <li>support routing of interaction task commands</li> </ul> <p>The following diagram depicts the distribution of the components and the messaging:</p> <p></p>"},{"location":"introduction/deployment.html#distribution-using-axon-server-core-component-as-part-of-process-engine","title":"Distribution using Axon Server (core component as part of process engine)","text":"<p>Requirements:</p> <ul> <li>Event store</li> <li>distributed event bus</li> </ul> <p>Axon Server provides an implementation for this requirement leading to a distributed buses and a central Event Store. It is easy to use, easy to configure and easy to run. If you need a HA setup, you will need the Enterprise license of Axon Server. Essentially, if you don't have another HA ready-to use messaging, this scenario might be your way to go.</p> <p>This scenario supports:</p> <ul> <li>Core components deployed locally to the process engine (including task pool and data pool and their projections)</li> <li>free choice for projection persistence (since Axon Server supports event replay)</li> <li>direct communication between task list / engines required (addressing, routing)</li> <li>local core components for higher resilience (nothing can fail in local task and data entry processing)</li> </ul> <p>The following diagram depicts the distribution of the components and the messaging:</p> <p></p>"},{"location":"introduction/deployment.html#distribution-without-axon-server","title":"Distribution without Axon Server","text":"<p>If you already have another messaging at place, like Kafka or RabbitMQ, you might skip the usage of Axon Server. In doing so, you will be responsible for distribution of events and will need to surrender some features.</p> <p>This scenario supports:</p> <ul> <li>Core components deployed locally to the process engine (including task pool and data pool and their projections)</li> <li>view MUST be persistent (no replay supported)</li> <li>direct communication between task list / engines required (addressing, routing)</li> <li>local core components for higher resilience (nothing can fail in local task and data entry processing)</li> </ul> <p>The following diagram depicts the distribution of the components and the messaging. See Distributed using Kafka</p> <p></p>"},{"location":"introduction/features.html","title":"Features","text":""},{"location":"introduction/features.html#task-pool","title":"Task Pool","text":"<p>A task list is an application that shows a list of tasks for each individual user, based on the user's profile, roles and authorizations. Polyflow's <code>taskpool</code> library provides a backend from which task lists can be served.</p> <p>Note</p> <p>If you are using taskpool with Camunda Platform 7, it can be seen as a replacement resp. add-on for Camunda's <code>TaskService</code>.</p> <p>The <code>taskpool</code> library provides the following features:</p> <ul> <li>Task mirroring: provides a list of tasks in the system including all standard task attributes provided by the process engine</li> <li>Include additional attributes that are important for processing</li> <li>Reacts on all task life cycle events fired by the process engine, automatically publishes user tasks to the <code>taskpool</code></li> <li>High performance queries: creates read-optimized projections including task-, process- and business data</li> <li>Centralized task list: running several Camunda BPM Engines in several applications is a common use case for larger companies. From the user's perspective, it   is not feasible to login to several task lists and check for relevant user tasks. The demand for a centralized task list can be addressed by using the   central <code>taskpool</code> component to which tasks from several process engines are transmitted over the network.</li> <li>Data enrichment: all scenarios, in which the data is not stored in the process payload, result in a cascade of queries executed after fetching the tasks. In   contrast to that, the usage of the <code>taskpool</code> library with a data enrichment plugin mechanism allows for caching additional business data along with the task   information.</li> </ul>"},{"location":"introduction/features.html#data-pool","title":"Data Pool","text":"<p>Each process instance works on one or more business objects and a business object's lifecycle usually spans a longer period of time than the process instance runtime. It's a common requirement to search for these business objects (independently of process tasks) and get a list of these objects including their current statuses (e.g. DRAFT, IN_PROGRESS, COMPLETED). The <code>datapool</code> library provides the necessary features to implement a high-performance Business Object View:</p> <ul> <li>Business object API providing additional attributes important for processing</li> <li>Business object modification API for creating an audit log (aka business object history)</li> <li>Authorization API for business objects</li> </ul>"},{"location":"introduction/features.html#process-definition-pool","title":"Process Definition Pool","text":"<p>A process repository provides a list of running instances and a list of process definitions deployed in the process engines connected to the library. It provides the following features:</p> <ul> <li>List of startable process definitions (including URLs to start forms)</li> <li>List of running process instances</li> <li>Reacts on life cycle events of process instance</li> </ul>"},{"location":"introduction/features.html#process-instance-pool","title":"Process Instance Pool","text":"<p>All process instances started, suspended, resumed, completed aor deleted in the process engine are reflected in the <code>process instance pool</code> component.</p>"},{"location":"introduction/features.html#process-variable-pool","title":"Process Variable Pool","text":"<p>Along with business data entities being modified during the execution of the business processes, the business process instance itself holds a collection of so-called process variables, representing the state of the execution. In contrast to the business data entities, their lifecycle is bound to the lifecycle of the business process instance. For different reasons the requirement might exist to have rapid access to the process variables of a running process instance, which is provided by the <code>taskpool</code> library.</p>"},{"location":"introduction/features.html#integration","title":"Integration","text":"<ul> <li>Generic task sender</li> <li>Generic data entry sender</li> <li>Camunda BPM collector (tasks, process definitions, process instances, process variables)</li> </ul>"},{"location":"introduction/solution-architecture.html","title":"Solution Architecture","text":""},{"location":"introduction/solution-architecture.html#general-idea","title":"General Idea","text":"<p>The implementation of a single (small) process application can be easily done using the process engine library itself (for example like Camunda BPM). If the solution becomes larger, for example by setting up multiple engines for different processes or if the load on a single process engine becomes unmanageable, it is worth to separate the solution into process specific and process agnostic parts. We call the process specific part of the solution Process Application and the process agnostic part Process Platform, as described in the concept section.</p> <p>Based on the assumption of the asymmetric read/write characteristics of task-oriented process applications, we decided to apply the Command Query Responsibility Segregation (CQRS) pattern for the architectural design. As a result, we supply components to collect the information from the process engines and create a read-optimized projections with user tasks and correlated business data. The components can be easily integrated into process applications and be used as foundation to build parts of the process platform.</p>"},{"location":"introduction/solution-architecture.html#design-decisions","title":"Design Decisions","text":"<p>We decided to build the library as a collection of loosely-coupled components which can be used during the construction of the process automation solution in different ways, depending on your Usage Scenario.</p> <p>The process platform is a central application consisting of business process independent components like a central user management, task inbox (aka task list), a business object view, audit logs and others. One or many process applications integrate with the process platform by implementing individual business processes and provide user tasks and business data changes to it. They may also ship application frontends, which are integrated into/with the frontends of the process platform, including business object views, user task forms and other required pieces.</p> <p>The following diagram depicts the overall logical architecture:</p> <p></p>"},{"location":"introduction/solution-architecture.html#implementation-decisions","title":"Implementation Decisions","text":"<p>The components are implemented using Kotlin programming language and rely on SpringBoot as execution environment. They make a massive use of Axon Framework as a basis of the CQRS implementation. In addition, we rely on event sourcing (ES) as persistence implementation. Due to the usage of the Axon Framework, you can choose the Event Store implementation, based on the available technology. Available options for the Event Store are Axon Server, JDBC, JPA or Mongo.</p> <p>The read-optimized projections use an internal API for constructing query models. Several query models are available: In-memory, Mongo and JPA are choices matching different requirements of the scenarios of process applications and process platform. All query models implement the same public API allowing an easy exchange of the implementation depending on available technology or changing requirements.</p> <p>Several integration components are available for collecting information from process engines and other third-party applications. Along with generic collectors for integrations of custom components, a specialized integration component for Camunda BPM engine is provided. It seamlessly integrates with the Camunda BPM engine using Camunda Engine's Plug-in mechanism and automatically delivers process definitions, process instances, process variables from the process engine. In addition, it allows enrichment of user tasks with custom process data. By doing so, the Camunda Integration component allows to include more information into user tasks and allows the task read projections to outperform the Camunda Task Service, providing more features and higher performance.</p> <p>The following figure demonstrates the architecture of the Camunda Integration component.</p> <p></p> <p>The loosely-coupled nature of the Polyflow framework supports different deployment strategies. It can be used in the context of a single process engine as well as in the context of a process landscape with multiple engines. In doing so, the Polyflow components can be used in a central system for collection, storage and provisioning of the user tasks and business data for the entire process landscape. The usage of the generic integration components allows for integration of heterogeneous process engines and other task and business data systems. For more details, please check the documentation on Deployment Scenarios. </p>"},{"location":"reference-guide/index.html","title":"Reference Overview","text":"<p>This reference guide is a primary source of information in order to understand how Polyflow components are used and how to configure them. It is divided into tow major sections:</p> <ul> <li>Components</li> <li>Configuration</li> </ul>"},{"location":"reference-guide/components/index.html","title":"Polyflow Components","text":"<p>We decided to build a library as a collection of loose-coupled components which can be used during the construction of the process automation solution. In doing so, we provide Process Engine Integration Components which are intended to be deployed as a part of the process application. In addition, we provide Process Platform Components which serve as a building blocks for the process platform itself.</p>"},{"location":"reference-guide/components/index.html#process-engine-integration-components","title":"Process Engine Integration Components","text":"<p>The Process Engine Integration Components are designed to be a part of process application deployment and react on engine changes / interact with the engine. These are split into common components which are independent of the used product and framework-dependent adapters:</p>"},{"location":"reference-guide/components/index.html#common-integration-components","title":"Common Integration Components","text":"<ul> <li>Datapool Sender</li> <li>Taskpool Sender</li> </ul>"},{"location":"reference-guide/components/index.html#camunda-bpm-integration-components","title":"Camunda BPM Integration Components","text":"<ul> <li>Camunda BPM Engine Interaction Client</li> <li>Camunda BPM Engine Taskpool Collector</li> <li>Camunda BPM Engine Taskpool Spring Boot Starter</li> </ul>"},{"location":"reference-guide/components/index.html#process-platform-components","title":"Process Platform Components","text":"<p>Process Platform Components are designed to build the process platform. The platform provides common functionality used by all process applications like common user management, unified task list and so on.</p>"},{"location":"reference-guide/components/index.html#core-components","title":"Core Components","text":"<p>Core Components are responsible for the processing of commands about user tasks, process instances, process variables, business data items and form an event stream consumed by the view components. Depending on the scenario, they can be deployed either within the process application, process platform or even completely separately.</p> <ul> <li>Taskpool Core</li> <li>Datapool Core</li> </ul>"},{"location":"reference-guide/components/index.html#view-components","title":"View Components","text":"<p>View Components are responsible for creation of a unified read-only projection of process definitions, process instances, process variables, user tasks and business data items. They are typically deployed as a part of the process platform.</p> <ul> <li>View API</li> <li>View API Client</li> <li>In-Memory View</li> <li>JPA View</li> <li>Mongo DB View</li> <li>Property Form URL Resolver</li> </ul>"},{"location":"reference-guide/components/index.html#other-components","title":"Other Components","text":"<ul> <li>Variable Serializer</li> <li>Tasklist URL Resolver</li> <li>Bus Jackson</li> </ul>"},{"location":"reference-guide/components/camunda-interaction-client.html","title":"Camunda Engine Interaction Client","text":""},{"location":"reference-guide/components/camunda-interaction-client.html#camunda-engine-interaction-client","title":"Camunda Engine Interaction Client","text":""},{"location":"reference-guide/components/camunda-interaction-client.html#purpose","title":"Purpose","text":"<p>This component performs changes delivered by Camunda Interaction Events on Camunda BPM engine. The following Camunda Interaction Events are supported:</p> <ul> <li>Claim User Task</li> <li>Unclaim User Task</li> <li>Defer User Task</li> <li>Undefer User Task</li> <li>Complete User Task</li> </ul>"},{"location":"reference-guide/components/camunda-interaction-client.html#usage-and-configuration","title":"Usage and configuration","text":"<p>To use Camunda Engine Interaction Client please add the following artifact to your classpath:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.holunda.polyflow&lt;/groupId&gt;\n&lt;artifactId&gt;polyflow-camunda-bpm-engine-client&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>In your <code>application.yml</code> configure the application name of your process engine, to receive commands:</p> <pre><code>polyflow:\n  integration:\n    client:\n      camunda:\n        application-name: my-process-application # defaults to ${spring.application.name}\n</code></pre>"},{"location":"reference-guide/components/camunda-starter.html","title":"Camunda Engine Taskpool Support SpringBoot Starter","text":""},{"location":"reference-guide/components/camunda-starter.html#purpose","title":"Purpose","text":"<p>The Polyflow Camunda Platform 7 SpringBoot Starter is a convenience module providing a single module dependency to be included in the process application. It includes all process application modules and provides meaningful defaults for their options.</p>"},{"location":"reference-guide/components/camunda-starter.html#configuration","title":"Configuration","text":"<p>In order to enable the starter, please put the following dependency on your class path:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.holunda.polyflow&lt;/groupId&gt;\n&lt;artifactId&gt;polyflow-camunda-bpm-springboot-starter&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The included <code>TaskpoolEngineSupportConfiguration</code> is a SpringBoot AutoConfiguration that configures the required components. If you want to configure it manually, please add the <code>@EnableTaskpoolEngineSupport</code> annotation on any <code>@Configuration</code> annotated class of your SpringBoot application.</p> <p>The <code>@EnableTaskpoolEngineSupport</code> annotation has the same effect as the following block of annotations:</p> <pre><code>@EnableCamundaTaskpoolCollector\n@EnableDataEntrySender\npublic class MyApplication {\n//...\n}\n</code></pre>"},{"location":"reference-guide/components/camunda-taskpool-collector.html","title":"Taskpool Collector","text":""},{"location":"reference-guide/components/camunda-taskpool-collector.html#purpose","title":"Purpose","text":"<p>Taskpool Collector is a component deployed as a part of the process application (aside with Camunda BPM Engine) that is responsible for collecting information from the Camunda BPM Engine. It detects the intent of the operations executed inside the engine and creates the corresponding commands for the Taskpool. The commands are enriched with data and transmitted to other Taskpool components (via Command Bus).</p> <p>In the following description, we use the terms event and command. Event denotes an entity received from Camunda BPM Engine (from delegate event listener or from history event listener) which is passed over to the Taskpool Collector using internal Spring eventing mechanism. The Taskpool Collector converts the series of such events into a Taskpool Command - an entity carrying an intent of change inside the Taskpool core. Please note that event has another meaning in CQRS/ES systems and other components of the Taskpool, but in the context of Taskpool collector an event always originates from Spring eventing.</p>"},{"location":"reference-guide/components/camunda-taskpool-collector.html#features","title":"Features","text":"<ul> <li>Collection of process definitions</li> <li>Collection of process instance events</li> <li>Collection of process variable change events</li> <li>Collection of task events and history events</li> <li>Creation of task engine commands</li> <li>Enrichment of task engine commands with process variables</li> <li>Attachment of correlation information to task engine commands</li> <li>Transmission of commands to Axon command bus</li> <li>Provision of properties for process application</li> </ul>"},{"location":"reference-guide/components/camunda-taskpool-collector.html#architecture","title":"Architecture","text":"<p>The Taskpool Collector consists of several components which can be divided into the following groups:</p> <ul> <li>Event collectors services are responsible for gathering information and forming commands</li> <li>Processors are performing command manipulation (e.g. command enrichment with payload and data correlation)</li> <li>Command senders are part of <code>command-sender</code> component and are responsible for accumulating commands and sending them to Axon Command List Gateway</li> </ul>"},{"location":"reference-guide/components/camunda-taskpool-collector.html#usage-and-configuration","title":"Usage and configuration","text":"<p>In order to enable collector component, include the Maven dependency to your process application:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.holunda.polyflow\n    &lt;groupId&gt;\n&lt;artifactId&gt;polyflow-camunda-bpm-taskpool-collector&lt;/artifactId&gt;\n&lt;version&gt;${camunda-taskpool.version}&lt;/version&gt;\n&lt;dependency&gt;\n</code></pre> <p>Then activate the Taskpool collector by providing the annotation on any Spring Configuration:</p> <pre><code>@Configuration\n@Import(CamundaTaskpoolCollectorConfiguration.class)\nclass MyProcessApplicationConfiguration {\n\n}\n</code></pre>"},{"location":"reference-guide/components/camunda-taskpool-collector.html#event-collection","title":"Event collection","text":"<p>Taskpool Collector registers Spring Event Listener to the following events, fired by Camunda Eventing Engine Plugin:</p> <ul> <li><code>DelegateTask</code> events:    create    update     delete    complete</li> <li><code>HistoryEvent</code> events:    HistoricTaskInstanceEvent    HistoricIdentityLinkLogEvent    HistoricProcessInstanceEventEntity    HistoricVariableUpdateEventEntity   ** HistoricDetailVariableInstanceUpdateEntity</li> </ul> <p>The events are transformed into corresponding commands and passed over to the processor layer. Until Camunda Platform 7.19, the eventing is fired using custom listeners only and polyflow components don't rely on that but rather on own implementation of built-in (unskippable) listeners. For this purpose, it is important to disable Camunda Platform custom listeners by setting <code>camunda.bpm.eventing.task</code> property to <code>false</code>.   </p>"},{"location":"reference-guide/components/camunda-taskpool-collector.html#task-commands-enrichment","title":"Task commands enrichment","text":"<p>Alongside with attributes received from the Camunda BPM engine, the engine task commands can be enriched with additional attributes.</p> <p>There are three enrichment modes available controlled by the <code>polyflow.integration.collector.camunda.task.enricher.type</code> property:</p> <ul> <li><code>no</code>: No enrichment takes place</li> <li><code>process-variables</code>: Enrichment of engine task commands with process variables</li> <li><code>custom</code>: User provides own implementation</li> </ul>"},{"location":"reference-guide/components/camunda-taskpool-collector.html#process-variable-enrichment","title":"Process variable enrichment","text":"<p>In particular cases, the data enclosed into task attributes is not sufficient for the task list or other user-related components. The information may be available as process variables and need to be attached to the task in the taskpool. This is where Process Variable Task Enricher can be used. For this purpose, active it, setting the property <code>polyflow.integration.collector.camunda.task.enricher.type</code> to <code>process-variables</code> and the enricher will put process variables into the task payload.</p> <p>You can control what variables will be put into task command payload by providing the Process Variables Filter. The <code>ProcessVariablesFilter</code> is a Spring bean holding a list of individual <code>VariableFilter</code> - at most one per process definition key and optionally one without process definition key (a global filter). If the filter is not provded, a default filter is used which is an empty <code>EXCLUDE</code> filter, resulting in all process variables being attached to the user task.</p> <p>A <code>VariableFilter</code> can be of the following type:</p> <ul> <li><code>TaskVariableFilter</code>:    <code>INCLUDE</code>: task-level include filter, denoting a list of variables to be added for the task defined in the filter.    <code>EXCLUDE</code>: task-level exclude filter, denoting a list of variables to be ignored for the task defined in the filter. All other variables are included.</li> <li><code>ProcessVariableFilter</code> with process definition key:    <code>INCLUDE</code>: process-level include filter, denoting a list of variables to be added for all tasks of the process.    <code>EXCLUDE</code>: process-level exclude filter, denoting a list of variables to be ignored for all tasks of the process.</li> <li><code>ProcessVariableFilter</code> without process definition key:    <code>INCLUDE</code>: global include filter, denoting a list of variables to be added for all tasks of all processes for which no dedicated <code>ProcessVariableFilter</code> is   defined.    <code>EXCLUDE</code>: global exclude filter, denoting a list of variables to be ignored for all tasks of all processes for which no dedicated <code>ProcessVariableFilter</code>   is defined.</li> </ul> <p>Here is an example, how the process variable filter can configure the enrichment:</p> <pre><code>  @Configuration\npublic class MyTaskCollectorConfiguration {\n\n@Bean\npublic ProcessVariablesFilter myProcessVariablesFilter() {\n\nreturn new ProcessVariablesFilter(\n// define a variable filter for every process\nnew VariableFilter[]{\n// define for every process definition\n// either a TaskVariableFilter or ProcessVariableFilter\nnew TaskVariableFilter(\nProcessApproveRequest.KEY,\n// filter type\nFilterType.INCLUDE,\nImmutableMap.&lt;String, List&lt;String&gt;&gt;builder()\n// define a variable filter for every task of the process\n.put(ProcessApproveRequest.Elements.APPROVE_REQUEST, Lists.newArrayList(\nProcessApproveRequest.Variables.REQUEST_ID,\nProcessApproveRequest.Variables.ORIGINATOR)\n)\n// and again\n.put(ProcessApproveRequest.Elements.AMEND_REQUEST, Lists.newArrayList(\nProcessApproveRequest.Variables.REQUEST_ID,\nProcessApproveRequest.Variables.COMMENT,\nProcessApproveRequest.Variables.APPLICANT)\n).build()\n),\n// optionally add a global filter for all processes\n// for that no individual filter was created\nnew ProcessVariableFilter(FilterType.INCLUDE,\nLists.newArrayList(CommonProcessVariables.CUSTOMER_ID))\n}\n);\n}\n}\n</code></pre> <p>Note</p> <p>If you want to implement a custom enrichment, please provide your own implementation of the interface <code>VariablesEnricher</code>   (register a Spring Component of the type) and set the property <code>polyflow.integration.collector.camunda.task.enricher.type</code> to <code>custom</code>.</p>"},{"location":"reference-guide/components/camunda-taskpool-collector.html#data-correlation","title":"Data Correlation","text":"<p>Apart from task payload attached by the enricher, the so-called Correlation with data entries can be configured. The data correlation allows to attach one or several references (that is a pair of values <code>entryType</code> and <code>entryId</code>) of business data entry(ies) to a task. In the projection (which is used for querying of tasks) these correlations are resolved and the information from business data events can be shown together with task information.</p> <p>The correlation to data events can be configured by providing a <code>ProcessVariablesCorrelator</code> bean. Here is an example how this can be done:</p> <pre><code>@Bean\nfun processVariablesCorrelator() = ProcessVariablesCorrelator(\n// define correlation for every process\nProcessVariableCorrelation(\nProcessApproveRequest.KEY,\nmapOf(\n// define a correlation for every task needed\nProcessApproveRequest.Elements.APPROVE_REQUEST to mapOf(\nProcessApproveRequest.Variables.REQUEST_ID to BusinessDataEntry.REQUEST\n)\n),\n// define a correlation globally (for the whole process)\nmapOf(ProcessApproveRequest.Variables.REQUEST_ID to BusinessDataEntry.REQUEST)\n)\n)\n</code></pre> <p>The process variable correlator holds a list of process variable correlations - one for every process definition key. Every <code>ProcessVariableCorrelation</code> configures for all tasks or for an individual task by providing a so-called correlation map. A correlation map is keyed by the name of a process variable inside Camunda Process Engine and holds the type of business data entry as value.</p> <p>Here is an example. Imagine the process instance is storing the id of an approval request in a process variable called <code>varRequestId</code>. The system responsible for storing approval requests fires data entry events supplying the data and using the entry type <code>io.my.approvalRequest</code> and the id of the request as <code>entryId</code>. In order to create a correlation in task <code>task_approve_request</code> of the <code>process_approval_process</code> we would provide the following configuration of the correlator:</p> <pre><code>@Bean\nfun processVariablesCorrelator() = ProcessVariablesCorrelator(\n\nProcessVariableCorrelation(\n\"process_approval_process\",\nmapOf(\n\"task_approve_request\" to mapOf(\n// process variable 'varRequestId' holds the id of a data entry of type 'io.my.approvalRequest'\n\"varRequestId\" to \"io.my.approvalRequest\"\n)\n)\n)\n)\n</code></pre> <p>If the process instance now contains the approval request id <code>\"4711\"</code> in the process variable <code>varRequestId</code> and the process reaches the task <code>task_approve_request</code>, the task will get the following correlation created (here written in JSON):</p> <pre><code>\"correlations\": [\n{ \"entryType\": \"approvalRequest\", \"entryId\": \"4711\" }\n]\n</code></pre>"},{"location":"reference-guide/components/camunda-taskpool-collector.html#message-codes","title":"Message codes","text":"<p>Please note that the logger root hierarchy is <code>io.holunda.polyflow.taskpool.collector</code></p> Message Code Severity Logger* Description Meaning <code>COLLECTOR-001</code> <code>INFO</code> Task commands will be collected. <code>COLLECTOR-002</code> <code>INFO</code> Task commands not be collected. <code>COLLECTOR-005</code> <code>TRACE</code> <code>.process.definition</code> Sending process definition command: $command <code>COLLECTOR-006</code> <code>TRACE</code> <code>.process.instance</code> Sending process instance command: $command <code>COLLECTOR-007</code> <code>TRACE</code> <code>.process.variable</code> Sending process variable command: $command <code>COLLECTOR-008</code> <code>TRACE</code> <code>.task</code> Sending engine task command: $command. <code>ENRICHER-001</code> <code>INFO</code> Task commands will be enriched with process variables. <code>ENRICHER-002</code> <code>INFO</code> Task commands will not be enriched. <code>ENRICHER-003</code> <code>INFO</code> Task commands will be enriched by a custom enricher. <code>ENRICHER-004</code> <code>DEBUG</code> <code>.task.enricher</code> Could not enrich variables from running execution ${command.sourceReference.executionId}, since it doesn't exist (anymore)."},{"location":"reference-guide/components/camunda-taskpool-collector.html#task-assignment","title":"Task Assignment","text":"<p>User task assignment is a core functionality for every process application fostering task oriented work. By default, Taskpool Collector uses information from Camunda User Task and maps that one-to-one to properties of the user task commands. The task attribute <code>assignee</code>, <code>candidateUsers</code> and <code>candidateGroups</code> are mapped to the corresponding attributes automatically.</p> <p>To control the task assignment mode you can configure taskpool collector using application properties. The property  <code>polyflow.integration.collector.camunda.task.assigner.type</code> has the following values:</p> <ul> <li><code>no</code>: No additional assignment takes place, the Camunda task attributes are used (default)</li> <li><code>process-variables</code>: Use process variables for assignment information, see below</li> <li><code>custom</code>: User provides own implementation implementing a bean implementing <code>TaskAssigner</code> interface.</li> </ul> <p>If the value is set to <code>process-variables</code>, you can set up a constant mapping defining the process variables carrying the assignment information. The corresponding properties are:</p> <pre><code>polyflow:\nintegration:\ncollector:\ncamunda:\ntask:\nassigner:\ntype: process-variables\nassignee: my-assignee-var\ncandidateUsers: my-candidate-users-var candidateGroup: my-candidate-group-var\n</code></pre>"},{"location":"reference-guide/components/camunda-taskpool-collector.html#task-importer","title":"Task Importer","text":"<p>Alongside with the event-based Task Collector based on Camunda Eventing, there exists a dedicated service which can query Camunda database for existing user tasks and publish the results. In order to avoid duplications in tasks, the collected tasks are filtered by a special filter. Currently, you may choose between the supplied <code>eventstore</code> filter or supply your own <code>custom</code> filter by providing your own implementation of a <code>EngineTaskCommandFilter</code> interface as  a Spring Bean. If you want to use this task importer facility, you need to activate it first in your application configuration.</p> <p>The following property block is used for configuration:</p> <pre><code>polyflow:\nintegration:\ncollector:\ncamunda:\ntask:\nimporter:\nenabled: true\ntask-filter-type: eventstore\n</code></pre> <p>By doing so, the <code>TaskServiceCollectorService</code> Bean is made available and can be used to trigger the import. The <code>eventstore</code> filter is useful in scenarios, in which the Taskpool Core is deployed on together with Taskpool Collector as part of the Process Application.</p>"},{"location":"reference-guide/components/common-datapool-sender.html","title":"Datapool Sender","text":""},{"location":"reference-guide/components/common-datapool-sender.html#purpose","title":"Purpose","text":"<p>Datapool sender is a component usually deployed as a part of the process application (but not necessary) that is responsible for collecting the Business Data Events fired by the application in order to allow for creation of a business data projection. In doing so, it collects and transmits it to Datapool Core.</p>"},{"location":"reference-guide/components/common-datapool-sender.html#features","title":"Features","text":"<ul> <li>Provides an API to submit arbitrary changes of business entities</li> <li>Provides an API to track changes (aka. Audit Log)</li> <li>Authorization on business entries</li> <li>Transmission of business entries commands</li> </ul>"},{"location":"reference-guide/components/common-datapool-sender.html#usage-and-configuration","title":"Usage and configuration","text":"<pre><code>    &lt;dependency&gt;\n&lt;groupId&gt;io.holunda.polyflow&lt;/groupId&gt;\n&lt;artifactId&gt;polyflow-datapool-sender&lt;/artifactId&gt;\n&lt;version&gt;${camunda-taskpool.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Then activate the datapool sender by providing the annotation on any Spring Configuration:</p> <pre><code>@Configuration\n@EnableDataEntrySender\nclass MyDataEntryCollectorConfiguration {\n\n}\n</code></pre>"},{"location":"reference-guide/components/common-datapool-sender.html#command-transmission","title":"Command transmission","text":"<p>In order to control sending of commands to command gateway, the command sender activation property <code>polyflow.integration.sender.data-entry.enabled</code> (default is <code>true</code>) is available. If disabled, the command sender will log any command instead of sending it to the command gateway.</p> <p>In addition, you can control by the property <code>polyflow.integration.sender.data-entry.type</code> if you want to use the default command sender or provide your own implementation. The default provided command sender (type: <code>simple</code>) just sends the commands synchronously using Axon Command Bus.</p> <p>Note</p> <p>If you want to implement a custom command sending, please provide your own implementation of the interface <code>DataEntryCommandSender</code>    (register a Spring Component of the type) and set the property <code>polyflow.integration.sender.data-entry.type</code> to <code>custom</code>.</p>"},{"location":"reference-guide/components/common-datapool-sender.html#serialization-of-payload","title":"Serialization of payload","text":"<p>By default, the data entry sender will serialize payload of the <code>DataEntry</code> into a JSON-Map structure, in order to be received by projections (Data Pool View)  and storage of it, independent of the classes which might be not on the classpath of the projection (generic structure instead of a typed Java object structure). This serialization can be disabled by the sender property <code>polyflow.integration.sender.data-entry.serialize-payload=false</code>. </p>"},{"location":"reference-guide/components/common-datapool-sender.html#handling-command-transmission","title":"Handling command transmission","text":"<p>The commands sent by the <code>Datapool Sender</code> are received by Command Handlers. The latter may accept or reject commands, depending on the state of the aggregate and other components. The <code>SimpleDataEntryCommandSender</code> is informed about the command outcome. By default, it will log the outcome to console (success is logged in <code>DEBUG</code> log level, errors are using <code>ERROR</code> log level).</p> <p>In some situations it is required to take care of command outcome. A prominent example is to include a metric for command dispatching errors into monitoring. For doing so, it is possible to provide own handlers for success and error command outcome.</p> <p>For Data Entry Command Sender (as a part of <code>Datapool Sender</code>) please provide a Spring Bean implementing the <code>io.holunda.polyflow.datapool.sender.DataEntryCommandSuccessHandler</code>  and <code>io.holunda.polyflow.datapool.sender.DataEntryCommandErrorHandler</code> accordingly.</p> <pre><code>  @Bean\n@Primary\nfun dataEntryCommandSuccessHandler() = object: DataEntryCommandResultHandler {\noverride fun apply(commandMessage: Any, commandResultMessage: CommandResultMessage&lt;out Any?&gt;) {\n// do something here\nlogger.info { \"Success\" }\n}\n}\n\n@Bean\n@Primary\nfun dataEntryCommandErrorHandler() = object: DataEntryCommandErrorHandler {\noverride fun apply(commandMessage: Any, commandResultMessage: CommandResultMessage&lt;out Any?&gt;) {\n// do something here\nlogger.error { \"Error\" }\n}\n}\n</code></pre>"},{"location":"reference-guide/components/common-taskpool-sender.html","title":"Taskpool Sender","text":""},{"location":"reference-guide/components/common-taskpool-sender.html#purpose","title":"Purpose","text":"<p>Taskpool sender is a component usually deployed as a part of the process application that is responsible for sending tasks collected e.G. by Camunda Taskpool Collector to Taskpool Core Components as commands.</p>"},{"location":"reference-guide/components/common-taskpool-sender.html#features","title":"Features","text":"<ul> <li>Allows fine-grained control of transactional behaviour during command sending</li> <li>Allows to integrate custom success and error handling</li> </ul>"},{"location":"reference-guide/components/common-taskpool-sender.html#usage-and-configuration","title":"Usage and configuration","text":"<p>Note</p> <p>If you are using <code>polyflow-camunda-bpm-collector</code> component, the sender component is included and activated by default, and you can skip this step.</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.holunda.polyflow&lt;/groupId&gt;\n&lt;artifactId&gt;polyflow-taskpool-sender&lt;/artifactId&gt;\n&lt;version&gt;${camunda-taskpool.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Then activate the taskpool sender by providing the annotation on any Spring Configuration:</p> <pre><code>@Configuration\n@EnableTaskpoolSender\nclass MyDataEntryCollectorConfiguration {\n\n}\n</code></pre> <p>In order to control sending of commands to command sender, the command sender activation property <code>polyflow.integration.sender.task.enabled</code> is available. If disabled, the command sender will log any command instead of aggregating sending it to the command gateway.</p>"},{"location":"reference-guide/components/common-taskpool-sender.html#command-sender-types","title":"Command sender types","text":"<p>Out of the box, Polyflow supplies several command senders to match your deployment scenario. The property  <code>polyflow.integration.task.sender.type</code>  is used to switch between different commands senders.</p> Sender type Property value Description Simple simple Simple command sender, used to send every command directly to Command Bus. Transactional Direct tx Transactional accumulating command sender, sending accumulated commands along with the running transaction. Transactional Job txjob Transactional accumulating command sender, writing a Camunda job to send the commands in a new transaction. Custom custom Setting to provide your own sender implementation <p>The default provided command sender (type: <code>tx</code>) is collects all task commands during one transaction, group them by task id and accumulates the commands to the minimum reflecting the intent(s) of the task operation(s). It uses Axon Command Bus (encapsulated by the <code>AxonCommandListGateway</code> for sending the result over to the Axon command gateway.</p> <p>Note</p> <p>If you want to implement a custom command sending, please provide your own implementation of the interface <code>EngineTaskCommandSender</code> (register a Spring Component of the type) and set the property <code>polyflow.integration.task.sender.type</code> to <code>custom</code>.</p>"},{"location":"reference-guide/components/common-taskpool-sender.html#command-aggregation","title":"Command aggregation","text":"<p>The Spring event listeners receiving events from the Camunda Engine plugin are called before the engine commits the transaction. Since all processing inside collector component and enricher is performed synchronously, the sender must waits until transaction to be successfully committed before sending any commands to the Command Gateway. Otherwise, on any error the transaction would be rolled-back and the command would create an inconsistency between the taskpool and the engine.</p> <p>Depending on your deployment scenario, you may want to control the exact point in time, when the commands are sent to command gateway. The property <code>polyflow.integration.task.sender.send-within-transaction</code> is designed to influence this. If set to <code>true</code>, the commands are accumulated before the process engine transaction is committed, otherwise commands are sent after the process engine transaction is committed.</p> <p>Warning</p> <p>Never send commands over remote messaging before the transaction is committed, since you may produce unexpected results if Camunda fails   to commit the transaction.</p> <p>If commands are delivered to a local component (this is the case if taskpool core is deployed in the same deployment as collector and sender components), the sending transaction is spanned across the taskpool core component. In particular, this means that the command dispatch and emission of events are happening inside the same transaction (unit of work). The default <code>Transactional Direct</code> command sender will send the commands inside this Unit of Work. The <code>Transactional Job</code> command sender will write a Camunda Job and finish current Unit of Work without sending anything and rely on Camunda Job Worker to perform the transmission of the commands to the Command Bus. This sender is in particular helpful, if you need to finish </p>"},{"location":"reference-guide/components/common-taskpool-sender.html#serialization-of-payload","title":"Serialization of payload","text":"<p>By default, the data entry sender will serialize payload of the <code>DataEntry</code> into a JSON-Map structure, in order to be received by projections (Data Pool View) and storage of it, independent of the classes which might be not on the classpath of the projection (generic structure instead of a typed Java object structure). This serialization can be disabled by the sender property <code>polyflow.integration.task.sender.serialize-payload=false</code>.</p>"},{"location":"reference-guide/components/common-taskpool-sender.html#handling-command-transmission","title":"Handling command transmission","text":"<p>The commands sent via gateway (e.g. <code>AxonCommandListGateway</code>) are received by Command Handlers. The latter may accept or reject commands, depending on the state of the aggregate and other components. The <code>AxonCommandListGateway</code> is informed about the command outcome. By default, it will log the outcome to console (success is logged in <code>DEBUG</code> log level, errors are using <code>ERROR</code> log level).</p> <p>In some situations it is required to take care of command outcome. A prominent example is to include a metric for command dispatching errors into monitoring. For doing so, it is possible to provide own handlers for success and error command outcome. For this purpose, please provide a Spring Bean implementing the <code>CommandSuccessHandler</code>and <code>CommandErrorHandler</code> accordingly.</p> <p>Here is an example, how such a handler may look like:</p> <pre><code>@Bean\n@Primary\nfun taskCommandErrorHandler(): CommandErrorHandler = object : LoggingTaskCommandErrorHandler(logger) {\noverride fun apply(commandMessage: Any, commandResultMessage: CommandResultMessage&lt;out Any?&gt;) {\nlogger.info { \"&lt;--------- CUSTOM ERROR HANDLER REPORT ---------&gt;\" }\nsuper.apply(commandMessage, commandResultMessage)\nlogger.info { \"&lt;------------------- END -----------------------&gt;\" }\n}\n}\n</code></pre>"},{"location":"reference-guide/components/common-taskpool-sender.html#message-codes","title":"Message codes","text":"<p>Please note that the logger root hierarchy is <code>io.holunda.camunda.taskpool.sender</code></p> Message Code Severity Logger* Description Meaning <code>SENDER-001</code> <code>DEBUG</code> <code>.gateway</code> Sending command over gateway disabled by property. Would have sent command <code>payload</code>. Sending of any commands is disabled. <code>SENDER-002</code> <code>DEBUG</code> <code>.gateway</code> Successfully submitted command <code>payload</code>. Logging the successfully sent command. <code>SENDER-003</code> <code>ERROR</code> <code>.gateway</code> Sending command $commandMessage resulted in error Error sending command. <code>SENDER-004</code> <code>DEBUG</code> <code>.task</code> Process task sending is disabled by property. Would have sent $command. <code>SENDER-005</code> <code>DEBUG</code> <code>.task</code> Handling ${taskCommands.size} commands for task $taskId using command accumulator $accumulatorName <code>SENDER-006</code> <code>DEBUG</code> <code>.task</code> Handling ${taskCommands.size} commands for task $taskId using command accumulator $accumulatorName <code>SENDER-007</code> <code>DEBUG</code> <code>.process.definition</code> Process definition sending is disabled by property. Would have sent $command. <code>SENDER-007</code> <code>DEBUG</code> <code>.process.instance</code> Process instance sending is disabled by property. Would have sent $command. <code>SENDER-009</code> <code>DEBUG</code> <code>.process.variable</code> Process variable sending is disabled by property. Would have sent $command. <code>SENDER-011</code> <code>INFO</code> Taskpool task commands will be distributed over command bus. <code>SENDER-012</code> <code>INFO</code> Taskpool task command distribution is disabled by property. <code>SENDER-013</code> <code>INFO</code> Taskpool process definition commands will be distributed over command bus. <code>SENDER-014</code> <code>INFO</code> Taskpool process definition command distribution is disabled by property. <code>SENDER-015</code> <code>INFO</code> Taskpool process instance commands will be distributed over command bus. <code>SENDER-016</code> <code>INFO</code> Taskpool process instance command distribution is disabled by property. <code>SENDER-017</code> <code>INFO</code> Taskpool process variable commands will be distributed over command bus. <code>SENDER-018</code> <code>INFO</code> Taskpool process variable command distribution is disabled by property."},{"location":"reference-guide/components/core-datapool.html","title":"Datapool Core","text":""},{"location":"reference-guide/components/core-datapool.html#purpose","title":"Purpose","text":"<p>The component is responsible for maintaining and storing the consistent state of the datapool core concept of Business Data Entry.</p> <p>The component receives all commands and emits events, if changes are performed on underlying entities. The event stream is used to store all changes (purely event-sourced) and should be used by all other parties interested in changes.</p>"},{"location":"reference-guide/components/core-datapool.html#configuration","title":"Configuration","text":""},{"location":"reference-guide/components/core-datapool.html#component-activation","title":"Component activation","text":"<p>In order to activate Datapool Core component, please include the following dependency to your application</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.holunda.taskpool&lt;/grouId&gt;\n&lt;artifactId&gt;camunda-bpm-datapool-core&lt;/artifactId&gt;\n&lt;version&gt;${taskpool.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>and activate its configuration by adding the following to a Spring configuration:</p> <pre><code>@Configuration\n@EnablePolyflowDataPool\nclass MyConfiguration\n</code></pre>"},{"location":"reference-guide/components/core-datapool.html#revision-aware-projection","title":"Revision-Aware Projection","text":"<p>The in-memory data entry projection is supporting revision-aware projection queries. To activate this, you need to activate the correlation of revision attributes between your data entries commands and the data entry events. To do so, please activate the correlation provider by putting the following code snippet in the application containing the Datapool Core Component:</p> <pre><code>@Configuration\n@EnablePolyflowDataPool\nclass MyConfiguration {\n\n@Bean\nfun revisionAwareCorrelationDataProvider(): CorrelationDataProvider {\nreturn MultiCorrelationDataProvider&lt;CommandMessage&lt;Any&gt;&gt;(\nlistOf(\nMessageOriginProvider(),\nSimpleCorrelationDataProvider(RevisionValue.REVISION_KEY)\n)\n)\n}\n\n}\n</code></pre> <p>By doing so, if a command is sending revision information, it will be passed to the resulting event and will be received by the projection, so the latter will deliver revision information in query results. The use of <code>RevisionAwareQueryGateway</code> will allow querying for specific revisions in the data entry projection, see documentation of <code>axon-gateway-extension</code> project.</p>"},{"location":"reference-guide/components/core-datapool.html#strategies-to-optimize-data-entry-access","title":"Strategies to optimize data entry access","text":"<p>The Business Data Entry is implemented using an <code>Aggregate</code> pattern and the corresponding projection as a part of the view component. By default,  the Datapool Sender is used to send commands expressing the change of a Business Data Entry to the <code>Aggregate Root</code>,  which is emitting the corresponding event. If you are dealing with Business Data Entries with a very long lifetime, the number of events emitted by  the <code>Aggregate Root</code> may become large and impacts the load time of it (it is event-sourced). To improve the load time of the aggregate, we developed  two strategies which can be applied: special repository and snapshotting.</p> <p>Snapshotting uses standard Axon Snapshotting and will use the latest snapshot and additionally apply the events emitted after the last snapshot instead  of replaying all events ever emitted by the aggregate.</p> <p>The special repository uses the fact that <code>Data Entry Aggregate Root</code> state is not changed by update events and the first event it emits during creation  already contains everything the aggregates require during loading.</p> <p>In order to select the strategy best matching your use case, please consult the configuration section. </p>"},{"location":"reference-guide/components/core-taskpool.html","title":"Taskpool Core","text":""},{"location":"reference-guide/components/core-taskpool.html#purpose","title":"Purpose","text":"<p>The component is responsible for maintaining and storing the consistent state of the taskpool core concepts:</p> <ul> <li>Task (represents a user task instance)</li> <li>Process Definition (represents a process definition)</li> </ul> <p>The component receives all commands and emits events, if changes are performed on underlying entities. The event stream is used to store all changes (purely event-sourced) and should be used by all other parties interested in changes.</p>"},{"location":"reference-guide/components/other-bus-jackson.html","title":"Bus Jackson","text":""},{"location":"reference-guide/components/other-bus-jackson.html#purpose","title":"Purpose","text":"<p>The component is a helper component if you configure your Axon busses (command, event, query) to use Jackson for serialization of messages. It provides helper Jackson Modules to configure serialization of classes used by Polyflow. </p>"},{"location":"reference-guide/components/other-bus-jackson.html#configuration-and-usage","title":"Configuration and Usage","text":"<p>To use the component, please add the following dependency to your classpath</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.holunda.taskpool&lt;/grouId&gt;\n&lt;artifactId&gt;polyflow-bus-jackson&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Inside your Object Mapper configuration call</p> <pre><code>import io.holunda.polyflow.bus.jackson.configureTaskpoolJacksonObjectMapper\n\nclass MyConfiguration {\n@Bean\n@Qualifier(\"payloadObjectMapper\")\nfun payloadObjectMapper(): ObjectMapper {\nreturn ObjectMapper().configureTaskpoolJacksonObjectMapper()\n}\n}\n</code></pre> <p>If you are not using Jackson for serialization of Axon messages (commands, events and queries) you are ready to go.</p> <p>If you want to use Jackson as Axon message serialization message format the following configuration is required. In your application properties, set-up the following properties:</p> <pre><code>axon:\nserializer:\nevents: jackson\nmessages: jackson\ngeneral: jackson </code></pre> <p>In addition, define configure the <code>ObjectMapper</code> to be used by Axon Framework:</p> <pre><code>import io.holunda.polyflow.bus.jackson.configureTaskpoolJacksonObjectMapper\n\nclass MyConfiguration {\n\n@Bean(\"defaultAxonObjectMapper\")\n@Qualifier(\"defaultAxonObjectMapper\")\nfun defaultAxonObjectMapper(): ObjectMapper {\nreturn ObjectMapper().configureTaskpoolJacksonObjectMapper()\n}\n}\n</code></pre>"},{"location":"reference-guide/components/other-tasklist-url-resolver.html","title":"Tasklist URL Resolver","text":""},{"location":"reference-guide/components/other-tasklist-url-resolver.html#tasklist-url-resolver","title":"Tasklist URL Resolver","text":""},{"location":"reference-guide/components/other-tasklist-url-resolver.html#purpose","title":"Purpose","text":"<p>The Tasklist URL Resolver is a helper component helping to provide the URL of the task list for other components. It is not use by other components, but is helpful, if you complete tasks using SPA on the side of the process application and needs a redirection target resolution of the task list  after completion.</p>"},{"location":"reference-guide/components/other-tasklist-url-resolver.html#usage-and-configuration","title":"Usage and Configuration","text":"<p>To use Tasklist URL Resolver please add the following artifact to your classpath:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.holunda.polyflow&lt;/groupId&gt;\n&lt;artifactId&gt;polyflow-tasklist-url-resolver&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>In your <code>application.yml</code> either configure the property for the static tasklist URL:</p> <pre><code>polyflow:\n  integration:\n    tasklist:\n      tasklist-url: http://my-task-list.application.url/\n</code></pre> <p>or provide your own implementation of the <code>TasklistUrlResolver</code> interface as Spring Bean in your configuration:</p> <p><pre><code>import java.beans.BeanProperty;\n\n@Configuration\nclass MyConfiguration {\n\n@Bean\npublic TasklistUrlResolver myTasklistUrlResolver() {\nreturn MyTasklistUrlResolver();\n}\n}\n</code></pre> .</p>"},{"location":"reference-guide/components/other-variable-serializer.html","title":"Variable Serializer","text":""},{"location":"reference-guide/components/other-variable-serializer.html#purpose","title":"Purpose","text":"<p>Provides shared code configuring the variable serializer.</p>"},{"location":"reference-guide/components/view-api-client.html","title":"View API Client","text":""},{"location":"reference-guide/components/view-api-client.html#purpose","title":"Purpose","text":"<p>The Polyflow View API Client is a client for the users of the task-pool and the data-pool query API. It provides simple components which can be used  in order to query the configured views. By doing so, it defines an easy-to-use API for callers.</p>"},{"location":"reference-guide/components/view-api-client.html#usage","title":"Usage","text":"<p>Pleas put the following component to you class path:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.holunda.polyflow&lt;/groupId&gt;\n&lt;artifactId&gt;polyflow-view-api-client&lt;/artifactId&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The components available are:</p> <ul> <li><code>io.holunda.polyflow.view.DataEntryQueryClient</code></li> <li><code>io.holunda.polyflow.view.ProcessDefinitionQueryClient</code></li> <li><code>io.holunda.polyflow.view.ProcessInstanceQueryClient</code></li> <li><code>io.holunda.polyflow.view.ProcessVariableQueryClient</code></li> <li><code>io.holunda.polyflow.view.TaskQueryClient</code></li> </ul> <p>If you are using Kotlin, you might like the extension functions of the <code>QueryGateway</code> provided by <code>io.holunda.polyflow.view.QueryGatewayExt</code> object.  </p>"},{"location":"reference-guide/components/view-api.html","title":"View API","text":""},{"location":"reference-guide/components/view-api.html#purpose","title":"Purpose","text":"<p>Note</p> <p>If you are looking for a convenient way to send out queries (API for callers), please check the View API Client</p> <p>The Polyflow View API defines the interfaces for the implementers of the task-pool and the data-pool query API. It defines the main query types of the  common read-projections. Its main purpose is to create a public stable API which is independent of the implementations. There are multiple implementations  available:</p> <ul> <li>In-Memory View</li> <li>JPA View</li> <li>Mongo DB View</li> </ul> <p>In addition, the API supplies filtering functionality for handling requests of filtering of view results in form of attribute filters ( like <code>attribute=value&amp;attrubute2=value2&amp;task.name=taskname</code>). Especially, it defines the main concepts like <code>Criteria</code> and <code>Operator</code> and generic query paging and sorting.</p>"},{"location":"reference-guide/components/view-api.html#feature-support-matrix","title":"Feature support matrix","text":""},{"location":"reference-guide/components/view-api.html#task-api","title":"Task API","text":"<p>The Task API allows to query for tasks handled by the task-pool.</p> Query Type Description Payload types In-Memory JPA Mongo DB AllTasksQuery Retrieves a list of tasks applying additional filters List yes yes no TasksForUserQuery Retrieves a list of tasks accessible by the user and applying additional filters List yes yes yes TasksForGroupQuery Retrieves a list of tasks accessible by the user's group and applying additional filters List yes yes no TaskForIdQuery Retrieves a task by id (without any other filters) Task or null yes yes yes TasksForApplicationQuery Retrieves all tasks by given application name (without any further filters) List yes yes yes AllTasksWithDataEntriesQuery Retrieves a list of tasks applying additional filters and correlates result with data entries, if available List&lt;(Task, List) yes incubation no TasksWithDataEntriesForGroupQuery Retrieves a list of tasks accessible by the user's group and applying additional filters and correlates result with data entries, if available List&lt;(Task, List) yes incubation no TasksWithDataEntriesForUserQuery Retrieves a list of tasks accessible by the user and applying additional filters and correlates result with data entries, if available List&lt;(Task, List) yes incubation yes TaskWithDataEntriesForIdQuery Retrieves a task by id and correlates result with data entries, if available (Task, List) or null yes yes yes TaskCountByApplicationQuery Counts tasks grouped by application names, useful for monitoring List&lt;(ApplicationName, Count)&gt; yes no yes"},{"location":"reference-guide/components/view-api.html#process-definition-api","title":"Process Definition API","text":"<p>The Process Definition API allows to query for process definitions handled by the task-pool.</p> Query Type Description Payload types In-Memory JPA Mongo DB ProcessDefinitionsStartableByUserQuery Retrieves a list of process definitions start-able by user List yes yes yes"},{"location":"reference-guide/components/view-api.html#process-instance-api","title":"Process Instance API","text":"<p>The Process Instance API allows to query for process instances handled by the task-pool.</p> Query Type Description Payload types In-Memory JPA Mongo DB ProcessInstancesByStateQuery Retrieves a list of process instances by state (started, finished, etc) List yes yes no"},{"location":"reference-guide/components/view-api.html#process-variable-api-incubation","title":"Process Variable API (incubation)","text":"<p>The Process Variable API allows to query for process variables handled by the task-pool.</p> <p>Warning</p> <p>The Process Variable API is supporting revision-aware queries, which are currently only supported by JPA and In-Memory implementations.  </p> Query Type Description Payload types In-Memory JPA Mongo DB ProcessVariablesForInstanceQuery Retrieves a list of process variables for given process instance and matching provided filters List yes no no"},{"location":"reference-guide/components/view-api.html#data-entry-api","title":"Data Entry API","text":"<p>The Data Entry API allows to query for data entries handled by the data-pool.</p> <p>Warning</p> <p>The Data Entry API supports revision-aware queries by JPA and In-Memory implementations ONLY.</p> Query Type Description Payload types In-Memory JPA Mongo DB DataEntriesForUserQuery Retrieves a list of data entries accessible by the user with some additional filters. List yes yes yes DataEntryForIdentityQuery Retrieves a single data entry by type and an id DataEntry yes yes yes DataEntryForDataEntryTypeQuery Retrieves a list of data entries by type List yes yes yes DataEntriesQuery Retrieves a list of data entries matching filters List yes yes yes"},{"location":"reference-guide/components/view-api.html#revision-aware-query-support","title":"Revision-aware query support","text":"<p>Projections can be built in a way, that they support and store event revision information transported by the event metadata. By doing so, you might send an update of the model by specifying the update revision and are waiting for the eventually consistent event delivery to the projection of this update. In order to achieve this, you might specify the minimum revision the query result must fulfill in order to match your query request. See axon-gateway-extension for more details. Please note, that not all implementations are implementing this feature. Especially, Mongo DB View is currently NOT SUPPORTING Revision Aware queries. </p>"},{"location":"reference-guide/components/view-api.html#filtering-paging-and-sorting","title":"Filtering, Paging and Sorting","text":"<p>Task API and Data Entries API supports filtering, paging and sorting in queries resulting in multiple results. For Task API these are <code>AllTasksQuery</code>, <code>TasksForGroupQuery</code>, <code>TasksForUserQuery</code>, <code>AllTasksWithDataEntriesQuery</code>, <code>TasksWithDataEntriesForGroupQuery</code>, <code>TasksWithDataEntriesForUserQuery</code> and for Data Entries API these are <code>DataEntriesForUserQuery</code> and <code>DataEntriesQuery</code>.  The queries implement the <code>PageableSortableQuery</code> interface, allowing to limit the amount of results and provide an optional sorting:</p> <p><pre><code>interface PageableSortableQuery {\nval page: Int\nval size: Int\nval sort: String?\n}\n</code></pre> The <code>page</code> parameter denotes the page number to deliver (starting with <code>0</code>). The <code>size</code> parameter denotes the number of elements on a page. By default, the <code>page</code> is set to <code>0</code> and the size is set to <code>Int.MAX</code>. </p> <p>An optional <code>sort</code> parameter allows to sort the results by a field attribute.  The format of the <code>sort</code> string is <code>&lt;+|-&gt;filedName</code>, <code>+fieldName</code> means sort by <code>fieldName</code> ascending, <code>-fieldName</code> means sort by <code>fieldName</code> descending. The field must be a direct member of the result (<code>Task</code>, <code>TaskWithDataEntries</code> or <code>DataEntry</code>) and must be one of the following type:</p> <ul> <li>java.lang.Integer</li> <li>java.lang.String</li> <li>java.util.Date</li> <li>java.time.Instant</li> </ul> <p>To filter the results, you might supply a list of filters. A filter is an expression in format <code>fieldName&lt;op&gt;value</code>, where <code>fieldName</code> is addressing the attribute of the search result, <code>&lt;op&gt;</code> is one of <code>&lt;</code>, <code>=</code>, <code>%</code> and <code>&gt;</code> and <code>value</code> is a string representation of the values. The <code>fieldName</code> can point to an attribute of the result entity itself (<code>Task</code>, <code>TaskWithDataEntries</code>  or <code>DataEntry</code>) or point to the attribute inside the payload. To avoid a possible name clash, you must prefix the field name with <code>task.</code> if you want to filter on direct attributes of a task, and you must prefix the field name with <code>data</code> if you want to filter on direct attributes of a dta entry. For example, <code>task.priority=50</code> would deliver tasks with priority set to 50, and <code>data.entryType=info.polyflow.Order</code> will deliver data entries of type <code>info.polyflow.Order</code> only.</p> <p>Following operations are supported:</p> Filter Operation In-Memory JPA (Task Attributes) JPA (Data Entries Attributes) Mongo DB (Task Attributes) Mongo DB (Data Entries Attributes) <code>&lt;</code> Less than all, payload none none all, payload all, payload <code>&gt;</code> Greater than all, payload none none all, payload all, payload <code>=</code> Equals all, payload business key, payload entry id, entry type, type, payload, processing state, user status all, payload all, payload <code>%</code> Like all, payload name, description none none none <p>If the field name has no prefix of above, it is considered as an attribute inside the payload of data entry or enriched variables of a user task. For example, imagine you have a data entry with payload attributes <code>{ \"attribute\": \"value\", \"another\": 45 }</code>. In order to search for those, just specify <code>attribute=value</code> in your filter criteria. </p>"},{"location":"reference-guide/components/view-form-url-resolver.html","title":"Form URL Resolver","text":""},{"location":"reference-guide/components/view-form-url-resolver.html#purpose","title":"Purpose","text":"<p>Building a central Task List and Business Data Entry List requires a UI integration logic between the use case agnostic central component and use case specific forms for display of the User Tasks and Business Data Entries. There exist multiple options to provide this UI integration, like dynamic loading of UI components from the distributed use case specific application and process engines or simple redirection to those applications being able to  display the requested form. In many those implementations, the process platform components like Task List and Business Data Entry List need to resolve the particular URL of the process application endpoint. The <code>form-url-resolver</code> component is designed exactly for this purpose, if this resolution is static and can be performed based on configuration. </p>"},{"location":"reference-guide/components/view-form-url-resolver.html#configuration","title":"Configuration","text":"<p>In order to use the <code>form-url-resolver</code> please add the following dependency to your project:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.holunda.polyflow&lt;/groupId&gt;\n&lt;artifactId&gt;polyflow-form-url-resolver&lt;/artifactId&gt;\n&lt;version&gt;${polyflow.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>In your configuration, please add the following annotation to enable the resolver:</p> <pre><code>@EnablePropertyBasedFormUrlResolver\n@Configuration\nclass MyTasklistConfiguration {\n\n}\n</code></pre> <p>Using your <code>application.yaml</code> or corresponding properties file, please provide the configuration of the URLs. Te configuration is separated into provision of some defaults (for user tasks, data entries, processes or even entire applications) In general the resolution of the URL happens from most specific to most generic, see the example below. If the specific request matches the configuration (e.g. URL for the User Task with process definition <code>task1</code> of the application with application name <code>app1</code>) it will be returned (<code>https://app1.server.io/app/forms/task1/foo/${id}</code>), otherwise the default for the application or even the default template is taken.</p> <p><pre><code>polyflow:\nintegration:\nform-url-resolver:\ndefaultApplicationTemplate: \"http://localhost:8080/${applicationName}\"\ndefaultDataEntryTemplate: \"/${entryType}/${entryId}\"\ndefaultProcessTemplate: \"/${processDefinitionKey}/${formKey}\"\ndefaultTaskTemplate:  \"/forms/${formKey}/${id}\"\napplications:\n- app1:\nurl: \"https://app1.server.io/app\"\ntasks:\n- task1: \"/forms/task1/foo/${id}\"\n- task2: \"/bar/2/foo/${id}\"\nprocesses:\n- process1: \"/proc-1/start\"\n- process2: \"/proc/2/begin\"\n- app2:\nurl: \"https://foo.app2.com\"\ntasks:\n- otherTask1: \"/views/task1/${id}\"\n- otherTask2: \"/other/2/foo/${id}\"\n</code></pre> As you can see in the example above, the component supports simple text templating using <code>${}</code> to indicate the template variable. The variables which can be used are direct attributes of the object for which the URL is  resolved (see View API). The keys in the configuration are:</p> <ul> <li>Value of attribute <code>applicationName</code> for applications</li> <li>Value of attribute <code>processDefinitionKey</code> for processes</li> <li>Value of attribute <code>taskDefinitionKey</code> for tasks</li> <li>Value of attribute <code>entryType</code> for data entries</li> </ul>"},{"location":"reference-guide/components/view-jpa.html","title":"JPA View","text":""},{"location":"reference-guide/components/view-jpa.html#purpose","title":"Purpose","text":"<p>The JPA View is component responsible for creating read-projections of tasks and business data entries. It currently implements Datapool View API and Taskpool API and persists the projection as entities and relations in a RDBMS using JPA. It is a useful if the JPA persistence is already used in the project setup.</p>"},{"location":"reference-guide/components/view-jpa.html#features","title":"Features","text":"<ul> <li>stores representation of business data entries</li> <li>stores representation of process definitions</li> <li>stores representation of process instances</li> <li>provides single query API supporting single and subscription queries</li> </ul>"},{"location":"reference-guide/components/view-jpa.html#configuration-options","title":"Configuration options","text":"<p>In order to activate the JPA View implementation, please include the following dependency on your classpath:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.holunda.polyflow&lt;/groupId&gt;\n&lt;artifactId&gt;polyflow-view-jpa&lt;/artifactId&gt;\n&lt;version&gt;${polyflow.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The implementation relies on Spring Data JPA and needs to activate those. </p> <pre><code>@Configuration\n@EnablePolyflowJpaView\npublic class MyViewConfiguration {\n\n}\n</code></pre> <p>In addition, configure a database connection to database using <code>application.properties</code> or <code>application.yaml</code>:</p> <pre><code>spring:\n  jpa:\n    show-sql: false\n    open-in-view: true # disable JPA warning\n  datasource:\n    url: &lt;jdbc-connnection-string&gt;\n    username: &lt;db-user&gt;\n    password: &lt;db-password&gt;\n</code></pre> <p>The JPA view uses a special facility for creating search indexes on unstructured payload. For this purpose it converts the payload into a recursive map structure (in which every primitive type is a leaf and every complex type is decomposed via the map) using Jackson ObjectMapper and then create search indexes for all  property paths (<code>myObj1.myProperty2.myOtherEmbeddedProperty3</code>) and their values. You can provide some  configuration of this indexing process by the following configuration options:</p> <pre><code>polyflow.view.jpa:\n  payload-attribute-level-limit: 2\n  stored-items: task, data-entry, process-instance, process-definition\n  data-entry-filters:\n    include: myProperty2.myOtherEmbeddedProperty3, myProperty2.myOtherEmbeddedProperty2\n#    exclude: myProperty\n</code></pre> <p>In the example below you see the configuration of the limit of keying depth and usage of include/exclude filters of the keys. In addition, the <code>stored-items</code> property is holding a set of items to be persisted to the database. The possible values of  stored items are: <code>task</code>, <code>data-entry</code>, <code>process-instance</code> and <code>process-definition</code>. By setting this property, you can disable storage of items not required by your application and save space consumption of your database. The property defaults to <code>data-entry</code>.</p>"},{"location":"reference-guide/components/view-jpa.html#entity-scan","title":"Entity Scan","text":"<p>The JPA View utilizes Spring Data repositories and Hibernate entities inside the persistence layer. As a result, it declares a <code>@EntityScan</code>  and <code>@EnableJpaRepositories</code> annotations pointing at the corresponding locations. If you are using Spring Data JPA on your own, you will need to add the <code>@EntityScan</code> and <code>@EnableJpaRepositores</code> annotation pointing at your packages. In addition, please check Persistence configuration.</p>"},{"location":"reference-guide/components/view-jpa.html#logging","title":"Logging","text":"<p>The view implementation provides runtime details using standard logging facility. If you want to increase the logging level, please setup it e.g. in your <code>application.yaml</code>:</p> <pre><code>logging.level:\n  io.holunda.polyflow.view.jpa: DEBUG\n</code></pre>"},{"location":"reference-guide/components/view-jpa.html#db-tables","title":"DB Tables","text":"<p>The JPA View uses several tables to store the results. These are:</p> <ul> <li><code>PLF_DATA_ENTRY</code>: table for business data entries</li> <li><code>PLF_DATA_ENTRY_AUTHORIZATIONS</code>: table for authorization information of data entries</li> <li><code>PLF_DATA_ENTRY_PAYLOAD_ATTRIBUTES</code>: table for data entry attribute search index</li> <li><code>PLF_DATA_ENTRY_PROTOCOL</code>: table for data entry protocol entry (users, groups)</li> <li><code>PLF_PROC_DEF</code>: table for process definitions</li> <li><code>PLF_PROC_DEF_AUTHORIZATIONS</code>: table for authorization information of process definitions </li> <li><code>PLF_PROC_INSTANCE</code>: table for process instances</li> <li><code>PLF_TASK</code>: table for user tasks</li> <li><code>PLF_TASK_AUTHORIZATIONS</code>: table for authorization information of user tasks</li> <li><code>PLF_TASK_CORRELATIONS</code>: table for user task correlation information</li> <li><code>PLF_TASK_PAYLOAD_ATTRIBUTES</code>: table for user task attribute search index</li> <li><code>TRACKING_TOKEN</code>: table for Axon Tracking Tokens</li> </ul> <p>If you are interested in DDLs for the view, feel free to generate one using the following call of Apache Maven  <code>mvn -Pgenerate-sql -f view/jpa</code>. Currently, DDLs for the databases H2, MSSQL and PostgreSQL are generated into <code>target/</code> directory.</p>"},{"location":"reference-guide/components/view-mongo.html","title":"Mongo View","text":""},{"location":"reference-guide/components/view-mongo.html#purpose","title":"Purpose","text":"<p>The Mongo View is component responsible for creating read-projections of tasks and business data entries. It implements the Taskpool and Datapool View API and persists the projection as document collections in a Mongo database.</p>"},{"location":"reference-guide/components/view-mongo.html#features","title":"Features","text":"<ul> <li>stores JSON document representation of enriched tasks, process definitions and business data entries</li> <li>provides single query API</li> <li>provides subscription query API (reactive)</li> <li>switchable subscription query API (AxonServer or MongoDB ChangeStream)</li> </ul> <p>Warning</p> <p>Mongo DB View is currently NOT SUPPORTING Revision Aware queries.</p>"},{"location":"reference-guide/components/view-mongo.html#configuration-options","title":"Configuration options","text":"<p>In order to activate the Mongo implementation, please include the following dependency on your classpath:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.holunda.polyflow&lt;/groupId&gt;\n&lt;artifactId&gt;polyflow-view-mongo&lt;/artifactId&gt;\n&lt;version&gt;${polyflow.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>The implementation relies on Spring Data Mongo and needs to activate those. Please add the following annotation to any class marked as Spring Configuration loaded during initialization:</p> <pre><code>@Configuration\n@EnablePolyflowMongoView\n@Import({\norg.springframework.boot.autoconfigure.mongo.MongoAutoConfiguration.class,\norg.springframework.boot.autoconfigure.mongo.MongoReactiveAutoConfiguration.class,\norg.springframework.boot.autoconfigure.data.mongo.MongoDataAutoConfiguration.class,\norg.springframework.boot.autoconfigure.data.mongo.MongoReactiveDataAutoConfiguration.class\n})\npublic class MyViewConfiguration {\n\n}\n</code></pre> <p>In addition, configure a Mongo connection to database called <code>tasks-payload</code> using <code>application.properties</code> or <code>application.yaml</code>:</p> <pre><code>spring:\n  data:\n    mongodb:\n      database: tasks-payload\n      host: localhost\n      port: 27017\n</code></pre> <p>The view implementation provides runtime details using standard logging facility. If you want to increase the logging level, please set up it e.g. in your <code>application.yaml</code>:</p> <pre><code>logging.level.io.holunda.polyflow.view.mongo: DEBUG\n</code></pre> <p>For further configuration, please check Mongo DB View Configuration</p>"},{"location":"reference-guide/components/view-mongo.html#collections","title":"Collections","text":"<p>The Mongo View uses several collections to store the results. These are:</p> <ul> <li>data-entries: collection for business data entries</li> <li>processes: collection for process definitions</li> <li>tasks: collection for user tasks</li> <li>tracking-tokens: collection for Axon Tracking Tokens</li> </ul>"},{"location":"reference-guide/components/view-mongo.html#data-entries-collection","title":"Data Entries Collection","text":"<p>The data entries collection stores the business data entries in a uniform Datapool format. Here is an example:</p> <pre><code>{\n\"_id\" : \"io.holunda.camunda.taskpool.example.ApprovalRequest#2db47ced-83d4-4c74-a644-44dd738935f8\",\n\"entryType\" : \"io.holunda.camunda.taskpool.example.ApprovalRequest\",\n\"payload\" : {\n\"amount\" : \"900.00\",\n\"subject\" : \"Advanced training\",\n\"currency\" : \"EUR\",\n\"id\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\",\n\"applicant\" : \"hulk\"\n},\n\"correlations\" : {},\n\"type\" : \"Approval Request\",\n\"name\" : \"AR 2db47ced-83d4-4c74-a644-44dd738935f8\",\n\"applicationName\" : \"example-process-approval\",\n\"description\" : \"Advanced training\",\n\"state\" : \"Submitted\",\n\"statusType\" : \"IN_PROGRESS\",\n\"authorizedUsers\" : [\n\"gonzo\",\n\"hulk\"\n],\n\"authorizedGroups\" : [], \"deleted\" : false,\n\"protocol\" : [\n{\n\"time\" : ISODate(\"2019-08-21T09:12:54.779Z\"),\n\"statusType\" : \"PRELIMINARY\",\n\"state\" : \"Draft\",\n\"username\" : \"gonzo\",\n\"logMessage\" : \"Draft created.\",\n\"logDetails\" : \"Request draft on behalf of hulk created.\"\n},\n{\n\"time\" : ISODate(\"2019-08-21T09:12:55.060Z\"),\n\"statusType\" : \"IN_PROGRESS\",\n\"state\" : \"Submitted\",\n\"username\" : \"gonzo\",\n\"logMessage\" : \"New approval request submitted.\"\n}\n]\n}\n</code></pre>"},{"location":"reference-guide/components/view-mongo.html#tasks-collections","title":"Tasks Collections","text":"<p>Tasks are stored in the following format (an example):</p> <pre><code>{\n\"_id\" : \"dc1abe54-c3f3-11e9-86e8-4ab58cfe8f17\",\n\"sourceReference\" : {\n\"_id\" : \"dc173bca-c3f3-11e9-86e8-4ab58cfe8f17\",\n\"executionId\" : \"dc1a9742-c3f3-11e9-86e8-4ab58cfe8f17\",\n\"definitionId\" : \"process_approve_request:1:91f2ff26-a64b-11e9-b117-3e6d125b91e2\",\n\"definitionKey\" : \"process_approve_request\",\n\"name\" : \"Request Approval\",\n\"applicationName\" : \"example-process-approval\",\n\"_class\" : \"process\"\n},\n\"taskDefinitionKey\" : \"user_approve_request\",\n\"payload\" : {\n\"request\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\",\n\"originator\" : \"gonzo\"\n},\n\"correlations\" : {\n\"io:holunda:camunda:taskpool:example:ApprovalRequest\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\",\n\"io:holunda:camunda:taskpool:example:User\" : \"gonzo\"\n},\n\"dataEntriesRefs\" : [\n\"io.holunda.camunda.taskpool.example.ApprovalRequest#2db47ced-83d4-4c74-a644-44dd738935f8\",\n\"io.holunda.camunda.taskpool.example.User#gonzo\"\n],\n\"businessKey\" : \"2db47ced-83d4-4c74-a644-44dd738935f8\",\n\"name\" : \"Approve Request\",\n\"description\" : \"Please approve request 2db47ced-83d4-4c74-a644-44dd738935f8 from gonzo on behalf of hulk.\",\n\"formKey\" : \"approve-request\",\n\"priority\" : 23,\n\"createTime\" : ISODate(\"2019-08-21T09:12:54.872Z\"),\n\"candidateUsers\" : [\n\"fozzy\",\n\"gonzo\"\n],\n\"candidateGroups\" : [],\n\"dueDate\" : ISODate(\"2019-06-26T07:55:00.000Z\"),\n\"followUpDate\" : ISODate(\"2023-06-26T07:55:00.000Z\"),\n\"deleted\" : false\n}\n</code></pre>"},{"location":"reference-guide/components/view-mongo.html#process-collection","title":"Process Collection","text":"<p>Process definition collection allows for storage of startable process definitions, deployed in a Camunda Engine. This information is in particular interesting, if you are building a process-starter component and want to react dynamically on processes deployed in your landscape.</p> <pre><code>{\n\"_id\" : \"process_approve_request:1:91f2ff26-a64b-11e9-b117-3e6d125b91e2\",\n\"processDefinitionKey\" : \"process_approve_request\",\n\"processDefinitionVersion\" : 1,\n\"applicationName\" : \"example-process-approval\",\n\"processName\" : \"Request Approval\",\n\"processDescription\" : \"This is a wonderful process.\",\n\"formKey\" : \"start-approval\",\n\"startableFromTasklist\" : true,\n\"candidateStarterUsers\" : [],\n\"candidateStarterGroups\" : [\n\"muppetshow\",\n\"avengers\"\n]\n}\n</code></pre>"},{"location":"reference-guide/components/view-mongo.html#tracking-token-collection","title":"Tracking Token Collection","text":"<p>The Axon Tracking Token reflects the index of the event processed by the Mongo View and is stored in the following format:</p> <pre><code>{\n\"_id\" : ObjectId(\"5d2b45d6a9ca33042abea23b\"),\n\"processorName\" : \"io.holunda.camunda.taskpool.view.mongo.service\",\n\"segment\" : 0,\n\"owner\" : \"18524@blackstar\",\n\"timestamp\" : NumberLong(1566379093564),\n\"token\" : { \"$binary\" : \"PG9yZy5heG9uZnJhbWV3b3JrLmV2ZW50aGFuZGxpbmcuR2xvYmFsU2VxdWVuY2VUcmFja2luZ1Rva2VuPjxnbG9iYWxJbmRleD40NDwvZ2xvYmFsSW5kZXg+PC9vcmcuYXhvbmZyYW1ld29yay5ldmVudGhhbmRsaW5nLkdsb2JhbFNlcXVlbmNlVHJhY2tpbmdUb2tlbj4=\", \"$type\" : \"00\" },\n\"tokenType\" : \"org.axonframework.eventhandling.GlobalSequenceTrackingToken\"\n}\n</code></pre>"},{"location":"reference-guide/components/view-simple.html","title":"In-Memory View","text":""},{"location":"reference-guide/components/view-simple.html#purpose","title":"Purpose","text":"<p>The In-Memory View is component responsible for creating read-projections of tasks and business data entries. It implements the Taskpool and Datapool View API and persists the projection in memory. The projection is transient and relies on event replay on every application start. It is good for demonstration purposes if the number of events is manageable small, but will fail to delivery high performance results on a large number of items.</p>"},{"location":"reference-guide/components/view-simple.html#features","title":"Features","text":"<ul> <li>uses concurrent hash maps to store the read model</li> <li>provides single query API</li> <li>provides subscription query API (reactive)</li> <li>relies on event replay and transient token store</li> </ul>"},{"location":"reference-guide/components/view-simple.html#configuration-options","title":"Configuration options","text":"<p>In order to activate the in-memory implementation, please include the following dependency on your classpath:</p> <pre><code>&lt;dependency&gt;\n&lt;groupId&gt;io.holunda.polyflow&lt;/groupId&gt;\n&lt;artifactId&gt;polyflow-view-simple&lt;/artifactId&gt;\n&lt;version&gt;${polyflow.version}&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> <p>Then, add the following annotation to any class marked as Spring Configuration loaded during initialization:</p> <pre><code>@Configuration\n@EnablePolyflowSimpleView\npublic class MyViewConfiguration {\n\n}\n</code></pre> <p>The view implementation provides runtime details using standard logging facility. If you want to increase the logging level, please setup it e.g. in your <code>application.yaml</code>:</p> <pre><code>logging.level.io.holunda.polyflow.view.simple: DEBUG\n</code></pre>"},{"location":"reference-guide/configuration/index.html","title":"Configuration Overview","text":"<p>This is a root of configuration reference guide.</p> <p>Here are some dedicated articles:</p> <ul> <li>Persistence configuration</li> <li>Mongo DB View configuration</li> <li>Datapool Aggregate Tuning</li> <li>Deployment of Polyflow Core inside Process applications</li> </ul>"},{"location":"reference-guide/configuration/core-datapool-aggregate-tuning.html","title":"Datapool Aggregate Tuning","text":""},{"location":"reference-guide/configuration/core-datapool-aggregate-tuning.html#core-datapool-aggregate-tuning","title":"Core Datapool Aggregate Tuning","text":"<p>Data pool is implemented using DDD Aggregate pattern following the CQRS/ES style. By doing so, the event sourced aggregate replays all events to restore its state. In case of Data Entry the aggregate is currently not storing any state, required for command validation, so it is possible to optimize the loading process. For this purpose the following options are provided.</p>"},{"location":"reference-guide/configuration/core-datapool-aggregate-tuning.html#configuration-properties","title":"Configuration properties","text":"Property (prefixed by <code>polyflow.core.data-entry</code>) Description Value Example <code>snapshot-threshold</code> Threshold of events to create a snapshot of the dat entry aggregate Long 5 <code>event-sourcing-repository-type</code> The full-qualified class name of the repository. <code>org.axonframework.eventsourcing.EventSourcingRepository</code> or <code>io.holunda.polyflow.datapool.core.repository.FirstEventOnlyEventSourcingRepository</code> String <code>deletion-strategy</code> Controls how the deletion of data entries is handled. Valid values are <code>lax</code> (default) and <code>strict</code>. See deletion strategy section below. String strict"},{"location":"reference-guide/configuration/core-datapool-aggregate-tuning.html#event-souring-repository","title":"Event Souring Repository","text":"<p>By default, the <code>EventSourcingRepository</code> for every Aggregate is provided by Axon Framework. This repository is supporting loading from snapshots and will load tha last snapshot and all events occurred after the snapshot. Alternatively, you can set the repository to <code>io.holunda.polyflow.datapool.core.repository.FirstEventOnlyEventSourcingRepository</code>. This repository loads the first event only to restore the state. This special repository is using the first event and saves space by not creating any snapshots.</p>"},{"location":"reference-guide/configuration/core-datapool-aggregate-tuning.html#deletion-strategy","title":"Deletion strategy","text":"<p>The data entries can be marked as deleted. If set to <code>strict</code>, no updates can be sent to this data entry, otherwise the update will undelete (re-create) the data entry.</p>"},{"location":"reference-guide/configuration/local-core-deployment.html","title":"Deployment of Core inside Process applications","text":"<p>As described in Distribution using Axon Server (core component as part of process engine)  you might want to deploy your Polyflow Core Components (Taskpool Core and Datapool Core) inside your process application. If you are doing so, you should there are two important decisions to make:</p> <ul> <li>How the Camunda transaction is related to Transaction (Unit of Work) or Polyflow?</li> <li>How to isolate Polyflow Core components from each other if deployed multiple times?</li> </ul>"},{"location":"reference-guide/configuration/local-core-deployment.html#transactional-support-of-integration-components","title":"Transactional support of integration components","text":"<p>The integration components support different transactional behavior inside the command sender components. To be more precise,  after the task collector has collected the commands from the integration points with Camunda, you can set up if the data is passed  to the Command Dispatching component (command bus) inside the same transaction or in a separate transaction.</p> <p>The relevant property to set this up is <code>polyflow.integration.sender.task.send-within-transaction</code>, like you can see in the following example: </p> <pre><code>polyflow:\nintegration:\nsender:\nenabled: true\ntask:\nenabled: true\ntype: tx\nsend-within-transaction: true </code></pre> <p>If set to true, the transaction will be shared between the Camunda Task Lifecycle and command dispatching components. The commands are passed to a special <code>CommandListGateway</code> responsible for sending commands one by one. This component allows to integrate a success and failure handlers to  react to command sending and any failures there. To do so, you need to implement two interfaces and provide bean factories for them:</p> <p><pre><code>  @Bean\n@Primary\nfun myCommandSuccessHandler() = object : CommandErrorHandler {\noverride fun apply(commandMessage: Any, commandResultMessage: CommandResultMessage&lt;out Any?&gt;) {\nlogger.trace { \"Everything went smooth\" }\n}\n}\n\n/**\n   * \n   */\n@Bean\n@Primary\nfun myCommandErrorHandler() = object : CommandSuccessHandler {\noverride fun apply(commandMessage: Any, commandResultMessage: CommandResultMessage&lt;out Any?&gt;) {\nthrow IllegalStateException(\"Something bad happened\")\n}\n}\n</code></pre> By doing so, you can propagate the exception and prevent the initial transaction from commit, if something goes wrong during command dispatch.</p> <p>Another approach for dealing with errors is to minimize their occurrence, by deploying the Core Components inside the same deployment unit as the  process engine itself. To demonstrate this scenario, we created a scenario in Polyflow examples Distributed with Axon Server Events Only.</p> <p>By using this deployment strategy, ever process engine deployment includes the Core components and are taking care of maintaining their state and receiving commands from the integration components. By doing so, you preserve a mean of locality of the tasks originated in a process engine.</p>"},{"location":"reference-guide/configuration/local-core-deployment.html#isolating-polyflow-components","title":"Isolating Polyflow Components","text":"<p>One of the problems that occurs if you use this deployment strategy with Axon Server is that you will get multiple Command Handlers in runtime which are capable of receiving Engine Task Commands. A good way to solve this problem is to prevent Polyflow from registering the Command Handlers  in Axon Server.</p> <p>Since we are using the <code>Axon-Gateway-Extension</code> library in the Polyflow, you can make use of the <code>DispatchAwareCommandBus</code> by configuring the  following properties, which limit the registration of Polyflow Command Handlers in Axon Server:</p> <pre><code>axon-gateway:\ncommand:\ndispatch-aware:\nenabled: true\nstrategy:\nexclude-command-packages:\n- io.holunda.camunda.taskpool.api\n- io.holunda.camunda.datapool.api\n</code></pre> <p>By doing so, the Polyflow Command Handlers (parts of the Core Components) are registered on the local segmet of the command bus only and don't  interfere with each other.</p>"},{"location":"reference-guide/configuration/persistence.html","title":"Persistence Configuration","text":""},{"location":"reference-guide/configuration/persistence.html#persistence","title":"Persistence","text":"<p>If you use relational databases for your Event Store of the DataPool or TaskPool or your view, using the JPA View, Axon Framework, used as a component of Polyflow will detect and autoconfigure itself. Especially, if you use Spring Data JPA or Spring JDBC, Axon auto-configuration will try to reuse it. </p> <p>If you are using <code>@EntityScan</code> annotation, you need to add Axon entities to the scan. To do so, please the following code on top of a class marked with <code>@Configuration</code>.</p> <pre><code>@Configuration\n@EntityScan(\nbasePackageClasses = [\nTokenEntry::class, DomainEventEntry::class, SagaEntry::class\n]\n)\nclass MyConfiguration </code></pre>"},{"location":"reference-guide/configuration/view-mongo.html","title":"Mongo DB View Configuration","text":"<p>The use of Mongo database or a Cosmos DB in Mongo mode as a persistence for the read projection of task-pool and data-pool may require some additional configuration, depending on your scenario.</p> <p>Note</p> <p>We strongly recommend to use a clustered Mongo DB or Cosmos DB installation for persistence to avoid data loss.</p>"},{"location":"reference-guide/configuration/view-mongo.html#configuration-properties","title":"Configuration properties","text":"<p>The configuration of View Mongo is performed via application properties of the component, that includes the <code>polyflow-view-mongo</code>. All configuration properties have the prefix <code>polyflow.view.mongo</code>. Here is the example with all properties:</p> <pre><code>polyflow:\nview:\nmongo:\nchange-stream:\nclear-deleted-tasks:\nafter: PT1H\nbuffer-size: 100000\njob-schedule: '@daily'\njob-jitter: PT1H\njob-timezone: UTC\nmode: SCHEDULED_JOB          clear-deleted-data-entries:\nafter: PT1H\nbuffer-size: 100000\njob-schedule: '@daily'\njob-jitter: PT1H\njob-timezone: UTC\nmode: SCHEDULED_JOB\n\nchange-tracking-mode: CHANGE_STREAM\nindexes:\ntoken-store: false\n</code></pre> Property (prefixed by <code>polyflow.view.mongo</code>) Description Value Default Example <code>change-stream.clear-deleted-tasks.after</code> How long should we keep deleted tasks around before clearing them Duration Duration.ZERO PT1H <code>change-stream.clear-deleted-tasks.buffer-size</code> While the change tracker waits for tasks that have been marked deleted to become due for clearing, it needs to buffer them. This property defines the buffer capacity. If more than [bufferSize] tasks are deleted within the time window defined by [after], the buffer will overflow and the latest task(s) will be dropped. These task(s) will not be automatically cleared in <code>CHANGE_STREAM_SUBSCRIPTION</code> [mode]. In <code>BOTH</code> [mode], the scheduled job will pick them up and clear them eventually. Only relevant if [mode] is <code>CHANGE_STREAM_SUBSCRIPTION</code> or <code>BOTH</code>. Long 10000 200 <code>change-stream.clear-deleted-tasks.job-schedule</code> Cron expression to configure how often the job run that clears deleted tasks should run. Only relevant if [mode] is <code>SCHEDULED_JOB</code> or <code>BOTH</code>. Cron expression @daily @hourly <code>change-stream.clear-deleted-tasks.job-jitter</code> The cleanup job execution time will randomly be delayed after what is determined by the cron expression by [0..this duration]. Duration PT5M PT3M <code>change-stream.clear-deleted-tasks.timezone</code> Time zone to use for resolving the cron expression. Default: UTC. Timezone UTC CET <code>change-stream.clear-deleted-tasks.mode</code> How exactly should we clear deleted tasks. see below see below see below <code>change-stream.clear-deleted-data-entries.after</code> How long should we keep deleted data entries around before clearing them Duration Duration.ZERO PT1H <code>change-stream.clear-deleted-data-entries.buffer-size</code> While the change tracker waits for data entries that have been marked deleted to become due for clearing, it needs to buffer them. This property defines the buffer capacity. If more than [bufferSize] data entries are deleted within the time window defined by [after], the buffer will overflow and the latest data entries(s) will be dropped. These data entries(s) will not be automatically cleared in <code>CHANGE_STREAM_SUBSCRIPTION</code> [mode]. In <code>BOTH</code> [mode], the scheduled job will pick them up and clear them eventually. Only relevant if [mode] is <code>CHANGE_STREAM_SUBSCRIPTION</code> or <code>BOTH</code>. Long 10000 200 <code>change-stream.clear-deleted-data-entries.job-schedule</code> Cron expression to configure how often the job run that clears deleted data entries should run. Only relevant if [mode] is <code>SCHEDULED_JOB</code> or <code>BOTH</code>. Cron expression @daily @hourly <code>change-stream.clear-deleted-data-entries.job-jitter</code> The cleanup job execution time will randomly be delayed after what is determined by the cron expression by [0..this duration]. Duration PT5M PT3M <code>change-stream.clear-deleted-data-entries.timezone</code> Time zone to use for resolving the cron expression. Default: UTC. Timezone UTC CET <code>change-stream.clear-deleted-data-entries.mode</code> How exactly should we clear deleted data entries. see below see below see below <code>change-tracking-mode</code> Mode to use for event tracking. see below see below see below <code>indexes.token-store</code> Controls the index of the token store. Boolean true false"},{"location":"reference-guide/configuration/view-mongo.html#change-tracking-mode","title":"Change Tracking Mode","text":"<p>The events delivering updates on view artifacts are received by the projection and cause updates in Mongo DB projections. If you are interested in delivering reactive updates (e.g. Server push, SSE, Websocket), you can control what is the trigger for the update modification. The reason for this is that Mongo DB itself is eventually consistent in replica set and the update operation will eventually be  written to all nodes. If you are reading the documents by the component triggered by the reactive update, you might end up in a race condition. </p> <p>Set the mode for event tracking by selecting from one of the following values:</p> Value Description EVENT_HANDLER Use Axon query bus and update subscriptions after the event has been processed. CHANGE_STREAM Use Mongo DB change stream. NONE Disable reactive updates."},{"location":"reference-guide/configuration/view-mongo.html#clear-deleted-tasks","title":"Clear deleted tasks","text":"<p>Removal of elements from collection is a costly operation in distributed Mongo DB cluster. For this purpose, if the user task gets deleted, it is marked by a <code>deleted</code> flag and immediately excluded from the selection. A later job is responsible for real wiping it out from the collection. The deletion mode controls how this operation is performed. Set the mode for task deletion by selecting from one of the following values: </p> Value Description <code>CHANGE_STREAM_SUBSCRIPTION</code> Subscribe to the change stream and clear any tasks that are marked deleted after the duration configured in <code>after</code> property. <code>SCHEDULED_JOB</code> Run a scheduled job to clear any tasks that are marked as deleted if the deletion timestamp is at least <code>after</code> property in the past. The job is run according to the cron expression defined in <code>job-schedule</code> property. <code>BOTH</code> Use <code>CHANGE_STREAM_SUBSCRIPTION</code> and <code>SCHEDULED_JOB</code>. <code>NONE</code> The application is taking care of clearing deleted tasks, e.g. by implementing its own scheduled job or using a partial TTL index."}]}